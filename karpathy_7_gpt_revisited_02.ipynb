{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sXVuplHTNPVn",
        "r6PlYOAjRNUz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "## Building GPT from Scratch\n",
        "\n",
        "This Colab notebook is mostly a compilation of personal notes created while working through Andrej Karpathy's YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) for the third time.\n",
        "\n",
        "- Contains large Notes section with much interesting Pytorch and GPT related information."
      ],
      "metadata": {
        "id": "Cc4lCdwbs6cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "W53M3ROCNjqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "5ysJGj2s1Fyi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yl4aFOR1RnY",
        "outputId": "49a3c194-79de-4d01-9fab-39408f974b28"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c1f6f105cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HgmHYOoePs_J"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "QsEkbCfz3IBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpaYcHPp1AkC",
        "outputId": "18f8923e-ea20-4741-f99f-de6a9eb9512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-16 14:28:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-16 14:28:49 (18.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ],
      "metadata": {
        "id": "ZO-HRGQa3P9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "mkx0ry2W1hsQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gm8i9yW12v9",
        "outputId": "ebb8e4b2-f1c7-4f50-b5dc-447948717fa9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MxYYEgkQ2ZMi",
        "outputId": "ebfa903a-f5a6-40cc-bdfa-19f21a794ef8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nszQiTfK2f6k",
        "outputId": "51cc66c7-9a9e-428d-c4ca-be134ff27f06"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "0CJ0bcha2joy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WIm090L21qT",
        "outputId": "8854fa17-447d-4fd4-8bfc-bf96ccef5da4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "TRr3favu28Gw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2UJs8jT3ARp",
        "outputId": "b0289217-4c01-4792-efee-bbe357365066"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "Y3UnDaa73Xva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {c:i for i,c in enumerate(chars)}"
      ],
      "metadata": {
        "id": "Y5nR0NV13bbV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hONpsKBt3boL",
        "outputId": "951cc6c4-9278-4fac-d423-78959011c1b5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = {i:c for i,c in enumerate(chars)}"
      ],
      "metadata": {
        "id": "Cr83cV1g5GoF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5WlHSIB3b2s",
        "outputId": "bfc96509-19d5-4e9a-dfdd-4823ecf80132"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s]"
      ],
      "metadata": {
        "id": "cdgyuN5u55xL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello world\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3WkKvzU556X",
        "outputId": "32033a6a-45af-4709-8473-8db860b71612"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode = lambda l: \"\".join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "47QMSiUyMA7a"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn9du_dF56aE",
        "outputId": "70f4a85b-4dfc-4b89-dad3-26291ddb9ea9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and test splits"
      ],
      "metadata": {
        "id": "sXVuplHTNPVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "2mtMyprgNB59"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lYjRFXZN9Za",
        "outputId": "f8590745-88f0-47b4-8f8d-72c22449fdbd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1115394])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Mhrf5XY-NCE2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5cVrsDLNCOK",
        "outputId": "7fabb6eb-db1a-4a44-abf8-26a9f38c3ce8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "r6PlYOAjRNUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 2\n",
        "n_embed = 2"
      ],
      "metadata": {
        "id": "ogWeFhzxRRU7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "M5O2hq2dP6El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "\n",
        "    ix = torch.randint(len(data)- block_size , (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "yTBTZKgUP7vb"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_batch(\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri50uLOBTUkb",
        "outputId": "625b0758-655b-4fd3-860b-11360222b938"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "         [44, 53, 56,  1, 58, 46, 39, 58]]),\n",
              " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
              "         [53, 56,  1, 58, 46, 39, 58,  1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Embedding Layer and Output Linear Transformation"
      ],
      "metadata": {
        "id": "wGuBwZDOaocM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # Embedding tokens\n",
        "        tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(tok_emb)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "rEL1bg7tpmiw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Generate Function"
      ],
      "metadata": {
        "id": "n3eVk2ntvYB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate new tokens given an initial index tensor.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Starting tensor of indices, shape (B, T)\n",
        "            max_new_tokens (int): Number of new tokens to generate\n",
        "            temperature (float): Controls the randomness of predictions by scaling logits\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor containing the original and new generated indices\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)  # Generate logits for the last token\n",
        "            logits = logits[:, -1, :] / temperature  # Use the last token's logits, apply temperature\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the probability distribution\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Append the new index to the sequence\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "b28Kw_XkXSNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # Embedding tokens\n",
        "        tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(tok_emb)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate new tokens given an initial index tensor.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Starting tensor of indices, shape (B, T)\n",
        "            max_new_tokens (int): Number of new tokens to generate\n",
        "            temperature (float): Controls the randomness of predictions by scaling logits\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor containing the original and new generated indices\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)  # Generate logits for the last token\n",
        "            logits = logits[:, -1, :] / temperature  # Use the last token's logits, apply temperature\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the probability distribution\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Append the new index to the sequence\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=10)[0].tolist()\n",
        "print(decode(generated_indices))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbOiBJTKv2dD",
        "outputId": "a4babf09-5296-42e9-86f5-276f5ec91302"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ",p-dazPolX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation loop"
      ],
      "metadata": {
        "id": "6-o2-J1najIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 200\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"eval\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "K1gDaVRLaAnq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "FvLL7qSnU_b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "71ePcsAio-NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 2\n",
        "n_embed = 2\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # Embedding tokens\n",
        "        tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(tok_emb)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate new tokens given an initial index tensor.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Starting tensor of indices, shape (B, T)\n",
        "            max_new_tokens (int): Number of new tokens to generate\n",
        "            temperature (float): Controls the randomness of predictions by scaling logits\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor containing the original and new generated indices\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)  # Generate logits for the last token\n",
        "            logits = logits[:, -1, :] / temperature  # Use the last token's logits, apply temperature\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the probability distribution\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Append the new index to the sequence\n",
        "\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "815ZAxn8XO-k",
        "outputId": "d873bc53-2dbf-424c-e0da-ca0307e31f8a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.468097686767578, val loss 4.53944206237793 \n",
            "step 500: training loss 3.06418776512146, val loss 3.1177852153778076 \n",
            "step 1000: training loss 2.915592670440674, val loss 2.8232614994049072 \n",
            "step 1500: training loss 2.8693275451660156, val loss 2.8686513900756836 \n",
            "step 2000: training loss 2.7696008682250977, val loss 2.90069580078125 \n",
            "step 2500: training loss 2.908082962036133, val loss 2.775252103805542 \n",
            "\n",
            ":Oty thedo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yteBo0_eZDUi",
        "outputId": "30479bb7-3e34-4b8d-d227-e700e98bf9c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mteonweet uueon v yhauinreiSmtaf m fakong goLUSAtea chitheon?\n",
            "\n",
            "Mb ?W tr.\n",
            "Y be tel  Bshhiveanf hinhhon o ! bt,\n",
            "LN reen Ru wit dererre s abldibesbeathos rry k marudhobehreot  m sys:\n",
            "\n",
            "RC .hede t g thovep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AEjppBNb4Hr",
        "outputId": "bba6385d-aec8-47ce-e97f-179fff029731"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 2)\n",
            "  (lm_head): Linear(in_features=2, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 2])\n",
            "lm_head.weight \t torch.Size([65, 2])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Total Parameters: 325\n",
            "Trainable Parameters: 325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Positional Embeddings"
      ],
      "metadata": {
        "id": "NM2R3jx8pDht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 2\n",
        "n_embed = 2\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-KZP_QpLS7",
        "outputId": "f5d9d0bd-323d-45cf-d091-7f950281adb8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.3499064445495605, val loss 4.3931756019592285 \n",
            "step 500: training loss 3.116835355758667, val loss 3.150700807571411 \n",
            "step 1000: training loss 2.948291063308716, val loss 2.844757556915283 \n",
            "step 1500: training loss 2.816631317138672, val loss 2.887141466140747 \n",
            "step 2000: training loss 2.8248682022094727, val loss 2.795835256576538 \n",
            "step 2500: training loss 2.9078564643859863, val loss 2.781541347503662 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXPE3GJxtNUx",
        "outputId": "007092a1-e2af-414f-c81a-d0c42c6c63b4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hipe so i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10DrXstS0wL7",
        "outputId": "a67e0fb7-2405-4316-a297-b742a287bf3e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 2)\n",
            "  (position_embedding_table): Embedding(8, 2)\n",
            "  (lm_head): Linear(in_features=2, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 2])\n",
            "position_embedding_table.weight \t torch.Size([8, 2])\n",
            "lm_head.weight \t torch.Size([65, 2])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 341\n",
            "Trainable Parameters: 341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical tricks in self-attention"
      ],
      "metadata": {
        "id": "LK1EMV7K9zq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1: Inefficient attention using averages"
      ],
      "metadata": {
        "id": "uGlcWeWMDcEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "print(x.shape)\n",
        "print(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MtPCpdZ97V5",
        "outputId": "1be5bbc9-1317-4448-e52a-833784b8e85a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 2])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkAlzCg8_Pkx",
        "outputId": "44865904-65b7-4219-d7fd-792a7150d9bb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]\n",
        "        if b == 0:\n",
        "            print(xprev)\n",
        "        xbow[b][t] = xprev.sum(dim=0)\n",
        "\n",
        "print(\"\\n---------\")\n",
        "print(x[0])\n",
        "print(xbow[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49sUIqdw_fFG",
        "outputId": "d8fdfe32-7158-45ee-d381-2d301150f6c7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "\n",
            "---------\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.1789, -0.9852],\n",
            "        [ 0.4469, -0.9597],\n",
            "        [ 1.4014, -0.8953],\n",
            "        [ 1.7626,  0.2725],\n",
            "        [ 0.4127, -0.2376],\n",
            "        [ 0.6486, -0.4774],\n",
            "        [-0.2725,  1.0659]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]\n",
        "        xbow[b][t] = xprev.mean(dim=0)\n",
        "\n",
        "print(x[0])\n",
        "print(xbow[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8q8w7eA4KY",
        "outputId": "a312c0ee-d06b-422f-bc31-ccecf7a432fe"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix multiplication - basic summing"
      ],
      "metadata": {
        "id": "p7VxS6kxDpi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print('----')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('----')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsKZKK8rDykB",
        "outputId": "3671d676-e6e3-4621-ae6d-cb754051200d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "b=\n",
            "----\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "----\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.tril` - **trick!!**"
      ],
      "metadata": {
        "id": "pft_jNfpLFCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# sum\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "\n",
        "c = a @ b\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print('----')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('----')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBpLLviTLYdU",
        "outputId": "1d940ec3-58ad-40ec-be23-78d633b720fb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "b=\n",
            "----\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "----\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# normalize the rows for INCREMENTAL AVERAGES\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(torch.sum(a, dim=1))\n",
        "print(torch.sum(a, dim=1).shape)\n",
        "print(torch.sum(a, dim=1, keepdim=True))\n",
        "print(torch.sum(a, dim=1, keepdim=True).shape)\n",
        "print(\"\\n---\")\n",
        "\n",
        "a = a / torch.sum(a, dim=1, keepdim=True)\n",
        "b = torch.randint(0,10, (3,2)).float()\n",
        "\n",
        "\n",
        "\n",
        "c = a @ b\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print('----')\n",
        "print(b)\n",
        "print('c=')\n",
        "print('----')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ei9pFgM72B",
        "outputId": "d35592b7-0f7f-4f3c-8839-0e5e8b2676bb"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "tensor([1., 2., 3.])\n",
            "torch.Size([3])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n",
            "\n",
            "---\n",
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b=\n",
            "----\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "----\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2: Weighted aggregation using Batched Matrix Multiply - **trick!!!!**"
      ],
      "metadata": {
        "id": "1_AgWjyhUfWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "xbow = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]\n",
        "        xbow[b][t] = xprev.mean(dim=0)\n",
        "\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "\n",
        "xbow2 = wei @ x\n",
        "\n",
        "# Check with a more relaxed tolerance\n",
        "close = torch.allclose(xbow, xbow2, atol=1e-6, rtol=1e-4)\n",
        "print(\"Are the tensors close within a relaxed tolerance?\", close)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gENdPzW4Uuc-",
        "outputId": "9f46393a-f86b-42f6-9b33-4a317239075f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the tensors close within a relaxed tolerance? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the maximum difference\n",
        "max_diff = torch.max(torch.abs(xbow - xbow2))\n",
        "print(\"Maximum difference between xbow and xbow2:\", max_diff)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-LLF7c_X7a5",
        "outputId": "1fe4fc1d-8023-4e52-ba95-69a398ca2aa6"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum difference between xbow and xbow2: tensor(3.2363e-08)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 3: Softmax - **trick!!!!**"
      ],
      "metadata": {
        "id": "hcqt60cSZqb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "print(tril)\n",
        "print(\"\\n----\")\n",
        "wei = torch.zeros(T,T)\n",
        "print(wei)\n",
        "print(\"\\n----\")\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "print(wei)\n",
        "print(\"\\n----\")\n",
        "wei = F.softmax(wei, dim=1)\n",
        "print(wei)\n",
        "print(\"\\n----\")\n",
        "\n",
        "xbow3 = wei @ x\n",
        "\n",
        "close = torch.allclose(xbow, xbow3, atol=1e-6, rtol=1e-4)\n",
        "print(\"Are the tensors close within a relaxed tolerance?\", close)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly0sie1gZx0T",
        "outputId": "6836806e-d895-4ab5-c89a-1862a798ad88"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "\n",
            "----\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "----\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "----\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "\n",
            "----\n",
            "Are the tensors close within a relaxed tolerance? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 4: Self-Attention"
      ],
      "metadata": {
        "id": "TtNGpnfs4WJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros(T,T) # we don't uniform. we want data-dependant\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "print(wei.shape)\n",
        "\n",
        "out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkbTMq2L4a-r",
        "outputId": "dc9ee091-2dce-4f9f-ae77-5ae611058e88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 8])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "query = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
        "key = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
        "\n",
        "q = query(x) # (4, T, 16)\n",
        "k = key(x) # (4, T, 16)\n",
        "wei = q @ k.transpose(-2,-1) # (B, T, T)\n",
        "# old wei = torch.zeros(T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros(T, T)\n",
        "wei = torch.masked_fill(wei, tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "out = wei @ x\n",
        "print(wei.shape)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SG2hqKR4_v2",
        "outputId": "9acadac4-12e2-4b3e-b29d-0f33d3273a81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 8])\n",
            "torch.Size([4, 8, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "query = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
        "key = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "q = query(x) # (B, T, 16)\n",
        "k = key(x) # (B, T, 16)\n",
        "v = value(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2,-1) * head_size**-0.5 # (B, T, T)\n",
        "print(wei.var())\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# wei = torch.zeros(T, T)\n",
        "wei = torch.masked_fill(wei, tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ v # v is the vector which we aggregate (for this single head)\n",
        "\n",
        "# out shape: (B, T, head_size)\n",
        "print(out.shape)\n",
        "print(wei[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6Oe0RuAJRlv",
        "outputId": "c6376de6-7606-434b-e5ca-d12deec36801"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1201, grad_fn=<VarBackward0>)\n",
            "torch.Size([4, 8, 16])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5221, 0.4779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3602, 0.3210, 0.3188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2980, 0.4039, 0.1578, 0.1404, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1643, 0.1243, 0.1678, 0.1865, 0.3570, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2656, 0.2110, 0.1137, 0.1214, 0.2018, 0.0865, 0.0000, 0.0000],\n",
            "        [0.1761, 0.1327, 0.1371, 0.0974, 0.1476, 0.1918, 0.1173, 0.0000],\n",
            "        [0.1046, 0.1260, 0.0922, 0.0906, 0.1476, 0.1588, 0.1432, 0.1371]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Single Attention Head"
      ],
      "metadata": {
        "id": "gLijIXx92NM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 2\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "head_size = 32\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.sa_head = Head(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsJ8Byga2WBa",
        "outputId": "c52ff147-0eb9-4733-948a-296a0d00d7b6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.172791004180908, val loss 4.200776100158691 \n",
            "step 500: training loss 3.2094807624816895, val loss 3.1048107147216797 \n",
            "step 1000: training loss 2.867919445037842, val loss 2.8942389488220215 \n",
            "step 1500: training loss 2.864621639251709, val loss 2.6551945209503174 \n",
            "step 2000: training loss 2.5643837451934814, val loss 2.692009687423706 \n",
            "step 2500: training loss 2.697690486907959, val loss 2.7760109901428223 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2lTU-RZ2bAA",
        "outputId": "7a7f215c-4a1d-4756-f8f9-558be7ec10dd"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CESSTk hitel mindendg,\n",
            "Oe, win me\n",
            "KWt wee.\n",
            "LAlld t f rin.\n",
            "QMMons best b ser yopince stele swy\n",
            "\n",
            "Tersind ve;\n",
            "B ll Vv:\n",
            "IWousisind bsle bl\n",
            "N; sheryo bovs iwerllerfon ord.\n",
            "CHt fenovehat pfounapons.\n",
            "OONTae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fN1gfPR2dT7",
        "outputId": "8e80cfee-c371-4395-ca7d-2f8ac65ae7ee"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_head): Head(\n",
            "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_head.tril \t torch.Size([8, 8])\n",
            "sa_head.query.weight \t torch.Size([32, 32])\n",
            "sa_head.key.weight \t torch.Size([32, 32])\n",
            "sa_head.value.weight \t torch.Size([32, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Multi-head Attention"
      ],
      "metadata": {
        "id": "Tg0I_1D3qhm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embed // 4)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cKUDFpgqmkS",
        "outputId": "3cf60b1c-5bd4-47e3-f817-bdfae7ba7c7b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.2066192626953125, val loss 4.234907627105713 \n",
            "step 500: training loss 2.9258944988250732, val loss 2.9589881896972656 \n",
            "step 1000: training loss 2.581874370574951, val loss 2.6997199058532715 \n",
            "step 1500: training loss 2.6127994060516357, val loss 2.601308822631836 \n",
            "step 2000: training loss 2.514824867248535, val loss 2.552429676055908 \n",
            "step 2500: training loss 2.52724027633667, val loss 2.5238077640533447 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvsn7NF6r6e4",
        "outputId": "ba761f92-9fc8-4701-bad0-a8458fd68b21"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MENAELK:!r Gond, ig' f oud tathee fus 'whaone th yon deemt.\n",
            "Blroret, keu Rounf fe;\n",
            "\n",
            "Shpar no, wis\n",
            "Ca'dl'sd to athevilleno nume yowsnry yoo.\n",
            "May ma&hep vere, ghr Hof wound ram, ito dred wow hag.\n",
            "Waco, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAMOqAOkr1O9",
        "outputId": "075a47f1-d1c3-459d-db5d-40d2a565b121"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_heads): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.0.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.1.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.2.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.3.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.value.weight \t torch.Size([8, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Feed forward"
      ],
      "metadata": {
        "id": "S6YupX1f9j8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embed // 4)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl-bFDyf9n8L",
        "outputId": "5b5959fa-1265-4599-cd4e-4a0e2e0c1213"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.222600936889648, val loss 4.2188401222229 \n",
            "step 500: training loss 2.9812910556793213, val loss 2.913625955581665 \n",
            "step 1000: training loss 2.76777720451355, val loss 2.5907235145568848 \n",
            "step 1500: training loss 2.5140411853790283, val loss 2.4913179874420166 \n",
            "step 2000: training loss 2.570660352706909, val loss 2.436124324798584 \n",
            "step 2500: training loss 2.3858771324157715, val loss 2.5086424350738525 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T7SopMZ9t6j",
        "outputId": "26e809ac-d302-4765-ffd3-e60702e0aad5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wat fhe e sarege paionst eaft eour doth P: lisas anding-eene Se\n",
            "SESISINO:\n",
            "I baml,\n",
            "Meatof sape. Rurs nis\n",
            "Youl, sou, inoth ou, creeng, dha waft!\n",
            " erot.\n",
            "\n",
            "Yrouse Wes!d museds'd\n",
            "Cere dronm heogor chelonp, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0sdvKDa9xDs",
        "outputId": "05378b43-4d95-4961-d9b1-0b4d9c9a32d5"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_heads): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ffwd): FeedForward(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.0.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.1.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.2.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.3.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.value.weight \t torch.Size([8, 32])\n",
            "ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "ffwd.net.0.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 8609\n",
            "Trainable Parameters: 8609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Transformer block"
      ],
      "metadata": {
        "id": "LCxydrjgA3kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnuVvZAKA5-H",
        "outputId": "7cf24730-c6c0-46c2-a5b4-6367389004ec"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.179017543792725, val loss 4.179043769836426 \n",
            "step 500: training loss 3.1474251747131348, val loss 3.066397190093994 \n",
            "step 1000: training loss 2.839433431625366, val loss 2.9424216747283936 \n",
            "step 1500: training loss 2.7702505588531494, val loss 2.642995834350586 \n",
            "step 2000: training loss 2.664306163787842, val loss 2.6620941162109375 \n",
            "step 2500: training loss 2.6165897846221924, val loss 2.5415029525756836 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnPbcatA8xN",
        "outputId": "f3b777c3-d85c-42e9-b1ec-37869295fa1d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLR:' me hic my nro ce Xed whe An\n",
            ":\n",
            "Sdyes tey wath hrate\n",
            "Pest.\n",
            "\n",
            "EINw eeond pcis, par sleilbige nghik cis oveord'ed tho yo'? foo ad w'e'tar I yasd isfe, buer thoneast tit\n",
            "Cend, sour, int\n",
            "Low; Ylou ooua\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSzoIHNfA_m_",
        "outputId": "2904afe7-7f00-4253-d487-b2d9befcfe96"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 12737\n",
            "Trainable Parameters: 12737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model optimization help 1: Skip connections"
      ],
      "metadata": {
        "id": "WSaQZDQtG4Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smeE-tS4G_-6",
        "outputId": "c39c63c4-d13a-410a-d96e-11a4ecf889d2"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 5.1030402183532715, val loss 5.022217750549316 \n",
            "step 500: training loss 2.778379201889038, val loss 2.8679325580596924 \n",
            "step 1000: training loss 2.7662031650543213, val loss 2.660550355911255 \n",
            "step 1500: training loss 2.5764899253845215, val loss 2.5275442600250244 \n",
            "step 2000: training loss 2.4651472568511963, val loss 2.4526543617248535 \n",
            "step 2500: training loss 2.3259408473968506, val loss 2.3860621452331543 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIlIlqDVHF7p",
        "outputId": "e5684354-2692-46b9-f978-1c3d7b5dd5e4"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tith be uldevithe\n",
            "Ther yheteaiuwanems sow pis.\n",
            "\n",
            "Gepeak dand to are bedreen alim, ancd veringr,\n",
            "Gof des,\n",
            "Mhip ol four he par sat'vy forwougor\n",
            "Fit silf theith nower the wow onik thy firs lon;;.\n",
            "\n",
            "Weres g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qixuzTOQHIo6",
        "outputId": "ba546103-c7d8-4680-c928-50a1ec7978d9"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 12737\n",
            "Trainable Parameters: 12737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Multihead Attention Projection Layer"
      ],
      "metadata": {
        "id": "bEnB0ozANBpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(head_outputs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6s7fbxNNGWL",
        "outputId": "b747bedb-1bbe-4579-832e-e22eacd000f0"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.460845947265625, val loss 4.462778568267822 \n",
            "step 500: training loss 2.599686622619629, val loss 2.7281298637390137 \n",
            "step 1000: training loss 2.5071492195129395, val loss 2.47788667678833 \n",
            "step 1500: training loss 2.5382513999938965, val loss 2.4370779991149902 \n",
            "step 2000: training loss 2.3791794776916504, val loss 2.384490489959717 \n",
            "step 2500: training loss 2.3141043186187744, val loss 2.386993408203125 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAT61y74NKS5",
        "outputId": "5a2caa51-9244-4678-a767-7950af3d118d"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fouf prened:\n",
            "And so meelf. I tota I:\n",
            "Thel My ghay whe, ot, mam, at geste utht that opotee urik wir ta, xn the wokss fabrtam?\n",
            "Tao dirp,\n",
            "Bo sore thare;\n",
            "t.\n",
            "Le- I Jinced to groatI howsine\n",
            "fratesppastng s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlCtrio-NMl5",
        "outputId": "a813835d-61a9-4f14-d175-503663f54e2b"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.0.sa.proj.bias \t torch.Size([32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.0.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.0.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.1.sa.proj.bias \t torch.Size([32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.1.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.1.ffwd.net.2.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 29441\n",
            "Trainable Parameters: 29441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model optimization help 2: Layer Normalization"
      ],
      "metadata": {
        "id": "MGCDbSJuTdoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(head_outputs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4),\n",
        "            nn.LayerNorm(n_embed)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7p9CwSsTmih",
        "outputId": "1cde8b95-d5af-4fcd-f5fb-c74c02730192"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.372638702392578, val loss 4.395672798156738 \n",
            "step 500: training loss 2.6571121215820312, val loss 2.645972728729248 \n",
            "step 1000: training loss 2.5392327308654785, val loss 2.563103437423706 \n",
            "step 1500: training loss 2.5982604026794434, val loss 2.486126661300659 \n",
            "step 2000: training loss 2.4181649684906006, val loss 2.4356579780578613 \n",
            "step 2500: training loss 2.403215169906616, val loss 2.3882241249084473 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_NMRn7qTq78",
        "outputId": "8be2e10d-9d62-47d4-a208-c88e8d342217"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NTo gou.\n",
            "\n",
            "Hithe Xeswid.\n",
            "\n",
            "This cull hbiedent thiserr:\n",
            "Hot chast youll ly shilnd.\n",
            "\n",
            "On be's ama?\n",
            "\n",
            "thes thy the--bak a t tholl tjor, an to my,urtes,e nat pasketit, Vin en's wi'cge pruthhe ste mh Vit may v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ-95ne6Ttns",
        "outputId": "5a4294b6-cd45-44eb-835a-41c15b2c29eb"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.0.sa.proj.bias \t torch.Size([32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.0.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.0.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.0.ln1.weight \t torch.Size([32])\n",
            "blocks.0.ln1.bias \t torch.Size([32])\n",
            "blocks.0.ln2.weight \t torch.Size([32])\n",
            "blocks.0.ln2.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.1.sa.proj.bias \t torch.Size([32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.1.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.1.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.1.ln1.weight \t torch.Size([32])\n",
            "blocks.1.ln1.bias \t torch.Size([32])\n",
            "blocks.1.ln2.weight \t torch.Size([32])\n",
            "blocks.1.ln2.bias \t torch.Size([32])\n",
            "blocks.2.weight \t torch.Size([32])\n",
            "blocks.2.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Tidy Up"
      ],
      "metadata": {
        "id": "A4h4_f-qb9ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "n_head=4\n",
        "n_layer=2\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(head_outputs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVDPRZYMb7_X",
        "outputId": "588859e0-8a64-46d8-8943-61c0a1485e9c"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.301537036895752, val loss 4.303574562072754 \n",
            "step 500: training loss 2.633894681930542, val loss 2.664252281188965 \n",
            "step 1000: training loss 2.502023458480835, val loss 2.4725911617279053 \n",
            "step 1500: training loss 2.3814761638641357, val loss 2.540698528289795 \n",
            "step 2000: training loss 2.3917155265808105, val loss 2.4259471893310547 \n",
            "step 2500: training loss 2.5475194454193115, val loss 2.3882737159729004 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNjEqdbgd865",
        "outputId": "2cd82106-3811-48ad-8811-6b4a578422d4"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hfamt haise to herooke fe, gurvealst let teasce,\n",
            "Te ware chand cas; m?\n",
            "-gorelfr:\n",
            "And,\n",
            "Illle;\n",
            "Wefut dell beay ry Mou wat wafll laves,\n",
            "Whankid yavehereucer cull haple t mick; otqres couj glo, hak had th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTF4AgEfeAE0",
        "outputId": "300f2fce-316a-44a2-e640-8881c198f266"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.0.sa.proj.bias \t torch.Size([32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.0.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.0.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.0.ln1.weight \t torch.Size([32])\n",
            "blocks.0.ln1.bias \t torch.Size([32])\n",
            "blocks.0.ln2.weight \t torch.Size([32])\n",
            "blocks.0.ln2.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.1.sa.proj.bias \t torch.Size([32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.1.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.1.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.1.ln1.weight \t torch.Size([32])\n",
            "blocks.1.ln1.bias \t torch.Size([32])\n",
            "blocks.1.ln2.weight \t torch.Size([32])\n",
            "blocks.1.ln2.bias \t torch.Size([32])\n",
            "ln_f.weight \t torch.Size([32])\n",
            "ln_f.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: Dropout (prevent overfitting)"
      ],
      "metadata": {
        "id": "ha9jCymxeYEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context length\n",
        "batch_size = 4\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "max_iters = 3000\n",
        "\n",
        "n_head = 4\n",
        "n_layer = 2\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(head_outputs)\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"Initialize the GPT model components.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"Forward pass for generating logits and optionally computing loss.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Input tensor of token indices with shape (B, T).\n",
        "            targets (torch.Tensor, optional): Target tensor of token indices with shape (B, T).\n",
        "                Default is None, which skips loss computation.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - logits (torch.Tensor): Logits tensor of shape (B, T, vocab_size).\n",
        "                - loss (torch.Tensor or None): Computed cross-entropy loss if targets are provided, else None.\n",
        "        \"\"\"\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, n_embed)\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # x = tok_emb + pos_emb\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Generating logits using the language model head\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshape for cross-entropy loss computation\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B*T, C)  # Shape: (B*T, vocab_size)\n",
        "            targets_flat = targets.view(B*T)  # Shape: (B*T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if(iter % eval_interval == 0):\n",
        "        losses = estimate_loss();\n",
        "        print(f\"step {iter}: training loss {losses['train']}, val loss {losses['val']} \")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKbK-XNjebae",
        "outputId": "b60fbfc7-09b4-4362-9052-9844f7dc4436"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: training loss 4.305357456207275, val loss 4.3150129318237305 \n",
            "step 500: training loss 2.766637086868286, val loss 2.698511838912964 \n",
            "step 1000: training loss 2.568530321121216, val loss 2.5664477348327637 \n",
            "step 1500: training loss 2.4630465507507324, val loss 2.4896674156188965 \n",
            "step 2000: training loss 2.4210150241851807, val loss 2.4258134365081787 \n",
            "step 2500: training loss 2.412698268890381, val loss 2.338477373123169 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=200)[0].tolist()\n",
        "print(decode(generated_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt9b0h8Ced3Y",
        "outputId": "c925b069-547b-482e-ef73-cf3da8971f73"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "oofs verars, shayd s be croanc, ho'e and he tliery aiscin, stheare tay sat, melt ablet, move shaot you she forkons bureendes. bis,\n",
            "Aye\n",
            "The fatese my fourse dedranld issgabeays yat wraak\n",
            "Ywareng iche n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(model)\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"\\n------------ \")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASVVYTcWekk5",
        "outputId": "4e82d58b-f044-4154-901e-ec4e82d36fc5"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Structure: \n",
            "GPT(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "          (2): ReLU()\n",
            "          (3): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): Dropout(p=0.1, inplace=False)\n",
            "          (2): ReLU()\n",
            "          (3): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "------------ \n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.0.sa.proj.bias \t torch.Size([32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.0.ffwd.net.3.weight \t torch.Size([32, 128])\n",
            "blocks.0.ffwd.net.3.bias \t torch.Size([32])\n",
            "blocks.0.ln1.weight \t torch.Size([32])\n",
            "blocks.0.ln1.bias \t torch.Size([32])\n",
            "blocks.0.ln2.weight \t torch.Size([32])\n",
            "blocks.0.ln2.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.1.sa.proj.bias \t torch.Size([32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.1.ffwd.net.3.weight \t torch.Size([32, 128])\n",
            "blocks.1.ffwd.net.3.bias \t torch.Size([32])\n",
            "blocks.1.ln1.weight \t torch.Size([32])\n",
            "blocks.1.ln1.bias \t torch.Size([32])\n",
            "blocks.1.ln2.weight \t torch.Size([32])\n",
            "blocks.1.ln2.bias \t torch.Size([32])\n",
            "ln_f.weight \t torch.Size([32])\n",
            "ln_f.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "------------ \n",
            "\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2"
      ],
      "metadata": {
        "id": "XjbLT5XjjuEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLOSqRtFjvnf",
        "outputId": "27987ae6-a07c-4379-8495-135f7e04896d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "model_name = \"gpt2\"\n",
        "m = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "input_text = \"This was a dark and dangerous \"\n",
        "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "output_tokens = m.generate(input_tokens, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "gRlF_KSzj_v_",
        "outputId": "358eed56-46ae-400f-cf76-a0125518655a"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.gpt2 because of the following error (look up to see its traceback):\ncannot import name 'is_torch_xla_available' from 'transformers.utils' (/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;31m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"torch\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPYTORCH_IMPORT_ERROR_WITH_TF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0malbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/depth_anything/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Backward compatibility imports, to make sure all those objects can be found in file_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mCLOUDFRONT_DISTRIB_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_torch_xla_available' from 'transformers.utils' (/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-173-e0d1b05509c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This was a dark and dangerous \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"jinja\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_jinja_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJINJA_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     ]\n\u001b[0;32m-> 1380\u001b[0;31m )\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"torch\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPYTORCH_IMPORT_ERROR_WITH_TF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m     \u001b[0;31m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"torch\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2 because of the following error (look up to see its traceback):\ncannot import name 'is_torch_xla_available' from 'transformers.utils' (/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "tdSrP7vblM2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Embedding Table and LM Head Dimensions in Language Models\n",
        "\n",
        "In the realm of natural language processing (NLP), the architecture of language models plays a critical role in determining their efficiency and effectiveness. Two essential components of these models are the **embedding table** and the **language model head (lm_head)**. Understanding the dimensions of these components and their interactions is crucial for both designing and interpreting model behaviors.\n",
        "\n",
        "### Embedding Table: Where Words Become Vectors\n",
        "\n",
        "The **embedding table** serves as the bridge between discrete language symbols (e.g., words or characters) and their continuous vector representations. Think of it as a lookup table where each unique word in your vocabulary is associated with a dense vector. These vectors capture semantic properties such that similar words have similar vectors. The dimensions of this table are typically denoted as `(vocab_size, embedding_dim)`:\n",
        "\n",
        "- `vocab_size` is the number of unique tokens in your model's vocabulary. It represents the \"width\" of the table.\n",
        "- `embedding_dim` (or `n_embd` in some contexts) is the size of the vector representation for each token. It represents the \"depth\" of the table and is a key factor in the model's capacity to capture semantic nuances.\n",
        "\n",
        "### LM Head: Projecting Embeddings to Predict Next Tokens\n",
        "\n",
        "The **language model head (lm_head)** is essentially a projection layer situated at the end of the model. Its role is to transform the output embeddings from the model's last layer back into the vocabulary space, facilitating the prediction of the next token in a sequence. The dimensions of this component are `(embedding_dim, vocab_size)`, mirroring the inverse of the embedding table:\n",
        "\n",
        "- `embedding_dim` here matches the `embedding_dim` of the embedding table, ensuring compatibility in the transformation process.\n",
        "- `vocab_size` is the same as in the embedding table, representing the target space for predictions.\n",
        "\n",
        "### The Envelope Structure: A Conceptual Visualization\n",
        "\n",
        "You can think of the relationship between the embedding table and the lm_head as forming an **envelope structure** in the architecture of a language model. Initially, the embedding table \"expands\" the discrete input tokens into a higher-dimensional, continuous vector space (embedding_dim). This expansion allows the model to process and learn from the semantic intricacies of the language. After processing through the model's layers, the lm_head \"contracts\" these learned representations back into the original vocabulary space, making predictions about the next tokens.\n",
        "\n",
        "This envelope structure is not just a physical manifestation but a conceptual framework that underscores the essence of transforming discrete language symbols into a form that a model can learn from and then translating those learnings back into the language domain.\n",
        "\n",
        "Understanding these dimensions and their roles is pivotal for customizing models to specific tasks, optimizing performance, and innovating on the existing architectures. It highlights the delicate balance between model complexity, computational efficiency, and the capacity to capture and generate nuanced language patterns.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XWHofgu1nZaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Reshaping `(B, T, C)` to `(B*T, C)`\n",
        "\n",
        "In the context of your model:\n",
        "\n",
        "- **B** represents the batch size.\n",
        "- **T** is the sequence length (number of time steps per batch).\n",
        "- **C** is the number of classes (vocabulary size).\n",
        "\n",
        "The output `logits` tensor of shape `(B, T, C)` holds the predictions of the next token at each position in each sequence for each sample in the batch. When you're using the `torch.nn.functional.cross_entropy` loss, the function expects inputs of shape `(N, C)` where:\n",
        "- **N** is the number of samples, and\n",
        "- **C** is the number of classes.\n",
        "\n",
        "To fit this requirement, the logits tensor is reshaped from `(B, T, C)` to `(B*T, C)`, essentially treating each position in each sequence as an independent sample. This allows the loss function to compute the loss for each predicted token against its corresponding true token in `targets`, which is also reshaped to `(B*T)`.\n",
        "\n",
        "Here’s a simple example to illustrate this:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "batch_size = 2\n",
        "seq_length = 3\n",
        "vocab_size = 4\n",
        "\n",
        "# Random sample data\n",
        "logits = torch.randn(batch_size, seq_length, vocab_size)\n",
        "targets = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "\n",
        "# Before reshaping\n",
        "print(\"Original logits shape:\", logits.shape)  # (B, T, C)\n",
        "print(\"Original targets shape:\", targets.shape)  # (B, T)\n",
        "\n",
        "# Reshape logits and targets to fit cross_entropy requirements\n",
        "logits_flat = logits.view(batch_size * seq_length, vocab_size)\n",
        "targets_flat = targets.view(-1)\n",
        "\n",
        "# Compute loss\n",
        "loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "print(\"Loss:\", loss.item())\n",
        "```\n",
        "\n",
        "### Alternative: Keeping the Batch Dimension Intact\n",
        "\n",
        "If you prefer to keep the batch dimension and process each timestep independently, you can use `torch.nn.CrossEntropyLoss`, which can handle inputs in `(N, C, d_1, d_2, ..., d_K)` format when given a target of `(N, d_1, d_2, ..., d_K)` format by setting the `reduction` argument to `mean` or `sum`.\n",
        "\n",
        "Here's how you can apply it:\n",
        "\n",
        "```python\n",
        "# Using CrossEntropyLoss to keep batch dimension\n",
        "loss_fn = nn.CrossEntropyLoss()  # By default, reduction='mean'\n",
        "\n",
        "# Apply loss function without reshaping\n",
        "loss = loss_fn(logits, targets)\n",
        "print(\"Loss without reshaping:\", loss.item())\n",
        "```\n",
        "\n",
        "Using `nn.CrossEntropyLoss` this way allows the loss computation to internally handle the sequence as separate dimensions, keeping your batch structure intact. This method is not only cleaner but also maintains the semantic grouping of data, which can be beneficial in understanding and debugging the model's behavior across different sequences within the batch."
      ],
      "metadata": {
        "id": "AoBA-9HfnaI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The difference between `torch.nn.functional.cross_entropy` and `torch.nn.CrossEntropyLoss`\n",
        "\n",
        "The difference between `torch.nn.functional.cross_entropy` (usually imported as `F.cross_entropy`) and `torch.nn.CrossEntropyLoss` mainly revolves around their usage patterns in PyTorch models. Both perform the same fundamental operation—computing the cross-entropy loss between the input logits and the target classes—but they are used in slightly different contexts.\n",
        "\n",
        "### `torch.nn.functional.cross_entropy`\n",
        "- **Functional API:** `F.cross_entropy` is a stateless function. It computes the cross-entropy loss given logits and targets directly whenever it is called. It does not maintain or update any internal state.\n",
        "- **Usage:** This function is useful in scenarios where you do not need to configure the behavior of the loss function beyond basic parameters provided at each call. It's particularly handy in scripts or simpler models where you want direct control over the computation, or when you're defining a custom training loop without using many of PyTorch's object-oriented features.\n",
        "\n",
        "### `torch.nn.CrossEntropyLoss`\n",
        "- **Class-based API:** `nn.CrossEntropyLoss` is a class that creates a loss function object. This object can be configured during instantiation with specific attributes like `weight`, `size_average`, `ignore_index`, `reduction`, etc., and then it can be used as a callable to compute the loss.\n",
        "- **Stateful:** Since it's an object, it can hold state. This can include things like class weights, making it suitable for datasets with class imbalances.\n",
        "- **Usage:** This class is typically used in more structured or complex models, especially where the same loss computation settings are repeatedly applied across different batches or epochs. It aligns well with object-oriented programming practices, making it ideal for integration into models built as classes.\n",
        "\n",
        "### Choosing Between `F.cross_entropy` and `nn.CrossEntropyLoss`\n",
        "\n",
        "1. **Custom Training Loops:**\n",
        "   - If you are writing a quick, custom training loop and you don't need to repeatedly configure the loss function, `F.cross_entropy` might be more straightforward. It's a one-off call that you can make with different parameters each time without creating an object.\n",
        "\n",
        "2. **Reusable Models and Configurable Loss:**\n",
        "   - For models that will be used in multiple training scenarios, or where you want to configure and possibly share the same loss configuration across different parts of the model or different models, `nn.CrossEntropyLoss` is more suitable. You set up the loss once and use the object throughout.\n",
        "\n",
        "3. **Handling Class Weights and Other Parameters:**\n",
        "   - If your training involves dealing with imbalanced data where you might want to specify weights for different classes to adjust the loss computation, using `nn.CrossEntropyLoss` becomes advantageous. It allows you to specify weights at instantiation and maintain consistent application of these settings.\n",
        "\n",
        "4. **Code Clarity and Maintainability:**\n",
        "   - Using `nn.CrossEntropyLoss` can make code cleaner and easier to maintain, especially in large projects where multiple loss computations might lead to clutter with the functional approach. The class-based approach encapsulates the functionality within an object, making the code more modular and easier to debug.\n",
        "\n",
        "In summary, the choice between using the functional API or the class-based approach often depends on the complexity of your project, the need for reusability and configurability, and personal or project-specific coding standards. Both achieve the same end result but cater to different development environments and preferences."
      ],
      "metadata": {
        "id": "mTMHcluqnehi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Visualize the Embedding Matrix\n",
        "\n",
        "The `nn.Embedding` layer in PyTorch, particularly when thinking about its weight matrix, is a critical component to understand for visualizing and interpreting how tokens are represented in a model. In your example, `nn.Embedding(vocab_size, n_embed)` creates an embedding matrix of shape `(vocab_size, n_embed)`, which translates to a matrix with 65 rows and 10 columns.\n",
        "\n",
        "### How to Visualize the Embedding Matrix\n",
        "\n",
        "1. **Matrix Visualization**:\n",
        "    - You should visualize this as a matrix with 65 rows and 10 columns. Each row corresponds to a unique token in the vocabulary.\n",
        "    - The `vocab_size` (65 in your case) represents the number of unique tokens that can be embedded. Each row is a unique \"embedding vector\" or a representation of that token in a 10-dimensional space.\n",
        "    - The `n_embed` (10) represents the number of features or dimensions each token is represented with. These are the \"coordinates\" of each token in the embedding space.\n",
        "\n",
        "2. **Array of Arrays Visualization**:\n",
        "    - Alternatively, you can think of the embedding matrix as an array of 65 indices, where each index contains an array of 10 items. This view is akin to seeing it as a list of vectors.\n",
        "    - Each vector (or array of 10 items) represents the transformed representation of a corresponding token in a 10-dimensional space.\n",
        "\n",
        "### Correct Mental Model\n",
        "- The first method (Matrix Visualization) is typically more aligned with traditional linear algebra concepts, where each row of a matrix represents a vector in higher-dimensional space. This visualization helps in understanding operations like matrix multiplication that occur during the embedding lookup.\n",
        "- The second method (Array of Arrays Visualization) might be more intuitive if you are thinking in terms of programming structures, particularly if you come from a background where data structures are pivotal.\n",
        "\n",
        "### Practical Example in PyTorch\n",
        "Here's how you might practically interact with such a matrix in a coding context:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 65\n",
        "n_embed = 10\n",
        "\n",
        "# Embedding layer\n",
        "embedding = nn.Embedding(vocab_size, n_embed)\n",
        "\n",
        "# Visualize the weight matrix\n",
        "print(\"Shape of embedding weight matrix:\", embedding.weight.shape)\n",
        "# Output should be torch.Size([65, 10])\n",
        "\n",
        "# To get the embedding vector for the first token\n",
        "first_token_vector = embedding.weight[0, :]\n",
        "print(\"Embedding vector for the first token:\", first_token_vector)\n",
        "```\n",
        "\n",
        "### Conclusion\n",
        "When working with embeddings in models like GPT, visualize the embedding matrix as a table where each row corresponds to a token and each column a feature of the embedding. This mental model will assist in understanding both the transformations applied to these embeddings and their role in the model's architecture, such as when these embeddings are inputted to subsequent layers (like the linear model head in the GPT model)."
      ],
      "metadata": {
        "id": "p6JZioBFlOET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Tensor Dimensions\n",
        "\n",
        "When working with multi-dimensional tensors, especially in the context of machine learning and deep learning, it is very helpful to have a consistent strategy for visualizing tensor shapes. Here’s some strategies to manage higher-dimensional tensors effectively:\n",
        "\n",
        "### Visualizing Tensor Dimensions\n",
        "\n",
        "1. **Last Dimension as Features/Columns**:\n",
        "   - Typically, the last dimension in many tensor operations (especially in PyTorch and similar libraries) represents the feature or channel dimension. For instance, in a 2D tensor (matrix), if the shape is `(8, 10)`, you can think of it as having 8 rows and 10 columns, where each row represents a data point and each column a feature.\n",
        "\n",
        "2. **Second-Last Dimension as Rows/Sequences**:\n",
        "   - The second-last dimension often acts as rows in matrix terminology or as a sequence in contexts like time-series or language models. For tensors used in neural networks, thinking of the second-last dimension as \"rows\" or \"samples\" aligns well with how data is often structured (e.g., batches of data points or sequences).\n",
        "\n",
        "### Handling Higher Dimensions\n",
        "\n",
        "For tensors with more than two dimensions, which is common in deep learning, here’s how to visualize:\n",
        "\n",
        "- **Shape `(1, 8, 10)`**:\n",
        "  - Think of it as one batch containing 8 sequences (or data points), each with 10 features.\n",
        "  - Visualize as a single block of 8 rows and 10 columns.\n",
        "\n",
        "- **Shape `(1, 8, 4, 10)`**:\n",
        "  - This might represent one batch containing 8 sequences, where each sequence is a 4x10 grid (possibly an image or a transformed representation).\n",
        "  - Visualize it as 8 separate blocks (or layers), each block being a 4x10 matrix.\n",
        "\n",
        "### Strategies for Visualization\n",
        "\n",
        "1. **Sketching**:\n",
        "   - Drawing diagrams of the tensors can help, especially when first learning or when explaining concepts to others. Sketch each dimension as a separate axis in a diagram.\n",
        "\n",
        "2. **Nested Lists Concept**:\n",
        "   - Think in terms of nested lists or arrays. For a shape like `(1, 8, 4, 10)`, think of it as a list containing one element, which is a list of 8 elements, where each element is a list of 4 lists, each containing 10 elements.\n",
        "\n",
        "3. **Use Real-world Analogies**:\n",
        "   - Relate tensor dimensions to real-world containers if possible (like boxes within boxes). For images in batches, think of a box (batch) containing several albums (images), where each page (row) of the album shows a sequence of features (columns).\n",
        "\n",
        "4. **Software Tools**:\n",
        "   - Use tensor visualization tools available in libraries like TensorBoard for TensorFlow or equivalents in other ecosystems. These tools can represent high-dimensional data visually and can be particularly enlightening.\n",
        "\n",
        "5. **Consistent Mental Model**:\n",
        "   - Develop a consistent method of breaking down dimensions as you interpret them across different projects. Consistency in how you mentally unpack dimensions will help in quickly understanding and reasoning about the shapes of tensors you encounter."
      ],
      "metadata": {
        "id": "BpgsMaZ-wjn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding (B, T) to (B, T, C) Transition\n",
        "\n",
        "In a tensor of shape `(B, T, C)`:\n",
        "- **B** represents the batch size.\n",
        "- **T** represents the sequence length or time-steps.\n",
        "- **C** represents the number of channels or features per timestep.\n",
        "\n",
        "### Correct Visualization Approach\n",
        "\n",
        "1. **Thinking of B as \"Planes\"**:\n",
        "   - Your concept of thinking about \"B\" as planes is a useful one. In this view, each plane can be seen as a separate entity (such as a data sample or a sequence), and within each plane, you visualize the sequence unfolding.\n",
        "   - This plane analogy is helpful, especially when dealing with images or sequences where each batch element is distinct.\n",
        "\n",
        "2. **Visualizing T (Time/Sequence Dimension)**:\n",
        "   - It's common to conceptualize the sequence or time dimension (T) as horizontal when plotting or imagining sequences over time (like a timeline), but in tensor shape terms, T is often more usefully thought of as the vertical axis in each \"plane\" of B when considering operations like convolution or recurrent processing in neural networks.\n",
        "   - So, for tensor operations, thinking of T as vertical is correct if you align it with rows within each B plane.\n",
        "\n",
        "3. **Interpreting C (Channels or Features)**:\n",
        "   - The last dimension, C, can be thought of as the depth at each point in the sequence. Each element of the sequence (each timestep) has C features, which can be thought of as extending \"downward\" or \"depth-wise\" from each point in the sequence.\n",
        "\n",
        "### Visual Model for (B, T, C)\n",
        "\n",
        "Visualizing `(B, T, C)` effectively means imagining a stack of B matrices (or planes), where each matrix is `T` rows tall and `C` columns wide. Each row in the matrix corresponds to a timestep, and each column within a row corresponds to a feature of that timestep.\n",
        "\n",
        "### Example for Clarity\n",
        "\n",
        "If you imagine processing sentences where each word at each timestep is represented by a vector of features:\n",
        "- **B** would be the number of sentences you're processing simultaneously (batch size).\n",
        "- **T** would be the number of words in each sentence (sequence length).\n",
        "- **C** would be the features representing each word (like embeddings).\n",
        "\n",
        "In this case, each sentence (or each plane) consists of several words (rows, one per word, hence T as vertical), and each word is described by a feature vector (C features deep at each point in the T sequence).\n",
        "\n",
        "### Tips for Practical Visualization\n",
        "\n",
        "- **Sketching Helps**: Draw B as separate matrices or grids, with T and C defining the rows and columns of each grid.\n",
        "- **Use Tensor Manipulation Tools**: Experiment with reshaping tensors in your programming environment to see how changing dimensions affects the arrangement of data.\n",
        "- **Analogies Are Useful**: Relate tensor dimensions to real-world examples where spatial orientation is easier to grasp, like pages in a book or layers in a cake, where each layer/page can have a grid layout.\n"
      ],
      "metadata": {
        "id": "K-5zvaOeel92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Visualization of Tensor Transformations\n",
        "\n",
        "1. **Input Tensor**:\n",
        "   - Shape: `(4, 8)`\n",
        "   - Visualization: Think of this as 4 sequences (or batches), where each sequence consists of 8 integers (token indices). Each integer is between 0 and `vocab_size - 1`.\n",
        "   - Mental Model: 4 planes, each with 8 rows (and implicitly, each row has 1 column here because each entry is a scalar).\n",
        "\n",
        "2. **Embedding Table**:\n",
        "   - Shape: `(65, 10)`\n",
        "   - Visualization: Imagine a table with 65 rows, each corresponding to a token. Each row has 10 columns representing the features of the token (embedding dimensions).\n",
        "   - Mental Model: 65 \"word profiles\" each described by a 10-feature vector.\n",
        "\n",
        "3. **Output After Embedding Lookup**:\n",
        "   - Shape: `(4, 8, 10)`\n",
        "   - Visualization: Now, each of the integers in the input tensor has been transformed into a 10-dimensional vector. So, for each of the 4 sequences, you have 8 tokens represented by 10 features each.\n",
        "   - Mental Model: 4 planes, each with 8 rows of tokens, and each row now extends into 10 columns of features.\n",
        "\n",
        "4. **Processing Through Transformer Blocks**:\n",
        "   - The tensor retains its shape `(4, 8, 10)` through the transformer blocks, assuming no change in dimensionality. The processing here involves complex interactions within and across the feature dimensions but the shape perspective remains the same.\n",
        "\n",
        "5. **The `lm_head` Layer**:\n",
        "   - Weight Matrix Shape: `(10, 65)`\n",
        "   - Bias Shape: `(65,)`\n",
        "   - Visualization: Each of the 10 rows in the weight matrix corresponds to an input feature, and each column (65 in total) corresponds to a token in the output vocabulary.\n",
        "   - Final Output Shape After `lm_head`: `(4, 8, 65)`\n",
        "   - Process: The tensor `(4, 8, 10)` is transformed to `(4, 8, 65)`. Here, for each of the 8 tokens in each of the 4 sequences, the 10-dimensional feature vector is projected onto a 65-dimensional output space (the vocabulary space).\n",
        "   - Matrix Multiplication: Yes, the operation can be thought of as `(4, 8, 10) @ (10, 65)`. The input tensor is on the left, and the weight matrix `W` is on the right during the matrix multiplication in a linear layer. The biases are then added to each resulting 65-dimensional vector.\n",
        "\n",
        "### General Strategy for Visualizing Tensors\n",
        "\n",
        "- **Row and Column Mentality**:\n",
        "  - For lower-dimensional data (2D), rows and columns work well (e.g., sequences or sets of features).\n",
        "  - For higher-dimensional data, think in terms of \"planes\" or \"blocks\". Each higher dimension adds a new \"block\" of data.\n",
        "- **Time as Horizontal vs. Vertical**:\n",
        "  - Conventionally in matrix notation, time (sequence length) can indeed be horizontal. However, in tensor notation, especially in PyTorch, time or sequence length as the second dimension often aligns better with being visualized vertically in each \"batch\" or \"plane\".\n",
        "- **Nested Visualization**:\n",
        "  - Start from the highest dimension and add depth as you go to lower dimensions. For a tensor like `(1, 8, 10)`, think of it as 1 group containing 8 sequences, each sequence containing 10 features."
      ],
      "metadata": {
        "id": "MvLM02Vajk0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the embedding table (`nn.Embedding`) and the linear model head (`nn.Linear`)\n",
        "\n",
        "### Embedding Table (`nn.Embedding`)\n",
        "\n",
        "1. **Purpose**:\n",
        "   - The embedding table converts integer indices (token IDs) into dense vectors. This transformation maps discrete tokens into a continuous, high-dimensional space where similar tokens often have similar vector representations.\n",
        "   - It serves as the initial layer in most NLP models to provide a more meaningful representation of input tokens than their raw indices.\n",
        "\n",
        "2. **Operation**:\n",
        "   - **No Matrix Multiplication During Lookup**: The operation is a lookup, not a matrix multiplication. Each index corresponds directly to a row in the embedding matrix. When an index is provided, the corresponding row (vector) from the embedding matrix is returned. This is more akin to an array indexing operation than a mathematical matrix-vector multiplication.\n",
        "   - Shape transformation: For an input tensor of shape `(B, T)` (batch size by sequence length), the output after passing through an embedding layer of shape `(vocab_size, n_embed)` will be `(B, T, n_embed)`. Each token ID is replaced by its corresponding embedding vector.\n",
        "\n",
        "### Linear Model Head (`nn.Linear`)\n",
        "\n",
        "1. **Purpose**:\n",
        "   - The linear layer, or fully connected layer, is used to transform data from one space to another, often used at the end of networks to map the learned representations to the desired output size (e.g., vocabulary size in language models).\n",
        "   - In the context of transformers, the lm_head is typically used to map the output embeddings of the last transformer block back to the vocabulary space, facilitating the prediction of the next token probabilities.\n",
        "\n",
        "2. **Operation**:\n",
        "   - **Matrix Multiplication**: Unlike the embedding lookup, the linear layer involves a matrix multiplication followed by a bias addition. The input data is multiplied by the weight matrix of the layer, and then a bias is added to each resulting vector.\n",
        "   - Shape transformation: For an input tensor of shape `(B, T, n_embed)`, where `n_embed` is the number of embedding dimensions, the linear layer with a weight matrix of shape `(n_embed, vocab_size)` outputs a tensor of shape `(B, T, vocab_size)`. Each `(n_embed)` vector is transformed into a `(vocab_size)` vector, predicting scores for each vocabulary token.\n",
        "\n",
        "### Matrix Multiplications and Shape Transformations\n",
        "\n",
        "- **Embedding Table**: Does not involve matrix multiplication during its operation. It's a straightforward mapping of indices to vectors based on the pre-trained or learned embeddings stored in a matrix-like structure.\n",
        "- **Linear Layer (lm_head)**: Involves matrix multiplication, transforming learned embeddings or features into outputs (like logits for each token in the vocabulary). The operation can be visualized as `(B, T, n_embed) @ (n_embed, vocab_size) + bias`, resulting in an output of shape `(B, T, vocab_size)`.\n",
        "\n",
        "### Visualizing the Difference\n",
        "\n",
        "- **Embedding Table**: Think of it as a dictionary where each word (token ID) has a specific and fixed vector representation (embedding) that is retrieved directly.\n",
        "- **Linear Layer (lm_head)**: Picture it as a transformation mechanism where every input vector (learned representation of data) is systematically modified (via weights and biases) to produce a new vector in a different space, typically aimed at classification or regression tasks."
      ],
      "metadata": {
        "id": "h6jCMTR_kPJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nn.Linear` examples\n",
        "\n",
        "The `nn.Linear` module in PyTorch is widely used across various types of neural networks. It acts as a fully connected (dense) layer that applies a linear transformation to the incoming data. Below, I will showcase some typical use cases of `nn.Linear` in neural network models and discuss the details of its weights and biases.\n",
        "\n",
        "### Use Cases of `nn.Linear`\n",
        "\n",
        "1. **Single Layer Perceptron**: Often used in simple linear models for regression or binary classification.\n",
        "2. **Hidden Layers in Multilayer Perceptrons (MLP)**: Employed as intermediate layers between the input and output in deep neural networks.\n",
        "3. **Output Layer in Classification Models**: Transforms features to logit scores, which are then passed through a softmax function to derive probabilities for each class.\n",
        "4. **Transforming Feature Dimensions**: Used in applications like dimensionality reduction or feature transformation before other operations, such as in convolutional networks before classification layers.\n",
        "\n",
        "### PyTorch Code Examples\n",
        "\n",
        "Below are examples that demonstrate some of these use cases:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example 1: Using nn.Linear for a simple regression model\n",
        "input_features = 10\n",
        "output_features = 1\n",
        "model = nn.Linear(input_features, output_features)\n",
        "\n",
        "# Example input tensor\n",
        "x = torch.randn(1, input_features)  # One sample with 10 features\n",
        "output = model(x)\n",
        "print(\"Output of single-layer perceptron for regression:\", output)\n",
        "\n",
        "# Example 2: Using nn.Linear in a Multilayer Perceptron for classification\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(10, 50)  # First hidden layer, 10 to 50 features\n",
        "        self.hidden2 = nn.Linear(50, 20)  # Second hidden layer, 50 to 20 features\n",
        "        self.output_layer = nn.Linear(20, 3)  # Output layer for 3 class classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "mlp_model = MLP()\n",
        "x = torch.randn(1, 10)  # One sample, 10 input features\n",
        "output = mlp_model(x)\n",
        "print(\"Output of MLP for classification:\", output)\n",
        "\n",
        "# Example 3: Dimensionality Reduction\n",
        "dim_reduction = nn.Linear(100, 10)  # Reducing dimensions from 100 to 10\n",
        "x = torch.randn(5, 100)  # Batch of 5 samples\n",
        "reduced_x = dim_reduction(x)\n",
        "print(\"Output after dimensionality reduction:\", reduced_x.shape)\n",
        "```\n",
        "\n",
        "### Understanding Bias in `nn.Linear`\n",
        "\n",
        "- **Shape**: The bias in `nn.Linear` is a 1D tensor with a size equal to `output_features`. In the above examples, for the first model, the bias shape would be `(1,)`, and for the output layer in the MLP, it would be `(3,)`.\n",
        "- **Application**: During the linear transformation, the bias is added to each row of the output matrix from the matrix multiplication. This operation is broadcasted across all inputs in the batch.\n",
        "\n",
        "Here’s how the bias is applied conceptually:\n",
        "- **Matrix multiplication**: `output = x @ weight^T + bias`\n",
        "  - Where `x` is the input matrix with shape `(batch_size, input_features)`, and `weight` is transposed to match dimensions.\n",
        "  - The bias is automatically expanded to match the dimensions of `batch_size` and `output_features` and added to each output.\n",
        "\n",
        "This addition of bias is crucial for the model as it provides an additional degree of freedom, allowing the model to fit the data better, especially when the inputs are zero or near-zero. This helps in shifting the activation function, thereby improving the model's ability to learn complex patterns."
      ],
      "metadata": {
        "id": "uaNp4TipsrAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch's `nn.Linear` layer\n",
        "\n",
        "In PyTorch's `nn.Linear` layer, the bias term is added to each row of the output of the matrix multiplication. The bias vector has the same number of elements as the output features (`out_features`), and each element of the bias is added to the corresponding element of every output row.\n",
        "\n",
        "### How Bias is Added\n",
        "\n",
        "When you use a linear layer configured as `(2, 3)` in PyTorch (`nn.Linear(3, 2)`), the matrix multiplication involves a weight matrix of shape `(2, 3)` and inputs of shape `(N, 3)` where `N` is the batch size. The output shape will be `(N, 2)`. Here, the bias will be a vector of length `2`, and each element of this vector is added to the corresponding column of the output.\n",
        "\n",
        "Here's a simple PyTorch example that demonstrates how bias is added:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model with a single linear layer\n",
        "class SimpleLinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleLinearModel, self).__init__()\n",
        "        # Linear layer with 3 input features and 2 output features\n",
        "        self.linear = nn.Linear(3, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the linear layer\n",
        "        return self.linear(x)\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleLinearModel()\n",
        "\n",
        "# Print the weights and bias of the linear layer\n",
        "print(\"Weights of the linear layer:\\n\", model.linear.weight)\n",
        "print(\"Bias of the linear layer:\\n\", model.linear.bias)\n",
        "\n",
        "# Example input tensor of shape (N, 3) where N is the batch size (e.g., N=4)\n",
        "input_tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                             [4.0, 5.0, 6.0],\n",
        "                             [7.0, 8.0, 9.0],\n",
        "                             [10.0, 11.0, 12.0]])\n",
        "\n",
        "# Forward pass to get outputs\n",
        "outputs = model(input_tensor)\n",
        "print(\"Output of the linear layer:\\n\", outputs)\n",
        "```\n",
        "\n",
        "### Explanation of the Code\n",
        "\n",
        "1. **Model Definition**: A simple model containing one `nn.Linear` layer is defined. This layer transforms inputs from 3-dimensional space to 2-dimensional space.\n",
        "\n",
        "2. **Weights and Bias**: The weights (`W`) of the layer are initialized randomly and have a shape of `(2, 3)`, while the bias (`b`) is a vector of shape `(2,)`.\n",
        "\n",
        "3. **Input Tensor**: An input tensor of shape `(4, 3)` is created, representing a batch of 4 samples, each with 3 features.\n",
        "\n",
        "4. **Forward Pass**: When the input is passed through the linear layer, the weight matrix multiplies each input row (shape `(3,)`), and the resulting `(4, 2)` matrix has the bias vector added to each row. This addition is performed automatically by broadcasting the bias vector across all rows of the output.\n",
        "\n",
        "### What Happens Specifically with the Bias\n",
        "\n",
        "Each element of the bias vector is added to each corresponding element of the rows produced by the matrix multiplication of the input with the transpose of the weight matrix. This operation ensures that the bias affects the entire batch uniformly, adjusting the linear transformation appropriately for each output feature."
      ],
      "metadata": {
        "id": "NGEINRpJGrea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rules for Matrix Multiplication (for dummies)\n",
        "\n",
        "Here are some general rules and guidelines for matrix multiplication in machine learning, along with how bias is handled:\n",
        "\n",
        "### Rules for Matrix Multiplication\n",
        "\n",
        "1. **Dimension Matching**:\n",
        "   - For two matrices \\(A\\) and \\(B\\) to be multiplied, the number of columns in \\(A\\) must equal the number of rows in \\(B\\). Mathematically, if \\(A\\) is of size \\(m \\times n\\) and \\(B\\) is of size \\(n \\times p\\), then the matrix multiplication \\(A \\times B\\) (or \\(A@B\\) in Python) is defined and results in a new matrix \\(C\\) of size \\(m \\times p\\).\n",
        "   - If the dimensions do not match (i.e., the columns of \\(A\\) do not match the rows of \\(B\\)), you cannot multiply the matrices directly. You might need to transpose one of the matrices or select different dimensions that do match.\n",
        "\n",
        "2. **Resultant Matrix Shape**:\n",
        "   - The resulting matrix \\(C\\) after the multiplication of \\(A\\) and \\(B\\) will have the number of rows of \\(A\\) and the number of columns of \\(B\\).\n",
        "\n",
        "3. **Element Calculation**:\n",
        "   - Each element \\(c_{ij}\\) of the matrix \\(C\\) is calculated as the dot product of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\). This means \\(c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}\\), where \\(n\\) is the common dimension of \\(A\\) and \\(B\\).\n",
        "\n",
        "### Rules for Adding Bias\n",
        "\n",
        "1. **Bias Shape**:\n",
        "   - In the context of neural networks, when a bias is added to the result of a matrix multiplication, the bias is typically a vector with a length equal to the number of columns of \\(B\\) (or the number of output features of the resulting matrix \\(C\\)).\n",
        "   - The bias shape is therefore \\(1 \\times p\\) when \\(B\\) is \\(n \\times p\\).\n",
        "\n",
        "2. **Broadcasting Bias**:\n",
        "   - The bias is added to each row of the resulting matrix \\(C\\). This operation leverages broadcasting, where the bias vector \\(b\\) of shape \\(1 \\times p\\) is added to every row of \\(C\\), effectively adjusting each element in the row by the corresponding element in the bias vector.\n",
        "   - Specifically, if \\(C\\) is \\(m \\times p\\), then each \\(1 \\times p\\) row of \\(C\\) has the \\(1 \\times p\\) bias vector added to it.\n",
        "\n",
        "### Python Code Example\n",
        "\n",
        "Here’s a simple Python code example to illustrate matrix multiplication and bias addition using NumPy:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Define matrices A and B\n",
        "A = np.array([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n",
        "B = np.array([[1, 4], [2, 5], [3, 6]])  # Shape (3, 2)\n",
        "\n",
        "# Multiply matrices\n",
        "C = np.dot(A, B)  # Alternatively, use A @ B in Python\n",
        "print(\"Matrix C (result of A @ B):\\n\", C)\n",
        "\n",
        "# Define bias\n",
        "bias = np.array([1, 2])  # Shape (2,)\n",
        "\n",
        "# Add bias to each row of matrix C\n",
        "C_plus_bias = C + bias  # Broadcasting bias across each row\n",
        "print(\"Matrix C after adding bias:\\n\", C_plus_bias)\n",
        "```\n",
        "\n",
        "### Summary\n",
        "\n",
        "These rules encapsulate the core principles of matrix operations in machine learning, specifically highlighting how dimensions must align for multiplication and how biases adjust the outputs, playing a crucial role in neural network layers. Understanding and applying these rules helps in designing and debugging neural network architectures effectively."
      ],
      "metadata": {
        "id": "jv2V1jHkDvQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define matrices A and B\n",
        "A = np.array([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n",
        "B = np.array([[1, 4], [2, 5], [3, 6]])  # Shape (3, 2)\n",
        "\n",
        "# Multiply matrices\n",
        "C = np.dot(A, B)  # Alternatively, use A @ B in Python\n",
        "print(\"Matrix C (result of A @ B):\\n\", C)\n",
        "\n",
        "# Define bias\n",
        "bias = np.array([1, 2])  # Shape (2,)\n",
        "\n",
        "# Add bias to each row of matrix C\n",
        "C_plus_bias = C + bias  # Broadcasting bias across each row\n",
        "print(\"Matrix C after adding bias:\\n\", C_plus_bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgr3C91CFDGK",
        "outputId": "c42e6d13-3933-4256-c06c-5086c4c93e6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix C (result of A @ B):\n",
            " [[14 32]\n",
            " [32 77]]\n",
            "Matrix C after adding bias:\n",
            " [[15 34]\n",
            " [33 79]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the main uses of bias in machine learning\n",
        "\n",
        "Bias terms play a crucial role in machine learning, particularly in neural networks, by providing an additional degree of freedom in model training and influencing the behavior of the activation functions applied to neurons. Let's discuss the main uses of bias in machine learning in detail, highlighting your example as one of the primary roles.\n",
        "\n",
        "### 1. **Offsetting the Input to Activation Functions**\n",
        "\n",
        "As you mentioned, one of the most critical roles of the bias is to adjust the input to the activation functions. This adjustment can be pivotal in determining the behavior of the activation function:\n",
        "\n",
        "- **Threshold Shifting**: Bias allows the activation thresholds of neurons to be shifted. For example, in a neuron with a ReLU activation function (`ReLU(x) = max(0, x)`), a negative bias can make it harder for the neuron to activate (output a non-zero value), as the bias would need to be overcome by positive input values. Conversely, a positive bias can make it easier for the neuron to activate by effectively lowering the threshold at which the activation function starts producing a non-zero output.\n",
        "- **Avoiding Dead Neurons in ReLU**: In the context of ReLU activation functions, without a proper bias, a significant number of neurons can end up never activating (a problem known as \"dying ReLU\"). By adjusting the bias, it's possible to ensure that more neurons fire, thus maintaining healthy gradients and improving the learning capabilities of the network.\n",
        "\n",
        "### 2. **Improving Model Flexibility and Fit**\n",
        "\n",
        "Bias increases the flexibility of the model to fit the data:\n",
        "\n",
        "- **Non-zero Output at Zero Input**: Without a bias, a linear model or a neural network layer would be forced to pass through the origin (zero output when input is zero), which can be highly restrictive. Bias terms allow the model to output non-zero values even when the input is zero, which can lead to a better fit, especially in cases where the data does not naturally center around the origin.\n",
        "- **Adjusting Decision Boundaries**: In classification tasks, bias terms help in shifting decision boundaries. For example, in a simple binary classifier like logistic regression, the bias term shifts the decision boundary away from the origin, allowing for more accurate classification when data classes are not symmetrically distributed about the origin.\n",
        "\n",
        "### 3. **Stabilizing and Accelerating Convergence**\n",
        "\n",
        "Bias terms can help in stabilizing and sometimes accelerating the convergence of learning algorithms:\n",
        "\n",
        "- **Initialization**: Proper initialization of bias can lead to a more stable start in the training process. For instance, initializing biases to a small positive value can help avoid initial dead neurons in networks using ReLU activations.\n",
        "- **Gradient Flow**: Biases affect the gradients during backpropagation. By properly managing bias values, it is possible to maintain healthier gradient flows across deep networks, which can prevent issues like vanishing and exploding gradients.\n",
        "\n",
        "### 4. **Model Complexity and Overfitting**\n",
        "\n",
        "While primarily adding flexibility, biases also contribute to the overall parameter count:\n",
        "\n",
        "- **Regulation and Overfitting**: Just like weights, biases need to be regularized. Overfitting can occur if biases are too large, leading to overly complex models that react too strongly to particular features of the input data.\n",
        "\n",
        "### Practical Example\n",
        "\n",
        "Here’s a quick example using PyTorch to illustrate the effect of bias in a neural network layer:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple model with one linear layer followed by a ReLU\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear = nn.Linear(2, 1)  # Two input features to one output\n",
        "        # Initialize bias such that the neuron has a negative threshold\n",
        "        self.linear.bias.data.fill_(-1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "# Instantiate the model and provide an example input\n",
        "model = SimpleNN()\n",
        "input_tensor = torch.tensor([[0.5, 0.5]])\n",
        "output = model(input_tensor)\n",
        "print(\"Output with bias affecting activation:\", output)\n",
        "```\n",
        "\n",
        "In this example, the bias is initialized to `-1.0`, which means the input sum must exceed 1.0 for the ReLU to activate, showcasing how bias can control neuron activation thresholds.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Bias terms are fundamental components that enhance the functional power of machine learning models by providing additional degrees of freedom, influencing activation functions, and aiding in the model's ability to generalize from training data. Proper management and tuning of bias parameters are crucial for building effective neural network architectures."
      ],
      "metadata": {
        "id": "9eva9frEEuta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple model with one linear layer followed by a ReLU\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear = nn.Linear(2, 1)  # Two input features to one output\n",
        "        # Initialize bias such that the neuron has a negative threshold\n",
        "        self.linear.bias.data.fill_(-1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "# Instantiate the model and provide an example input\n",
        "model = SimpleNN()\n",
        "input_tensor = torch.tensor([[0.5, 0.5]])\n",
        "output = model(input_tensor)\n",
        "print(\"Output with bias affecting activation:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIvKkgMaEiAG",
        "outputId": "2a564cea-71ef-4c2a-af2c-e24d99f2fce3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output with bias affecting activation: tensor([[0.]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple model with one linear layer followed by a ReLU\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear = nn.Linear(2, 1)  # Two input features to one output\n",
        "        # Initialize bias such that the neuron has a negative threshold\n",
        "        self.linear.bias.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "# Instantiate the model and provide an example input\n",
        "model = SimpleNN()\n",
        "input_tensor = torch.tensor([[0.5, 0.5]])\n",
        "output = model(input_tensor)\n",
        "print(\"Output with bias affecting activation:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZYhOnkmEoUH",
        "outputId": "72a431b1-1d88-4c5a-c901-f3997cc5b043"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output with bias affecting activation: tensor([[1.0063]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical tricks with matrices\n",
        "\n",
        "Matrix multiplication is not just a basic linear algebra operation; it's a powerful tool packed with possibilities for manipulating and transforming data, especially in computational settings like programming and data science. By carefully designing or modifying the matrices involved—particularly the second matrix in a multiplication, \\(B\\), as you noted—you can leverage a number of mathematical \"tricks\" or techniques. Here are several key methods and their practical implications:\n",
        "\n",
        "### 1. **Summing Rows of \\(A\\)**\n",
        "\n",
        "If you want to sum all the rows of matrix \\(A\\), you can multiply \\(A\\) by a column vector \\(B\\) where every element of \\(B\\) is 1.\n",
        "\n",
        "- **Matrix Setup**: Let \\(A\\) be a matrix of size \\(m \\times n\\) and \\(B\\) a column vector of size \\(n \\times 1\\) with all elements being 1.\n",
        "- **Operation**: \\( A \\times B \\)\n",
        "- **Result**: The resulting vector will be of size \\(m \\times 1\\) where each element is the sum of the corresponding row in \\(A\\).\n",
        "\n",
        "### 2. **Calculating Column Averages of \\(A\\)**\n",
        "\n",
        "To find the average of each column in matrix \\(A\\), multiply \\(A\\) by a column vector \\(B\\) where each element is \\(1/n\\) (where \\(n\\) is the number of columns in \\(A\\)).\n",
        "\n",
        "- **Matrix Setup**: \\(A\\) is \\(m \\times n\\), \\(B\\) is \\(n \\times 1\\) where each element is \\(1/n\\).\n",
        "- **Operation**: \\( A \\times B \\)\n",
        "- **Result**: A vector of size \\(m \\times 1\\) where each element is the average of the rows in \\(A\\).\n",
        "\n",
        "### 3. **Accumulating Values Across Columns**\n",
        "\n",
        "To accumulate (sum up) all values across the columns of \\(A\\) into a single sum, multiply \\(A\\) by a vector \\(B\\) where all elements are 1, and then sum up all the elements of the resulting vector.\n",
        "\n",
        "- **Matrix Setup**: \\(A\\) is \\(m \\times n\\), \\(B\\) is \\(n \\times 1\\) filled with 1s.\n",
        "- **Operation**: \\( C = A \\times B \\) then sum all elements of \\(C\\).\n",
        "- **Result**: A single scalar that is the sum of all elements in \\(A\\).\n",
        "\n",
        "### 4. **Transforming Data by Weighting**\n",
        "\n",
        "You can apply different weights to the columns of \\(A\\) by using a diagonal matrix \\(B\\) where diagonal elements represent the weights.\n",
        "\n",
        "- **Matrix Setup**: \\(A\\) is \\(m \\times n\\), \\(B\\) is \\(n \\times n\\) diagonal matrix with weights as diagonal elements.\n",
        "- **Operation**: \\( A \\times B \\)\n",
        "- **Result**: A matrix where each column of \\(A\\) has been scaled by the corresponding weight.\n",
        "\n",
        "### 5. **Projection Operations**\n",
        "\n",
        "Projecting data onto lower dimensions for operations like PCA or for simplifying models can be achieved by matrix multiplication. \\(B\\) would typically contain the projection vectors.\n",
        "\n",
        "- **Matrix Setup**: \\(A\\) is \\(m \\times n\\), \\(B\\) is \\(n \\times k\\) where \\(k < n\\) representing lower-dimensional space.\n",
        "- **Operation**: \\( A \\times B \\)\n",
        "- **Result**: A matrix of dimension \\(m \\times k\\) representing the data in \\(A\\) projected onto a \\(k\\)-dimensional subspace.\n",
        "\n",
        "---\n",
        "\n",
        "### Useful calculations such as summing rows, calculating averages, and transforming data using weights.\n",
        "\n",
        "```\n",
        "\n",
        "### 1. Summing Rows of \\(A\\)\n",
        "\n",
        "Here's how you can sum all the rows of a matrix \\(A\\) using NumPy:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Create a matrix A\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9]])\n",
        "\n",
        "# Create a vector B with all elements being 1\n",
        "B = np.ones((3, 1))\n",
        "\n",
        "# Multiply A by B\n",
        "sum_rows = np.dot(A, B)\n",
        "\n",
        "print(\"Sum of each row in A:\\n\", sum_rows)\n",
        "```\n",
        "\n",
        "### 2. Calculating Column Averages of \\(A\\)\n",
        "\n",
        "To calculate the average of each column in a matrix \\(A\\), you can scale the vector \\(B\\) by \\(1/n\\):\n",
        "\n",
        "```python\n",
        "# Number of columns in A\n",
        "n = A.shape[1]\n",
        "\n",
        "# Create a vector B with each element being 1/n\n",
        "B = np.full((3, 1), 1/n)\n",
        "\n",
        "# Multiply A by B\n",
        "column_averages = np.dot(A, B)\n",
        "\n",
        "print(\"Average of each column in A:\\n\", column_averages)\n",
        "```\n",
        "\n",
        "### 3. Accumulating Values Across Columns\n",
        "\n",
        "Here's how to accumulate all values in matrix \\(A\\):\n",
        "\n",
        "```python\n",
        "# Create a vector B with all elements being 1\n",
        "B = np.ones((3, 1))\n",
        "\n",
        "# Multiply A by B\n",
        "column_sums = np.dot(A, B)\n",
        "\n",
        "# Sum all elements of the resulting vector\n",
        "total_sum = np.sum(column_sums)\n",
        "\n",
        "print(\"Total sum of all elements in A:\", total_sum)\n",
        "```\n",
        "\n",
        "### 4. Transforming Data by Weighting\n",
        "\n",
        "To apply different weights to the columns of \\(A\\):\n",
        "\n",
        "```python\n",
        "# Create a diagonal matrix B with weights on the diagonal\n",
        "B = np.diag([0.5, 1.0, 1.5])\n",
        "\n",
        "# Multiply A by B\n",
        "weighted_A = np.dot(A, B)\n",
        "\n",
        "print(\"Weighted A:\\n\", weighted_A)\n",
        "```\n",
        "\n",
        "### 5. Projection Operations\n",
        "\n",
        "Projecting data onto lower dimensions can be achieved as follows:\n",
        "\n",
        "```python\n",
        "# Assume A is m x n and we want to project onto a 2-dimensional space\n",
        "# Create a projection matrix B with size n x 2\n",
        "B = np.random.rand(3, 2)  # Randomly chosen projection vectors\n",
        "\n",
        "# Multiply A by B\n",
        "projected_A = np.dot(A, B)\n",
        "\n",
        "print(\"Projected A onto a 2-dimensional space:\\n\", projected_A)\n",
        "```\n",
        "\n",
        "These examples showcase how you can leverage matrix multiplication in NumPy to perform various mathematical operations efficiently, exploiting linear algebra for data transformations and summarizations in practical settings."
      ],
      "metadata": {
        "id": "1G2e5xP_KVzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a matrix A\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9]])\n",
        "\n",
        "# Create a vector B with all elements being 1\n",
        "B = np.ones((3, 1))\n",
        "\n",
        "# Multiply A by B\n",
        "sum_rows = np.dot(A, B)\n",
        "\n",
        "print(\"B:\\n\", B)\n",
        "print(\"Sum of each row in A:\\n\", sum_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAbChgWYKZlN",
        "outputId": "3dca3c54-ed07-4b69-b09a-641ea4b55e03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B:\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Sum of each row in A:\n",
            " [[ 6.]\n",
            " [15.]\n",
            " [24.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of columns in A\n",
        "n = A.shape[1]\n",
        "\n",
        "# Create a vector B with each element being 1/n\n",
        "B = np.full((3, 1), 1/n)\n",
        "\n",
        "# Multiply A by B\n",
        "column_averages = np.dot(A, B)\n",
        "print(\"B:\\n\", B)\n",
        "print(\"Average of each column in A:\\n\", column_averages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKiVi8XAKk5G",
        "outputId": "1b8c056c-cbbd-474d-c07f-2494cb1906f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B:\n",
            " [[0.33333333]\n",
            " [0.33333333]\n",
            " [0.33333333]]\n",
            "Average of each column in A:\n",
            " [[2.]\n",
            " [5.]\n",
            " [8.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector B with all elements being 1\n",
        "B = np.ones((3, 1))\n",
        "\n",
        "# Multiply A by B\n",
        "column_sums = np.dot(A, B)\n",
        "\n",
        "# Sum all elements of the resulting vector\n",
        "total_sum = np.sum(column_sums)\n",
        "\n",
        "print(\"B:\", B)\n",
        "print(\"Total sum of all elements in A:\", total_sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju9lJdTBLKM0",
        "outputId": "4cdf374f-0735-434e-f1db-e9a781d7d682"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B: [[1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Total sum of all elements in A: 45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a diagonal matrix B with weights on the diagonal\n",
        "B = np.diag([0.5, 1.0, 1.5])\n",
        "\n",
        "# Multiply A by B\n",
        "weighted_A = np.dot(A, B)\n",
        "\n",
        "print(\"A:\\n\", A)\n",
        "print(\"B:\\n\", B)\n",
        "print(\"Weighted A:\\n\", weighted_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lV-ekcGLiM8",
        "outputId": "e5c6a79f-b4b6-4472-b4d4-ec1efdf8a94f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            " [[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "B:\n",
            " [[0.5 0.  0. ]\n",
            " [0.  1.  0. ]\n",
            " [0.  0.  1.5]]\n",
            "Weighted A:\n",
            " [[ 0.5  2.   4.5]\n",
            " [ 2.   5.   9. ]\n",
            " [ 3.5  8.  13.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume A is m x n and we want to project onto a 2-dimensional space\n",
        "# Create a projection matrix B with size n x 2\n",
        "B = np.random.rand(3, 2)  # Randomly chosen projection vectors\n",
        "\n",
        "# Multiply A by B\n",
        "projected_A = np.dot(A, B)\n",
        "\n",
        "print(\"B:\\n\", B)\n",
        "print(\"Projected A onto a 2-dimensional space:\\n\", projected_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sRFgdHTL8yo",
        "outputId": "7c3d1d74-93c3-4694-9b53-a520c7d9abd1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B:\n",
            " [[0.6329597  0.57643139]\n",
            " [0.34358579 0.07585142]\n",
            " [0.09522777 0.53817742]]\n",
            "Projected A onto a 2-dimensional space:\n",
            " [[1.6058146  2.34266647]\n",
            " [4.82113438 5.91404714]\n",
            " [8.03645417 9.4854278 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: Embedding Layer and Output Linear Transformation (Tensor shapes)\n",
        "\n",
        "Focusing on the data, weight matrices, bias, and tensor shapes at each step of the model. This step-by-step breakdown will make it easier to grasp the structure and operation of your model.\n",
        "\n",
        "### Input\n",
        "\n",
        "1. **Input Shape**:\n",
        "   - **Shape**: `(4, 8)`\n",
        "   - **Description**: This represents a batch of 4 sequences, each containing 8 tokens. Each token is represented by an integer index ranging from 0 to 64 (inclusive).\n",
        "\n",
        "### Embedding Layer\n",
        "\n",
        "2. **Embedding Table** (`nn.Embedding`):\n",
        "   - **Module**: `nn.Embedding(65, 10)`\n",
        "   - **Weight Matrix Shape**: `(65, 10)`\n",
        "   - **Operation**: Maps each token index to a 10-dimensional embedding vector.\n",
        "   - **Output Shape**: `(4, 8, 10)`\n",
        "   - **Description**: The output after the embedding layer is a tensor where each token index from the input is replaced by its corresponding 10-dimensional embedding. Each sequence now is represented as a matrix of size `8 x 10`.\n",
        "\n",
        "### Linear Model Head (lm_head)\n",
        "\n",
        "3. **Linear Layer** (`nn.Linear`):\n",
        "   - **Module**: `nn.Linear(10, 65)`\n",
        "   - **Weight Shape**: `(65, 10)`\n",
        "   - **Bias Shape**: `(65,)`\n",
        "   - **Operation**: Transforms the embedding vector of dimension 10 back into the vocabulary space of dimension 65, effectively calculating logits for each token in the vocabulary.\n",
        "   - **Output Shape**: `(4, 8, 65)`\n",
        "   - **Description**: The output after the `lm_head` is a tensor where each 10-dimensional embedding vector is transformed into a 65-dimensional vector representing the raw scores (logits) for each vocabulary token. This output shape corresponds to each of the 8 positions in each of the 4 sequences now having a vector of length 65, which encodes the likelihood of each token being the next token.\n",
        "\n",
        "### Detailed Breakdown of Operations\n",
        "\n",
        "- **Embedding Lookup**:\n",
        "  - For each token index in the input shape `(4, 8)`, the embedding layer looks up the corresponding 10-dimensional vector in its weight matrix of shape `(65, 10)`.\n",
        "- **Matrix Multiplication and Bias Addition in lm_head**:\n",
        "  - The embedding output `(4, 8, 10)` is transformed by the linear layer. This involves a matrix multiplication between each `(10)` vector and the weight matrix `(65, 10)`. The resulting shape is `(4, 8, 65)`.\n",
        "  - The bias vector of shape `(65,)` is added to each `(65)` vector in the output, utilizing broadcasting to match dimensions. This means that the same bias values are added to the outputs of all positions in all sequences, effectively shifting the logits.\n",
        "\n",
        "### Visualization of Tensor Transformations\n",
        "\n",
        "- **Input to Embedding**:\n",
        "  - Input: Indices -> Embedding Lookup -> 10D Vectors\n",
        "- **Embedding to lm_head**:\n",
        "  - 10D Embedding Vectors -> Linear Transformation (Weight Multiplication + Bias Addition) -> 65D Logits\n",
        "\n",
        "This structured presentation clarifies how data is transformed through your model, detailing the roles and interactions of weights, biases, and tensor dimensions. This approach not only aids in understanding the model's function but also prepares you for debugging and potentially scaling or modifying the model architecture."
      ],
      "metadata": {
        "id": "bqSoQfGCSRFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  how the `dim` parameter works in PyTorch\n",
        "\n",
        "### Understanding `dim` in PyTorch\n",
        "\n",
        "In PyTorch, when you specify a `dim` parameter for operations like `softmax`, you're indicating which dimension of the input tensor should be used to compute the function such that the operation is applied across that dimension.\n",
        "\n",
        "- **Matrix [n x m]**: Think of it as `n` rows and `m` columns.\n",
        "- **dim=0**: The operation will be applied to each column across all rows.\n",
        "- **dim=1**: The operation will be applied to each row across all columns.\n",
        "\n",
        "### What does `dim=1` mean?\n",
        "\n",
        "For a matrix `[n x m]` (say, `[[1, 2, 3], [4, 5, 6]]`):\n",
        "- **dim=0**: Collapses each column across the rows. Think of squishing the matrix vertically.\n",
        "- **dim=1**: Collapses each row across the columns. Think of squishing the matrix horizontally.\n",
        "\n",
        "When applying `softmax` or any similar function with `dim=1`, it means the function processes each row individually, treating each row as a separate set of inputs. In many neural network operations, this is common for treating each row as a separate data point (or batch item) with multiple features.\n",
        "\n",
        "### Simple Examples\n",
        "\n",
        "Let's see how this works with actual PyTorch code:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a 2x3 matrix\n",
        "A = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                  [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Apply softmax with dim=1\n",
        "softmax_A_dim1 = F.softmax(A, dim=1)\n",
        "print(\"Softmax over dim=1:\\n\", softmax_A_dim1)\n",
        "\n",
        "# Apply softmax with dim=0\n",
        "softmax_A_dim0 = F.softmax(A, dim=0)\n",
        "print(\"Softmax over dim=0:\\n\", softmax_A_dim0)\n",
        "```\n",
        "\n",
        "**Output Explanation**:\n",
        "- **Softmax with `dim=1`**: Processes each row independently, converting the numbers in each row into probabilities that sum to 1. This treats each row as a separate set of class scores in a classification task.\n",
        "- **Softmax with `dim=0`**: Processes each column independently, converting the numbers in each column into probabilities that sum to 1. This is less common but might be used in specific contexts where columns represent different classes or categories.\n"
      ],
      "metadata": {
        "id": "A_FmjRldsziP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `dim` in PyTorch\n",
        "\n",
        "There are two distinct types of operations based on how they handle tensor dimensions when applying a function across a specified `dim` in PyTorch. Let's discuss the two function types in detail and explore how the `dim` parameter affects their behavior:\n",
        "\n",
        "### 1. **Functions that Preserve Tensor Shape (e.g., `softmax`)**\n",
        "\n",
        "**Softmax** is a classic example of a function where the tensor shape remains unchanged. It is primarily used to convert raw scores (logits) into probabilities which are easier to interpret and are required for tasks such as classification. Each element is transformed based on other elements along the specified dimension, but the overall shape of the tensor doesn't change.\n",
        "\n",
        "- **`dim=1` Example**:\n",
        "  - When applied with `dim=1` on a tensor, `softmax` processes each row independently. For a tensor `A` of shape `[n, m]`:\n",
        "    ```python\n",
        "    A = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                      [4.0, 5.0, 6.0]])\n",
        "    softmax_A = torch.nn.functional.softmax(A, dim=1)\n",
        "    ```\n",
        "    Each row in `A` is treated as a separate group of logits, and `softmax` converts them into probabilities that sum to 1 across each row. The shape remains `[n, m]`.\n",
        "\n",
        "- **`dim=0` Example**:\n",
        "  - When `softmax` is applied with `dim=0`, it processes each column across rows:\n",
        "    ```python\n",
        "    softmax_A = torch.nn.functional.softmax(A, dim=0)\n",
        "    ```\n",
        "    Each column of `A` is now treated separately, converting values into probabilities that sum to 1 down each column. Again, the shape remains `[n, m]`.\n",
        "\n",
        "### 2. **Functions that Change Tensor Shape (e.g., `sum`)**\n",
        "\n",
        "**Sum** is an operation that aggregates elements along the specified dimension, resulting in a tensor whose shape is reduced in that dimension.\n",
        "\n",
        "- **`dim=1` Example**:\n",
        "  - Applying `sum` with `dim=1` collapses the rows (sums across them), reducing the dimensionality of the tensor:\n",
        "    ```python\n",
        "    sum_A = A.sum(dim=1)\n",
        "    ```\n",
        "    For the tensor `A` with shape `[n, m]`, the result of `sum_A` will have the shape `[n]`. Each element of the resulting tensor is the sum of elements across the corresponding row in `A`.\n",
        "\n",
        "- **`dim=0` Example**:\n",
        "  - Similarly, applying `sum` with `dim=0` collapses the columns (sums down them):\n",
        "    ```python\n",
        "    sum_A = A.sum(dim=0)\n",
        "    ```\n",
        "    The output `sum_A` will have the shape `[m]`. Each element of `sum_A` is the sum of elements down the corresponding column in `A`.\n",
        "\n",
        "### Visual Memory Aid: \"Collapsing or Expanding?\"\n",
        "\n",
        "- **Preserve Shape (Expanding)**: Think of operations like `softmax` as **expanding** the information across the dimension without changing the structure—each element affects others along that dimension but retains the \"space\" (shape).\n",
        "- **Change Shape (Collapsing)**: Operations like `sum` **collapse** the tensor along the specified dimension, reducing its dimensionality by aggregating values along it.\n",
        "\n",
        "This framework not only aids in understanding the effect of these operations on data structure but also helps in deciding which operation to use based on the desired outcome in data transformation and analysis tasks."
      ],
      "metadata": {
        "id": "EywvAU47uy4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.sum()`\n",
        "\n",
        "The `torch.sum()` function in PyTorch is a versatile tool for summing elements of a tensor across specified dimensions. The parameters `dim` and `keepdim` influence how the summation is performed and the shape of the output. Below, I'll provide examples that demonstrate how these parameters work.\n",
        "\n",
        "### Basic Summation without Specifying `dim`\n",
        "\n",
        "First, let's start with the simplest use of `torch.sum()` where no dimension is specified. The function sums all elements of the tensor, reducing it to a single scalar value.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "total_sum = torch.sum(A)\n",
        "print(\"Total sum of all elements:\", total_sum)  # Output: 21\n",
        "```\n",
        "\n",
        "### Summation with `dim` Parameter\n",
        "\n",
        "The `dim` parameter specifies the dimension along which to sum, reducing the size of that dimension to 1 unless `keepdim` is set to `True`.\n",
        "\n",
        "#### Example 1: Summing Along a Dimension\n",
        "\n",
        "```python\n",
        "# Sum along columns (dim=0)\n",
        "column_sum = torch.sum(A, dim=0)\n",
        "print(\"Sum of each column:\", column_sum)  # Output: [5, 7, 9]\n",
        "\n",
        "# Sum along rows (dim=1)\n",
        "row_sum = torch.sum(A, dim=1)\n",
        "print(\"Sum of each row:\", row_sum)  # Output: [6, 15]\n",
        "```\n",
        "\n",
        "#### Example 2: Using `keepdim=True`\n",
        "\n",
        "When `keepdim=True`, the output tensor retains the same number of dimensions as the input, though the dimension summed over is of size 1.\n",
        "\n",
        "```python\n",
        "# Sum along columns but keep the dimensions\n",
        "column_sum_keepdim = torch.sum(A, dim=0, keepdim=True)\n",
        "print(\"Sum of each column with same dimensions:\", column_sum_keepdim)  # Output: [[5, 7, 9]]\n",
        "print(\"Shape:\", column_sum_keepdim.shape)  # Output: torch.Size([1, 3])\n",
        "\n",
        "# Sum along rows but keep the dimensions\n",
        "row_sum_keepdim = torch.sum(A, dim=1, keepdim=True)\n",
        "print(\"Sum of each row with same dimensions:\", row_sum_keepdim)  # Output: [[6], [15]]\n",
        "print(\"Shape:\", row_sum_keepdim.shape)  # Output: torch.Size([2, 1])\n",
        "```\n",
        "\n",
        "### Explanation and Visualization\n",
        "\n",
        "- **`dim=0`**: Summing along the first dimension (columns) collapses the columns (vertical sum). Each element in the output corresponds to the sum of elements in that column across all rows.\n",
        "- **`dim=1`**: Summing along the second dimension (rows) collapses the rows (horizontal sum). Each element in the output is the sum of elements in that row across all columns.\n",
        "- **`keepdim=True`**: By keeping dimensions, the output tensor retains the number of dimensions of the input tensor, making it easier to align this output with other tensors for further computations that require matching dimensions.\n",
        "\n",
        "These examples illustrate how `torch.sum()` can be used in various scenarios to compute sums across different dimensions while controlling the shape of the output. This flexibility is crucial in data processing pipelines, especially when dealing with multidimensional data in neural network operations."
      ],
      "metadata": {
        "id": "cEN_1XlJP-iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "total_sum = torch.sum(A)\n",
        "print(\"Total sum of all elements:\", total_sum)  # Output: 21\n",
        "\n",
        "print(\"\\n-----\")\n",
        "# Sum along columns (dim=0)\n",
        "column_sum = torch.sum(A, dim=0)\n",
        "print(\"Sum of each column:\", column_sum)  # Output: [5, 7, 9]\n",
        "print(\"Shape:\", column_sum.shape)  # Output: torch.Size([3])\n",
        "\n",
        "# Sum along columns but keep the dimensions\n",
        "column_sum_keepdim = torch.sum(A, dim=0, keepdim=True)\n",
        "print(\"Sum of each column with same dimensions:\", column_sum_keepdim)  # Output: [[5, 7, 9]]\n",
        "print(\"Shape:\", column_sum_keepdim.shape)  # Output: torch.Size([1, 3])\n",
        "\n",
        "print(\"\\n-----\")\n",
        "# Sum along rows (dim=1)\n",
        "row_sum = torch.sum(A, dim=1)\n",
        "print(\"Sum of each row:\", row_sum)  # Output: [6, 15]\n",
        "print(\"Shape:\", row_sum.shape)  # Output: torch.Size([2])\n",
        "\n",
        "# Sum along rows but keep the dimensions\n",
        "row_sum_keepdim = torch.sum(A, dim=1, keepdim=True)\n",
        "print(\"Sum of each row with same dimensions:\", row_sum_keepdim)  # Output: [[6], [15]]\n",
        "print(\"Shape:\", row_sum_keepdim.shape)  # Output: torch.Size([2, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGYwLSkfQG9a",
        "outputId": "07a5418c-99b0-4905-b566-9707d7d09b06"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sum of all elements: tensor(21)\n",
            "\n",
            "-----\n",
            "Sum of each column: tensor([5, 7, 9])\n",
            "Shape: torch.Size([3])\n",
            "Sum of each column with same dimensions: tensor([[5, 7, 9]])\n",
            "Shape: torch.Size([1, 3])\n",
            "\n",
            "-----\n",
            "Sum of each row: tensor([ 6, 15])\n",
            "Shape: torch.Size([2])\n",
            "Sum of each row with same dimensions: tensor([[ 6],\n",
            "        [15]])\n",
            "Shape: torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `model.to(device)`\n",
        "\n",
        "In PyTorch, when you use the `.to(device)` method to move a model or tensors to a specific device (like GPU or CPU), the method returns a new object that resides on the specified device. This is crucial to understand because the operation itself does not mutate the model in-place but instead returns a new model on the specified device.\n",
        "\n",
        "Here’s the breakdown of the best practice for each option you listed:\n",
        "\n",
        "### 1. `model.to(device)`\n",
        "- This statement moves the `model` to the specified `device`, but it does not update the `model` variable itself. Therefore, if you only write `model.to(device)` without reassigning it back to `model` or another variable, the original `model` will remain on the original device (typically CPU).\n",
        "- This can lead to confusion because subsequent operations on `model` might still use the CPU, potentially causing device mismatches especially when combining it with tensors on a different device.\n",
        "\n",
        "### 2. `model = model.to(device)`\n",
        "- This is generally considered the best practice. It explicitly updates `model` to be the version that is on the new `device`. This ensures that all subsequent operations on `model` are performed on the correct device, and it makes the code clearer and less error-prone.\n",
        "- This method clearly communicates that the model has been moved and is now being referenced by the original variable name on the new device.\n",
        "\n",
        "### 3. `m = model.to(device)`\n",
        "- This approach is also valid and works similarly to the second option, but it assigns the device-transferred model to a new variable `m`. This can be useful if you want to maintain the original model on the CPU while also using a GPU-transferred version for certain operations.\n",
        "- However, it can introduce complexity and potential bugs if you're not careful with how you manage and distinguish between `model` and `m` throughout your code.\n",
        "\n",
        "### Recommended Approach and Example\n",
        "Based on clarity and safety, the best practice is typically:\n",
        "\n",
        "```python\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "This practice ensures that your model is moved to the appropriate device, and all subsequent operations on the model are performed on that device, reducing the risk of device mismatch errors. Here is how you should structure your example:\n",
        "\n",
        "```python\n",
        "# Example usage\n",
        "# Assume 'vocab_size', 'n_embed', 'device' are defined\n",
        "model = GPT(vocab_size=65, n_embed=10)\n",
        "model = model.to(device)  # Move model to specified device correctly\n",
        "start_idx = torch.tensor([[0]], dtype=torch.long, device=device)  # Ensure start_idx is also on the same device\n",
        "generated_indices = model.generate(start_idx, max_new_tokens=10)\n",
        "print(\"Generated indices:\", generated_indices)\n",
        "```\n",
        "\n",
        "This code snippet avoids potential errors by ensuring that both the model and the tensors it interacts with are on the same device, providing a clean and error-free setup for deploying models especially in environments where both CPU and GPU are used."
      ],
      "metadata": {
        "id": "0Nlc_tOqFl-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Temperature Works\n",
        "\n",
        "### Temperature in Softmax\n",
        "\n",
        "The temperature parameter in the softmax function is a crucial tool in controlling the distribution of probabilities produced by the logits. In the context of models like GPT and other transformers, adjusting the temperature allows you to manage the trade-off between randomness and confidence in the model's predictions.\n",
        "\n",
        "### How Temperature Works\n",
        "\n",
        "- **Scaling**: The logits are divided by the temperature value before applying the softmax function. A higher temperature results in a softer probability distribution across the output classes, enhancing diversity. Conversely, a lower temperature makes the distribution sharper, with the highest logit value becoming more dominant.\n",
        "- **Formula**: For a given logit vector \\( z \\), the softmax function with temperature \\( T \\) is given by:\n",
        "  \\[\n",
        "  \\text{softmax}(z_i) = \\frac{e^{z_i/T}}{\\sum_{j} e^{z_j/T}}\n",
        "  \\]\n",
        "\n",
        "### Practical Implementation\n",
        "\n",
        "In many neural network frameworks, including PyTorch, this is implemented simply by dividing the logits by the temperature:\n",
        "```python\n",
        "logits = logits / temperature\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "```\n",
        "\n",
        "### Answers to Your Questions:\n",
        "\n",
        "1. **Production Systems with Variable Temperature Settings**:\n",
        "   - Yes, in most systems that use a temperature parameter, adjusting the temperature is indeed typically implemented as division of the logits by the temperature scalar before applying softmax. This simple operation allows for dynamic control over the behavior of the model, making it versatile in applications that require varying levels of randomness or exploration.\n",
        "\n",
        "2. **Greedy Decoding and Temperature**:\n",
        "   - Greedy decoding can be considered a special case where the temperature is implicitly set to a very small value (approaching zero), making the model highly confident, effectively picking the maximum logit value every time.\n",
        "   - **Temperature = 1.0**: This setting keeps the logits unchanged in the softmax calculation. It represents the \"natural\" or \"default\" behavior of the model without any scaling to influence the confidence of the predictions.\n",
        "\n",
        "3. **Avoiding Division by Zero**:\n",
        "   - When implementing temperature scaling, care must be taken to avoid division by zero. This is particularly relevant if the temperature could dynamically change to zero due to some runtime condition or error.\n",
        "   - **Safety Check**: Ensure there’s a check to avoid setting the temperature to zero. If the temperature is part of a user input or adjustable parameter, validate the input to ensure it's within a safe range (e.g., greater than a very small epsilon value close to zero but not zero itself).\n",
        "   - **Default Fallback**: Implement a fallback to a default temperature (like 1.0) if an invalid (zero or negative) temperature is provided.\n",
        "\n",
        "### Example with Check to Avoid Division by Zero\n",
        "\n",
        "Here's an implementation with a safety check for the temperature:\n",
        "\n",
        "```python\n",
        "def safe_softmax(logits, dim, temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "        raise ValueError(\"Temperature must be greater than zero.\")\n",
        "    scaled_logits = logits / temperature\n",
        "    return F.softmax(scaled_logits, dim=dim)\n",
        "```\n",
        "\n",
        "Using such a function ensures that your system behaves robustly, avoiding computational errors and providing meaningful error messages for debugging and user feedback.\n",
        "\n",
        "In summary, the temperature is a powerful parameter for controlling the behavior of softmax in production systems, allowing for everything from exploration (high temperature) to exploitation (low temperature, approaching greedy decoding). Ensuring safety in temperature adjustments is crucial for robust system behavior."
      ],
      "metadata": {
        "id": "jUl5uN6oID5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Matrix Multiplications in ML Contexts\n",
        "\n",
        "In general, the orientation and the role of matrices in a multiplication depend heavily on what you're trying to achieve:\n",
        "\n",
        "1. **nn.Linear**:\n",
        "   - In neural networks, particularly in layers like `nn.Linear`, the weight matrix is conventionally placed on the right side of the input matrix during the mathematical formulation, but due to implementation details, it appears as if the weights are on the left because the input matrix is often transposed or the weights themselves are transposed during computation.\n",
        "   - **Formulation**: If `X` is your input matrix of shape `(n, in_features)` and `W` is your weight matrix of shape `(out_features, in_features)`, the matrix multiplication in a linear layer is typically represented as `X @ W^T` (where `W^T` is the transpose of `W`). This gives an output matrix of shape `(n, out_features)`.\n",
        "\n",
        "2. **Attention Mechanisms**:\n",
        "   - In attention mechanisms, particularly in transformers, the weight matrices (or the projections of queries, keys, and values) might be arranged differently depending on how the attention function is structured.\n",
        "   - **Attention Calculation**: The key difference in attention is the interaction between the Query (`Q`), Key (`K`), and Value (`V`) matrices. The attention score is typically computed as `Q @ K^T`, where `Q` and `K` are of shapes `(n, d_k)` and `(n, d_k)` respectively if you're using scaled dot-product attention.\n",
        "   - The result of `Q @ K^T` is then used to weight the `V` matrix. This kind of multiplication aligns with how you'd multiply matrices where dimensions that face each other (inner dimensions in the multiplication) must match, and you're effectively computing a weighted sum of the values based on the attention scores.\n",
        "\n",
        "### Example Explanation\n",
        "\n",
        "From your Python example with matrices `a` and `b`:\n",
        "\n",
        "- `c = a @ b` results in each row of `c` being the sum of all the elements of `b` because matrix `a` is essentially acting like an accumulator given all its elements are 1. Here, `a` is on the left, but it acts uniformly across the rows of `b`.\n",
        "- `d = b @ a` also accumulates values, but in this case, since `a` is on the right, it sums up columns of `b` across rows. Each row in `d` is a repeat of these sums because all elements of `a` are 1, thus replicating the column sums across all rows of `d`.\n",
        "\n",
        "### Key Points to Remember\n",
        "\n",
        "- The position (left/right) of a weight matrix in a dot product depends on the specific operation and layer architecture. In `nn.Linear`, weights typically are conceptually on the left after considering transposition (`Wx + b` as in matrix notation, it becomes `xW^T + b` in practical code due to the way matrices are handled in computations).\n",
        "- In attention, the interaction between `Q`, `K`, and `V` dictates the arrangement and thus might feel different as it focuses on aligning matrices to compute similarities (or attention scores) before applying them to `V`.\n",
        "\n",
        "Understanding these nuances helps in appreciating how different layers and mechanisms manipulate data through matrix operations, adjusting your intuition about how data flows and transforms through complex networks like transformers."
      ],
      "metadata": {
        "id": "8_tXLJqPKv0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.allclose`\n",
        "\n",
        "The issue you're facing with `torch.allclose` returning `False` even though the tensors `xbow` and `xbow2` look very similar visually lies in the numerical precision and the operations you've performed to compute these tensors.\n",
        "\n",
        "### Issue Analysis\n",
        "\n",
        "1. **Matrix Multiplication Precision**: When you use matrix multiplication (`@`) with weight matrices computed as `wei` (normalized lower triangular matrix), the resulting computations may introduce very slight numerical inaccuracies due to floating-point precision. These differences are often very small, but they're enough to make `torch.allclose` return `False`.\n",
        "\n",
        "2. **Computational Differences**:\n",
        "   - The `xbow` computation uses a direct method that calculates the mean for slices of the tensor `x` in a loop. This computation accumulates results directly from the data.\n",
        "   - The `xbow2` computation uses matrix multiplication which may involve different internal optimizations and floating-point precision handling, especially when using GPU or optimized CPU routines.\n",
        "\n",
        "### Closer Look with `torch.allclose`\n",
        "\n",
        "`torch.allclose` checks if two tensors are element-wise equal within a tolerance. The default settings for `atol` (absolute tolerance) and `rtol` (relative tolerance) are very strict (`1e-08` for `atol` and `1e-05` for `rtol`). These default tolerances might be too tight given the potential small discrepancies introduced by different computational paths in `xbow` and `xbow2`.\n",
        "\n",
        "### Solution\n",
        "\n",
        "You can adjust the tolerance parameters in `torch.allclose` to see if the tensors are close within a more reasonable tolerance, acknowledging that minor discrepancies are expected due to the reasons mentioned:\n",
        "\n",
        "```python\n",
        "# Check with a more relaxed tolerance\n",
        "close = torch.allclose(xbow, xbow2, atol=1e-6, rtol=1e-4)\n",
        "print(\"Are the tensors close within a relaxed tolerance?\", close)\n",
        "```\n",
        "\n",
        "### Additional Debugging Step\n",
        "\n",
        "To better understand the differences, you can calculate and inspect the maximum difference between the two tensors:\n",
        "\n",
        "```python\n",
        "# Calculate the maximum difference\n",
        "max_diff = torch.max(torch.abs(xbow - xbow2))\n",
        "print(\"Maximum difference between xbow and xbow2:\", max_diff)\n",
        "```\n",
        "\n",
        "This will give you a sense of how significant the differences are, which can help in deciding appropriate values for `atol` and `rtol`.\n",
        "\n",
        "### Summary\n",
        "\n",
        "In numerical computations, especially those involving iterative or summing operations over floating-point numbers, slight differences are common. Understanding and adjusting for these in functions like `torch.allclose` is essential for accurate debugging and validation of results in scenarios where exact precision isn't always achievable due to underlying hardware or software computation methods."
      ],
      "metadata": {
        "id": "QINAMsQnYeyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `self.tril[:T, :T] == 0`\n",
        "\n",
        "The line `self.tril[:T, :T] == 0` is used to apply a mask to the attention weights (`wei`). This mask is crucial for enforcing causality within the self-attention mechanism, particularly in scenarios where the model must not be allowed to \"see\" future tokens. This type of masking is standard in models processing sequential data where each output should only be influenced by previous inputs and not by any future inputs, such as in language modeling or other types of generative tasks.\n",
        "\n",
        "### Explanation of the Masking Step\n",
        "\n",
        "- **Purpose of `tril`:** The `torch.tril` function generates a lower triangular matrix where all elements above the diagonal are zero. This matrix acts as a template to specify which positions in the attention matrix `wei` should be considered (past tokens) and which should not (future tokens).\n",
        "\n",
        "- **Masking Operation**: The operation `self.tril[:T, :T] == 0` dynamically generates a boolean mask where all positions corresponding to future tokens (above the main diagonal) are `True`. The `torch.masked_fill()` function then uses this mask to set the attention scores in `wei` at these positions to `-inf`, effectively preventing the softmax function from assigning any probability mass to future tokens.\n",
        "\n",
        "### Why `[:T, :T]` is Needed\n",
        "\n",
        "- **Dynamic Sequence Lengths**: In practice, batches of sequences may not always be filled to the maximum length (`block_size`). The slicing `[:T, :T]` ensures that the mask is correctly sized to the actual sequence length `T` in each batch, providing flexibility for handling sequences of varying lengths without the need to regenerate the triangular mask for each input.\n",
        "\n",
        "### Alternative Clearer Implementation\n",
        "\n",
        "While the current implementation with `torch.tril` and slicing is quite efficient, it can be made more intuitive by explicitly creating the mask within the `forward` method or by encapsulating the masking logic into a separate method. Here’s a clearer alternative using a function to generate the mask:\n",
        "\n",
        "```python\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "    def generate_mask(self, size):\n",
        "        \"\"\"Generates a lower triangular mask to ensure causality in attention.\"\"\"\n",
        "        return torch.tril(torch.ones(size, size, device=self.query.weight.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n",
        "        \n",
        "        # Apply causal mask to weights\n",
        "        mask = self.generate_mask(T)\n",
        "        wei = torch.masked_fill(wei, mask == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        out = wei @ v\n",
        "        return out\n",
        "```\n",
        "\n",
        "### Benefits of the Alternative Approach\n",
        "\n",
        "- **Clarity**: Encapsulating the mask generation in a separate method makes the code more organized and the functionality of each part clearer.\n",
        "- **Flexibility**: Generating the mask dynamically based on the actual sequence length each time allows the module to handle variable sequence lengths more naturally without slicing operations.\n",
        "- **Reusability**: By abstracting the mask generation, it becomes easier to modify or extend the masking logic if needed for different variants of attention mechanisms or models.\n",
        "\n",
        "This approach enhances the readability and maintainability of the code without sacrificing performance, making the underlying model's behavior more transparent."
      ],
      "metadata": {
        "id": "knQV-r8OZN-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The slicing notation `[:T, :T]`\n",
        "\n",
        "The slicing notation `[:T, :T]` is a powerful way to dynamically adjust the size of tensors in PyTorch based on the current requirements, such as the actual sequence length in batch processing. Let's dive into a simple example that illustrates how this type of slicing works to manipulate tensors.\n",
        "\n",
        "### Scenario\n",
        "\n",
        "Suppose you have a full square matrix representing some kind of relationship between elements (e.g., distances, similarities) for a set number of elements `block_size`. In each batch, however, you only process `T` elements where `T <= block_size`. You need to extract a `T x T` submatrix from the larger `block_size x block_size` matrix for computations relevant to the current batch.\n",
        "\n",
        "### Example Setup\n",
        "\n",
        "Let's create a square matrix of size `block_size` and demonstrate how to dynamically slice it to size `T` when needed.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Initialize a fixed block_size\n",
        "block_size = 5  # Assume our block size is 5\n",
        "\n",
        "# Create a full matrix of size block_size x block_size\n",
        "full_matrix = torch.arange(block_size * block_size).reshape(block_size, block_size)\n",
        "print(\"Full Matrix:\")\n",
        "print(full_matrix)\n",
        "\n",
        "# Now, let's say in a particular operation, we only need the first T x T part of this matrix\n",
        "T = 3  # For this example, let's consider T to be 3\n",
        "\n",
        "# Extract the T x T submatrix\n",
        "sub_matrix = full_matrix[:T, :T]\n",
        "print(\"\\nExtracted Submatrix for T=3:\")\n",
        "print(sub_matrix)\n",
        "```\n",
        "\n",
        "### Output Explanation\n",
        "\n",
        "The code will output the following matrices:\n",
        "\n",
        "1. **Full Matrix**: A `5x5` matrix filled with numbers from 0 to 24 arranged in a square format.\n",
        "2. **Extracted Submatrix**: When `T=3`, the submatrix extracted will be the top left `3x3` portion of the full matrix.\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```plaintext\n",
        "Full Matrix:\n",
        "tensor([[ 0,  1,  2,  3,  4],\n",
        "        [ 5,  6,  7,  8,  9],\n",
        "        [10, 11, 12, 13, 14],\n",
        "        [15, 16, 17, 18, 19],\n",
        "        [20, 21, 22, 23, 24]])\n",
        "\n",
        "Extracted Submatrix for T=3:\n",
        "tensor([[ 0,  1,  2],\n",
        "        [ 5,  6,  7],\n",
        "        [10, 11, 12]])\n",
        "```\n",
        "\n",
        "### Why This Is Useful\n",
        "\n",
        "This slicing technique is particularly useful in applications where:\n",
        "- The size of data to be processed can vary (e.g., variable sequence lengths in NLP, different batch sizes in image processing).\n",
        "- Only a portion of a larger dataset or matrix is relevant for a specific computation or analysis.\n",
        "- You need to ensure that operations are flexible and efficient without the need to recreate or resize tensors unnecessarily.\n",
        "\n",
        "This method of dynamic slicing helps manage memory more efficiently and makes your code adaptable to varying data sizes, enhancing both performance and scalability of your applications."
      ],
      "metadata": {
        "id": "-xFNpvnwZqV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nn.ModuleList` and `nn.Sequential`\n",
        "\n",
        "In PyTorch, both `nn.ModuleList` and `nn.Sequential` are used to store and manage multiple submodules (like layers), but they serve different purposes and provide different functionalities based on the needs of your model architecture.\n",
        "\n",
        "### `nn.ModuleList`\n",
        "\n",
        "**Description**: `nn.ModuleList` is essentially a Python list that is also a PyTorch module. It holds submodules in a list but doesn't define how they should interact, merely ensuring they're registered as part of the parent module.\n",
        "\n",
        "**Use Cases**:\n",
        "- **Custom Combinations of Modules**: When you need to iterate over a list of modules and apply them in some custom way that isn't strictly sequential.\n",
        "- **Variable Module Application**: Useful when the application of modules can change dynamically during runtime, such as selectively applying certain modules under specific conditions.\n",
        "- **Module Management**: It helps in managing a list of modules where each module might need to be accessed individually or modified separately.\n",
        "- **Example from Your Code**:\n",
        "  - `MultiHeadAttention` class uses `nn.ModuleList` to hold multiple attention heads. This allows the model to process inputs through multiple heads separately and then combine their outputs. This wouldn't be easily manageable with `nn.Sequential` because each head might need to be applied independently or in parallel, and then their outputs concatenated.\n",
        "\n",
        "```python\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # Assume initialization details\n",
        "        pass\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "```\n",
        "\n",
        "### `nn.Sequential`\n",
        "\n",
        "**Description**: `nn.Sequential` is a container that processes modules in a sequential manner. When you place modules in `nn.Sequential`, the output of one module becomes the input to the next module automatically.\n",
        "\n",
        "**Use Cases**:\n",
        "- **Simplified Sequential Processing**: Perfect for cases where the model's structure is a simple feed-forward type, with a clear sequence of operations that do not require branching, skipping, or any other form of non-linear connectivity.\n",
        "- **Ease of Use**: Helps in quickly setting up networks without explicitly defining the `forward` pass for connecting each layer.\n",
        "- **Stacked Layers**: Commonly used for creating chains of layers in neural networks, such as a series of convolutional layers followed by activations.\n",
        "- **Example from Your Code**:\n",
        "  - The `FeedForward` class uses `nn.Sequential` for a straightforward linear transformation followed by an activation function, which is a typical use case for `nn.Sequential`.\n",
        "\n",
        "```python\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "```\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The choice between `nn.ModuleList` and `nn.Sequential` depends on how you need to apply the submodules:\n",
        "- Use `nn.ModuleList` when you need more control over how modules are applied, need to handle them individually, or require custom architectures.\n",
        "- Use `nn.Sequential` for straightforward, linear sequences of modules where the output of one module feeds directly into the next without any modifications or conditions.\n",
        "\n",
        "These distinctions make each tool suitable for different scenarios, allowing you to optimize the design of your neural network architectures based on specific requirements."
      ],
      "metadata": {
        "id": "WjVxlf7xEOL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip Connections and Layer Normalizations\n",
        "\n",
        "Adding skip connections, also known as residual connections, to a transformer block as you've described is indeed quite straightforward and aligns well with the architecture of popular transformer models like those described in the original \"Attention is All You Need\" paper. Your proposed modification does effectively implement skip connections. Let's go over why this is effective and ensure you're implementing it correctly.\n",
        "\n",
        "### What Are Skip Connections?\n",
        "\n",
        "Skip connections are a technique used in neural network architectures to help mitigate the vanishing gradient problem and to enable deeper networks by allowing gradients to flow through the network directly. In transformers, they help the network to preserve information from earlier layers and combine it with new transformations, which can enhance learning and model performance.\n",
        "\n",
        "### Implementation in Transformers\n",
        "\n",
        "In the context of transformers, a skip connection typically involves adding the input directly to the output of a sub-layer (like multi-head attention or feedforward neural networks), followed by normalization. Here’s what you need to ensure when adding skip connections:\n",
        "\n",
        "1. **Addition Before Normalization**: While your code snippet directly adds the outputs from the self-attention and feed-forward layers to their respective inputs, it's common practice in transformers to also apply layer normalization after each addition. This helps in stabilizing the training process.\n",
        "\n",
        "2. **Scaling (Optional)**: Some implementations also scale the outputs by \\(\\sqrt{0.5}\\) (or similar factors) to balance the variance of the sum.\n",
        "\n",
        "### Suggested Code Modification\n",
        "\n",
        "Here’s an improved version of your code, adding layer normalization, which is a critical component in transformer models:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.norm1 = nn.LayerNorm(n_embed)\n",
        "        self.norm2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply self-attention and add the input (residual connection), then normalize\n",
        "        x = self.norm1(x + self.sa(x))\n",
        "        # Apply feed-forward network, add the result to the input of this step, then normalize\n",
        "        x = self.norm2(x + self.ffwd(x))\n",
        "        return x\n",
        "```\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- **LayerNorm**: Each block now includes two `nn.LayerNorm` layers. Normalization is applied after each skip connection, which is essential for stabilizing the learning process.\n",
        "- **Norm + Residual**: This order (first add, then normalize) is typical in many successful transformer models.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Adding skip connections as you've proposed is indeed straightforward but adding normalization is also crucial. These connections make the model more robust by allowing it to learn identity functions where beneficial, thereby preventing the deeper layers from corrupting the information processed in earlier layers. This pattern has been fundamental in enabling the training of very deep networks in practice, significantly improving performance across a wide range of tasks."
      ],
      "metadata": {
        "id": "HCk2V4LIMqIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip connections are architectural features\n",
        "\n",
        "Skip connections themselves are architectural features rather than specific parameters or modules that directly appear in printouts like the structure, state_dict, or parameter counts. They are implemented in the code as operations that add the output of a layer (or block) to its input and do not have their own learnable parameters. Therefore, they won't be explicitly listed in the outputs you're planning to generate. Here’s what you can expect from each type of printout regarding skip connections:\n",
        "\n",
        "### 1. Model's Structure Printout\n",
        "When you print the model's structure using `print(model)`, it shows the components (submodules) of the model, such as layers defined in the `__init__` method. Skip connections, being merely operational steps within the `forward` method, do not have separate submodules and thus won’t be explicitly visible in this structure printout.\n",
        "\n",
        "### Example Output Snippet\n",
        "For a transformer block similar to what you described, the printout might look like this (without skip connections visible):\n",
        "```plaintext\n",
        "Block(\n",
        "  (sa): MultiHeadAttention(\n",
        "    ... # Details of the attention mechanism\n",
        "  )\n",
        "  (ffwd): FeedForward(\n",
        "    ... # Details of the feedforward network\n",
        "  )\n",
        "  (norm1): LayerNorm(...)\n",
        "  (norm2): LayerNorm(...)\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. Model's `state_dict` Printout\n",
        "The `state_dict` holds the model's parameters (weights and biases). Since skip connections do not involve learnable parameters—they merely add tensors—they won't appear in the `state_dict`. Only layers like `nn.Linear`, `nn.Conv2d`, `nn.LayerNorm`, etc., which have weights and/or biases, will be visible in the `state_dict`.\n",
        "\n",
        "### Example Output Snippet\n",
        "Here, you would see entries for each weight and bias tensor in the model’s layers, but nothing specifically for skip connections:\n",
        "```plaintext\n",
        "sa.query.weight    torch.Size([...])\n",
        "sa.key.weight      torch.Size([...])\n",
        "ffwd.net.0.weight  torch.Size([...])\n",
        "norm1.weight       torch.Size([...])\n",
        "norm2.weight       torch.Size([...])\n",
        "```\n",
        "\n",
        "### 3. Number of Parameters Printout\n",
        "The total and trainable parameters counts will reflect the sum of all parameters in the model. Since skip connections have no parameters, they do not affect these counts. The counts will only include parameters from the defined layers (like those in `MultiHeadAttention` and `FeedForward`) and any normalization layers.\n",
        "\n",
        "### Implications\n",
        "Understanding that skip connections are not directly represented in parameter or structure listings is important for interpreting these printouts. Their effects are embodied in how they influence the training dynamics and performance, not in the parameter count or structure listings.\n",
        "\n",
        "### Conclusion\n",
        "While skip connections are integral to the functionality and performance improvements in models like transformers, they are essentially part of the model's logic and flow (in how data is processed through the network), rather than discrete components. Their presence needs to be understood and traced through the source code in the `forward` method rather than through inspection tools that list components or parameters."
      ],
      "metadata": {
        "id": "K1zN5Iz5OQQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Add a Projection Layer?\n",
        "\n",
        "In the context of transformers and particularly within the design of MultiHeadAttention mechanisms, a projection layer after the concatenation of heads is typically required to consolidate the multiple attention outputs into a single tensor that matches the dimensions expected by subsequent layers or operations. This ensures that the MultiHeadAttention's output can be integrated smoothly within the broader architecture of the model.\n",
        "\n",
        "### Why Add a Projection Layer?\n",
        "\n",
        "1. **Dimensionality Alignment**: MultiHeadAttention expands the feature dimensionality by concatenating outputs from multiple heads. If each head returns a vector of length `head_size` and there are `num_heads` heads, the concatenated result will have a dimension of `num_heads * head_size`. A projection layer (typically an `nn.Linear` layer) is needed to bring this back to the original embedding dimension `n_embed`, making the output compatible with other parts of the network that expect inputs of this dimension.\n",
        "\n",
        "2. **Mixing Information Across Heads**: The outputs from different heads might focus on different aspects or parts of the input sequence, given that each head computes attention independently. A projection layer helps to mix or integrate these diverse representations into a unified output, potentially enhancing the representation power by combining various learned aspects.\n",
        "\n",
        "3. **Maintaining Network Depth**: In the original transformer architecture proposed by Vaswani et al., the depth of the network (number of layers) is crucial for learning complex representations. The projection layer contributes to this depth, adding another linear transformation that the network can learn from.\n",
        "\n",
        "### How to Implement This in Your `MultiHeadAttention` Class\n",
        "\n",
        "Your current implementation of `MultiHeadAttention` concatenates outputs from each head but does not apply any projection afterward. Here’s how you might modify your class to include this important projection step:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embed):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # Projection layer to bring concatenated heads back to the dimension n_embed\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate outputs from all attention heads\n",
        "        head_outputs = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project concatenated heads back to the original embedding size\n",
        "        output = self.proj(head_outputs)\n",
        "        return output\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **`self.proj`**: This is an `nn.Linear` layer that projects the concatenated output of the attention heads back to the original embedding dimension (`n_embed`). It's essential to ensure that the dimensionality of the MultiHeadAttention output matches that of the input, especially in a typical encoder or decoder architecture where each layer expects inputs and outputs to be of consistent dimensions.\n",
        "\n",
        "Adding this projection layer effectively allows the MultiHeadAttention to integrate the information gathered by different attention heads and refine it into a format usable by subsequent layers, adhering to the design principles of the transformer architecture."
      ],
      "metadata": {
        "id": "lz-1fmZFRa-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout\n",
        "\n",
        "Dropout is a regularization technique used in neural networks to prevent overfitting. The method temporarily drops units (along with their connections) from the network during training. This random omission of units during different training epochs forces the network to learn more robust features that are not reliant on any small set of neurons, promoting better generalization to unseen data.\n",
        "\n",
        "### Main Benefits of Dropout:\n",
        "\n",
        "1. **Prevention of Overfitting**: Dropout reduces the model's reliance on any individual neuron by randomly dropping out units during the training process. This helps in preventing the model from fitting too closely to the training data, which can lead to poor performance on new, unseen data (overfitting).\n",
        "\n",
        "2. **Model Robustness**: By randomly removing neurons during training, dropout forces the network to develop redundant pathways for the same information, increasing its robustness and reducing the likelihood of developing fragile co-adaptations among neurons (where neurons overly depend on the specific presence of other neurons).\n",
        "\n",
        "3. **Ensemble Effect**: Each training step with dropout can be seen as training a different model that shares weights with other models in the ensemble. At test time, using all neurons can be viewed as averaging the predictions of all these thinned models, akin to an ensemble method, which often results in better performance.\n",
        "\n",
        "### Implementation in Transformers:\n",
        "\n",
        "In transformer architectures, dropout is typically applied in several places:\n",
        "- **After Attention and Before Residual Add**: Applying dropout to the output of the attention mechanism (or any sub-layer outputs) before adding the residual connection can help the model to not rely too heavily on specific paths or weights, promoting more robust learning.\n",
        "- **On the Attention Weights**: Dropout can also be applied to the attention scores before they are used to create the weighted sum of values, helping to ensure that the model does not overfit to specific aspects of the data.\n",
        "- **Feedforward Network**: Within the feedforward network of each transformer block, dropout is often applied to the output of the first linear transformation before the activation function.\n",
        "\n",
        "### Is Applying Dropout Just Before Residual Rejoining Good?\n",
        "\n",
        "Yes, applying dropout just before the tensors rejoin the residual pathway is indeed beneficial and a common practice in many state-of-the-art transformer models. Here's why:\n",
        "\n",
        "- **Effective Regularization**: This placement ensures that the information passing through the network cannot rely on any single path for accurate predictions, encouraging the network to find alternative pathways and thus generalize better.\n",
        "- **Maintains Information Flow**: Since the residual connection bypasses the dropout, essential information is not entirely lost even when dropout is applied. This setup helps in maintaining the flow of information through the network while still benefiting from the regularization effect of dropout.\n",
        "\n",
        "### Example in Transformer Block:\n",
        "\n",
        "Here’s how you might typically see dropout applied in a transformer block:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(n_head, n_embed)\n",
        "        self.norm1 = nn.LayerNorm(n_embed)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.norm2 = nn.LayerNorm(n_embed)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention part\n",
        "        attn_output = self.attention(x)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        x = x + attn_output\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward part\n",
        "        ffwd_output = self.ffwd(x)\n",
        "        ffwd_output = self.dropout2(ffwd_output)\n",
        "        x = x + ffwd_output\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "```\n",
        "\n",
        "In this structure, dropout is applied right after the attention outputs and the feedforward outputs but before they are added back to the main data flow (residual connection). This approach is optimal for ensuring that all parts of the model contribute to the final result without overfitting to the training data."
      ],
      "metadata": {
        "id": "a0L8_E3WgPvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `nn.Sequential`\n",
        "\n",
        "Using `nn.Sequential` in MLP (Multi-Layer Perceptron) implementations is quite common in PyTorch, as it provides a convenient way to define a simple sequence of operations where the output of one module is the input to the next. Both the `MLP` class and the `FeedForward` class you've shown serve similar purposes but are structured differently—one using explicit layer calls in its forward method and the other encapsulating the layers in an `nn.Sequential` container. Here are the pros and cons of using `nn.Sequential` in such implementations:\n",
        "\n",
        "### Using `nn.Sequential`\n",
        "**Pros:**\n",
        "1. **Simplicity**: The main advantage is the simplicity of setup and readability. `nn.Sequential` allows you to easily define a clear and concise pipeline of layers and operations where the output of one layer is automatically the input of the next layer. It reduces the boilerplate code required in the `forward` method.\n",
        "2. **Compactness**: It compacts the model definition, making the model easier to read and maintain, especially for straightforward feed-forward architectures without branching or skip connections.\n",
        "3. **Ease of Use**: It's straightforward to add or remove layers, which can be beneficial during experimentation and model tuning.\n",
        "\n",
        "**Cons:**\n",
        "1. **Lack of Flexibility**: `nn.Sequential` is limited to operations that fit a linear flow—each layer's output is directly passed as input to the next. This makes it unsuitable for more complex models where you might need to implement branches, merges, or residual connections easily.\n",
        "2. **Direct Control**: You have less direct control over the intermediate outputs, which can be a drawback when you need to perform operations based on intermediate results or when conditions based on these are required.\n",
        "3. **Customization**: Adding custom behavior (like dynamic layer behavior based on input properties) between layers is not straightforward without breaking the sequential chain.\n",
        "\n",
        "### Manual Layer Definition in `forward` Method\n",
        "**Pros:**\n",
        "1. **Flexibility**: Defining each layer and its activation explicitly in the `forward` method, as in the `MLP` class, offers more control and flexibility. This allows for conditional operations, more complex manipulations of data between layers, and the integration of non-linear connectivity patterns (like skip connections).\n",
        "2. **Debugging and Inspection**: It's easier to debug and inspect intermediate results, as you can add print statements or logging between layers.\n",
        "3. **Customization**: Easier to integrate layers with custom behavior or conditional processing of data between layers.\n",
        "\n",
        "**Cons:**\n",
        "1. **Verbosity**: This method can be more verbose and complex, particularly for larger models. It requires manually handling each layer and operation, which can lead to more boilerplate code and increase the chance of errors.\n",
        "2. **Maintainability**: More complex `forward` methods might be harder to maintain, especially when changes involve modifying several interconnected lines of code.\n",
        "\n",
        "### Conclusion\n",
        "Choosing between `nn.Sequential` and manual layer definition depends on the specific needs of your application and the complexity of the model:\n",
        "- Use `nn.Sequential` when your model architecture is straightforward, and layers can be arranged in a simple linear sequence without conditional logic.\n",
        "- Prefer manual definition in the `forward` method when your model requires more complex data flow control, branching, or when you need to perform specific operations between layers that `nn.Sequential` cannot handle directly.\n",
        "\n",
        "For example, the `MLP` implementation you provided might be preferred in scenarios where the precise control of dropout application or integration of custom activations or processing steps is necessary. The `FeedForward` using `nn.Sequential` could be more appropriate for simpler, quick-to-deploy models where such granular control is unnecessary."
      ],
      "metadata": {
        "id": "osfGrb6nT6Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight-tying\n",
        "\n",
        "Weight-tying, also known as parameter sharing, is a technique used in machine learning to reduce the memory footprint of models and potentially improve their generalization performance. In the context of neural networks, and particularly transformers, weight-tying involves using the same weight matrix for multiple different parts of the model.\n",
        "\n",
        "### Applications in Transformers\n",
        "\n",
        "In transformers, weight-tying is most commonly used between the embedding layers and the final linear layer before the softmax in language modeling tasks. Here are some details on how it's applied:\n",
        "\n",
        "1. **Embedding and Output Layer Tying**:\n",
        "   - **Concept**: The same weight matrix that is used in the input embedding layer is transposed and used in the output layer, which predicts the next token based on the context provided by the transformer's decoder.\n",
        "   - **Reasoning**: The embedding layer converts token indices into vectors. The output layer, on the other hand, converts the decoder's output vectors back into token logits (which are then passed through a softmax to predict probabilities). Since both layers deal with the same vocabulary space and the embedding dimension is the same as the transformer's hidden size, tying these weights can improve performance by reducing overfitting and enforcing a consistent representation of token semantics across the model.\n",
        "\n",
        "2. **Efficiency and Regularization**:\n",
        "   - **Reduced Parameters**: By tying weights, the model's total number of parameters decreases significantly, which can lead to less memory usage and often faster inference.\n",
        "   - **Regularization Effect**: Sharing weights across different layers acts as a form of regularization. It can help the model learn more robust features, as the same weights must work well in both the embedding and the output contexts.\n",
        "\n",
        "### Example Implementation\n",
        "\n",
        "In PyTorch, implementing weight tying in a transformer model could look like this (assuming the model structure has an embedding layer named `embeddings` and an output projection layer named `output_projection`):\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerWithTiedWeights(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=8)\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Tie weights between embedding and output projection\n",
        "        self.output_projection.weight = self.embeddings.weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        embedded_input = self.embeddings(input)\n",
        "        transformer_output = self.transformer(embedded_input)\n",
        "        logits = self.output_projection(transformer_output)\n",
        "        return logits\n",
        "\n",
        "# Usage\n",
        "model = TransformerWithTiedWeights(vocab_size=10000, d_model=512)\n",
        "```\n",
        "\n",
        "### Benefits and Considerations\n",
        "\n",
        "- **Benefits**:\n",
        "  - **Efficiency**: Reduces the number of trainable parameters, which can lead to faster training and lower memory consumption.\n",
        "  - **Performance**: Can improve model performance, particularly in language tasks, by helping the model generalize better from its training data.\n",
        "- **Considerations**:\n",
        "  - **Design Constraints**: Tying weights imposes a constraint that the dimensions of the tied layers must match. This can limit some design choices regarding the model architecture.\n",
        "  - **Application Specificity**: Weight tying is more beneficial in some applications (like NLP) than others. Its effectiveness can vary based on the task and the data.\n",
        "\n",
        "Weight-tying is a powerful technique in model optimization, particularly for large-scale models like those used in NLP, where reducing the number of parameters without losing model capacity can significantly enhance computational efficiency and model robustness.\n",
        "\n",
        "---\n",
        "\n",
        "Weight-tying in the context of transformers, particularly between the token embedding layer and the output linear layer (often referred to as the \"lm_head\" or language model head), is an interesting technique used to reduce the number of parameters and to theoretically improve the learning efficiency and generalization of the model. Let's clarify how this works and address your concerns.\n",
        "\n",
        "### How Weight-Tying Works in Transformers\n",
        "\n",
        "In models like GPT (Generative Pre-trained Transformer), the embedding layer converts input token indices into vectors. These vectors are then processed through various transformer blocks. At the end of the transformer, an output layer (lm_head) converts the transformer's output vectors back into a vocabulary-sized vector for each token, where each element represents a score for that token being the next token in the sequence.\n",
        "\n",
        "Here's where weight-tying comes into play:\n",
        "- **Weight-Tying Between Layers**: The weight matrix in the lm_head (used for transforming the output of the last transformer layer back to the token vocabulary space) is set to be the same as the weight matrix in the embedding layer (used for mapping token indices to vectors). Mathematically, this means that `lm_head.weight` is set to `embedding_layer.weight.transpose()` or directly to `embedding_layer.weight` depending on the implementation and shape requirements.\n",
        "\n",
        "### Implications and Effects\n",
        "\n",
        "1. **Parameter Efficiency**: This method significantly reduces the model's total number of parameters since we effectively eliminate one large matrix of parameters from the model. For large vocabulary sizes, this can result in substantial savings in memory and computational requirements.\n",
        "\n",
        "2. **Theoretical Justification**: By tying the weights, the model learns a shared representation for both embedding tokens and decoding them into predicted next tokens. This is thought to help the model learn more robust, generalizable features since the same weights must successfully perform both encoding input tokens and decoding output tokens.\n",
        "\n",
        "### Addressing Your Concerns\n",
        "\n",
        "- **Changing Token Embeddings**: You're correct in noting that the weights are shared and any updates to them during training will affect both the embeddings and the output projections. However, this isn't necessarily a downside. Since both tasks (embedding and predicting tokens) are closely related, learning a shared representation can be beneficial. The weight updates are driven by both the embedding role and the projection role, potentially leading to a more integrated understanding of the token space.\n",
        "\n",
        "- **Different Layers, Same Weights**: It might seem odd that an `nn.Embedding` layer and an `nn.Linear` layer share weights because they are used in very different contexts within the model. However, fundamentally, both are performing linear transformations. An `nn.Embedding` can be thought of as a lookup table that is a special case of a linear transformation without multiplication operations, where indices directly select rows from the weight matrix. When you use these weights in a linear layer (`nn.Linear`), you're applying these weights in a matrix multiplication, which is a more general operation. The fact that these operations are related allows for the possibility of weight-tying without significant conceptual conflict.\n",
        "\n",
        "### Practical Example\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class TiedTransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer_blocks = nn.TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "        self.lm_head.weight = self.embeddings.weight  # Tying the weights\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embeddings(input_ids)\n",
        "        x = self.transformer_blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "```\n",
        "\n",
        "In summary, weight-tying in transformers is a beneficial technique that can reduce the model's complexity and improve parameter efficiency without necessarily compromising performance. This strategy leverages the fundamental similarity between embedding inputs and projecting transformer outputs to make learning more efficient."
      ],
      "metadata": {
        "id": "3m16AAa7DzSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `print(model)`\n",
        "\n",
        "In PyTorch, when you call `print(model)`, it outputs a formatted string representation of your model, showing all its components, including layers, submodules, and other elements defined within the model. This functionality is indeed part of PyTorch and is typically used to give developers a clear, hierarchical view of the model's architecture.\n",
        "\n",
        "### How `print(model)` Works in PyTorch\n",
        "\n",
        "1. **Hierarchy Representation**: `print(model)` leverages the nested structure of PyTorch modules. Each module (`nn.Module`) in PyTorch maintains a list of its sub-modules that are automatically registered when you assign an `nn.Module` as an attribute of another `nn.Module`.\n",
        "\n",
        "2. **Module Registration**: For something to appear in the `print(model)` output, it must be a subclass of `nn.Module` and must be registered as a submodule or a field of another module. This registration happens automatically when you assign an instantiated `nn.Module` to an attribute of another `nn.Module` in its `__init__` method.\n",
        "\n",
        "3. **`__repr__` Method**: Under the hood, `print(model)` calls the `__repr__` method of the `nn.Module` class, which is designed to recursively fetch and format the string representations of all registered sub-modules.\n",
        "\n",
        "### Qualifications for Appearing in Model Printout\n",
        "\n",
        "- **Must Be a Subclass of `nn.Module`**: Any component that should appear in the model structure printout must be an instance of a subclass of `nn.Module`. This is because only `nn.Module` instances can be registered as submodules.\n",
        "  \n",
        "- **Must Be Registered as a Submodule**: When constructing a model, any layers or components that are instances of `nn.Module` must be assigned as attributes of the class. For example:\n",
        "  ```python\n",
        "  self.layer = nn.Linear(in_features, out_features)\n",
        "  ```\n",
        "  This code snippet not only creates a linear layer but also registers it as a submodule of the model, ensuring it appears in the `print(model)` output.\n",
        "\n",
        "- **Does Not Necessarily Need to Be Part of the `nn` Library**: While most commonly used layers and functions are part of the `torch.nn` library, you can create your own custom modules by subclassing `nn.Module`. As long as these custom modules follow the two rules above, they will appear in the model printout.\n",
        "\n",
        "### Example with Custom Module\n",
        "\n",
        "Here’s a brief example demonstrating how custom modules are included in the model's printout:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(features, features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.relu(self.linear(x))\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = CustomLayer(10)\n",
        "        self.layer2 = nn.Linear(10, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        return self.layer2(x)\n",
        "\n",
        "model = MyModel()\n",
        "print(model)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "MyModel(\n",
        "  (layer1): CustomLayer(\n",
        "    (linear): Linear(in_features=10, out_features=10, bias=True)\n",
        "  )\n",
        "  (layer2): Linear(in_features=10, out_features=5, bias=True)\n",
        ")\n",
        "```\n",
        "\n",
        "In this example, `CustomLayer` appears in the printout alongside the standard `nn.Linear` layer because it is a subclass of `nn.Module` and is registered as a submodule of `MyModel`. This explains how custom functionalities are seamlessly integrated into PyTorch’s modular design, allowing for complex and highly customized architectures while maintaining a clear and understandable module hierarchy."
      ],
      "metadata": {
        "id": "gA6M_loYGIxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nn.Module`\n",
        "\n",
        "Being a subclass of `nn.Module` in PyTorch provides a structured way to encapsulate network layers and their associated computations. This design not only makes it easier to build and manage complex models but also ensures that these models are extensible, maintainable, and easily integrable within the PyTorch ecosystem. Here are the main benefits and functionalities that inheriting from `nn.Module` provides:\n",
        "\n",
        "### 1. **Parameter Management**\n",
        "\n",
        "One of the key benefits of making a class a subclass of `nn.Module` is the automatic management of parameters. `nn.Module` automatically tracks all fields that are instances of `nn.Parameter` or `nn.Module`, allowing:\n",
        "- **Automatic Parameter Registration**: When you define a model's components (e.g., layers) as attributes of a subclass of `nn.Module`, PyTorch automatically registers these components' parameters.\n",
        "- **Easy Access to All Parameters**: You can easily access all parameters of a model for purposes like feeding them into an optimizer using `model.parameters()` or `model.named_parameters()`.\n",
        "\n",
        "### 2. **Device Management**\n",
        "\n",
        "PyTorch models can seamlessly move computations between different devices (CPUs, GPUs) with simple method calls:\n",
        "- **`.to(device)`**: Easily move all model parameters to a specified device (CPU, GPU, or other accelerators), helping manage hardware resources efficiently.\n",
        "- **`.cuda()` or `.cpu()`**: Specific methods to move parameters to GPU or CPU, respectively.\n",
        "\n",
        "### 3. **Forward Pass Definition**\n",
        "\n",
        "The `forward()` method:\n",
        "- **Central Computing Logic**: `nn.Module` requires you to define a `forward()` method in your subclass, which is automatically called when you execute `model(input)`. This method specifies the computations performed by the module on the input data, effectively defining the model's forward pass.\n",
        "- **Clarity and Intuitiveness**: By encapsulating the forward pass logic in the `forward()` method, `nn.Module` makes the model's operation clear and easy to understand and debug.\n",
        "\n",
        "### 4. **Integration with PyTorch Ecosystem**\n",
        "\n",
        "Subclassing from `nn.Module` ensures full compatibility with other PyTorch features:\n",
        "- **Optimizers**: Direct integration with PyTorch optimizers, which require a list of parameters (`model.parameters()`) to optimize.\n",
        "- **Serialization**: Support for saving and loading models using PyTorch’s serialization APIs (`torch.save`, `torch.load`).\n",
        "- **Hooks**: Ability to register forward and backward hooks for debugging and inspecting the flow of tensors through the network.\n",
        "\n",
        "### 5. **Modularity and Reusability**\n",
        "\n",
        "- **Encapsulation**: Encapsulating layers and operations within modules makes complex models more organized and modular, promoting reusability and maintainability.\n",
        "- **Composition**: Models can be composed of other smaller models or layers, all as `nn.Module` subclasses, promoting code reuse and simplifying complex architectures.\n",
        "\n",
        "### 6. **Customization and Extensibility**\n",
        "\n",
        "- **Custom Layers and Functions**: Users can define their own custom layers, loss functions, or even complex dynamic behaviors within the `nn.Module` framework, giving extensive flexibility to innovate beyond standard neural network layers.\n",
        "\n",
        "### Example: Custom Module and Utilization\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_features, output_features)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "# Usage in a model\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = CustomLayer(10, 50)\n",
        "        self.layer2 = CustomLayer(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        return self.layer2(x)\n",
        "\n",
        "model = MyNetwork()\n",
        "print(model)\n",
        "# Output the structure, demonstrating encapsulation and ease of understanding\n",
        "```\n",
        "\n",
        "In summary, being a subclass of `nn.Module` is fundamental in PyTorch for defining neural networks, as it provides systematic management of components, parameters, and their interactions, alongside integration into the broader ecosystem for training, evaluation, and deployment."
      ],
      "metadata": {
        "id": "CxanLW__JDYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The autograd system\n",
        "\n",
        "In PyTorch, the autograd system is a fundamental component that underlies the training process, but it is not specifically part of the loss function or `nn.Module`. Rather, it's a core feature of PyTorch that provides automatic differentiation capabilities across all tensor operations in the framework. This system tracks operations performed on tensors, allowing for the automatic computation of gradients when performing backpropagation.\n",
        "\n",
        "### Autograd System Overview\n",
        "\n",
        "The autograd system is built around the concept of the computational graph. Operations performed on tensors create a graph of function nodes that link together the operations that created new tensors. This graph is used to compute derivatives (gradients) in reverse order by tracing the graph from the output back to the inputs.\n",
        "\n",
        "### Role of Autograd in Different Components\n",
        "\n",
        "1. **`nn.Module`**:\n",
        "   - **Definition and Structure**: The `nn.Module` class is a building block for creating neural network layers and models. It primarily manages parameters, sub-modules, and the forward computations.\n",
        "   - **Gradient Tracking**: While `nn.Module` itself doesn't handle gradients, any operations performed in its `forward` method involving tensors with `requires_grad=True` are tracked by the autograd system. This tracking is what enables gradients to be automatically computed during backpropagation.\n",
        "\n",
        "2. **Loss Functions**:\n",
        "   - **Gradient Computation**: Loss functions, whether they are part of `torch.nn` or custom functions, typically return a scalar tensor representing the loss. This tensor is the point where the backward pass is usually initiated. The autograd system uses this scalar value to compute the gradients of parameters with respect to the loss by traversing the computational graph.\n",
        "   - **Interface with Autograd**: Loss functions are crucial in defining how outputs of the model relate to the expected outcomes (targets), but they rely on the autograd system to handle the differentiation and gradient calculations.\n",
        "\n",
        "### How Autograd Interfaces with Training Components\n",
        "\n",
        "Here’s a simple illustration of how autograd interacts with `nn.Module` and loss functions during training:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Create a model instance and define a loss function and optimizer\n",
        "model = SimpleModel()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Sample data\n",
        "input_tensor = torch.tensor([[1.0]], requires_grad=True)\n",
        "target_tensor = torch.tensor([[2.0]])\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "loss = loss_function(output, target_tensor)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()  # Autograd computes gradients here\n",
        "\n",
        "# Optimizer uses the gradients to update model parameters\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The `forward` pass of `model` calculates the output using `nn.Module`.\n",
        "- The `loss_function` computes the loss, which is a scalar tensor.\n",
        "- Calling `loss.backward()` triggers the autograd system to compute gradients for all tensors with `requires_grad=True` by tracing backwards through the computational graph from the loss tensor.\n",
        "- The optimizer then updates the model's parameters using the gradients computed by autograd.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The autograd system is a backbone of PyTorch's training process, enabling automatic differentiation across tensor operations. It is not a part of `nn.Module` or loss functions but rather operates across all computations involving tensors that require gradients. Its ability to automatically compute gradients by tracing tensor operations makes it indispensable for efficiently training neural network models."
      ],
      "metadata": {
        "id": "2_lWJB11K3Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a 2x3 matrix\n",
        "A = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                  [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Apply softmax with dim=1\n",
        "softmax_A_dim1 = F.softmax(A, dim=1)\n",
        "print(\"Softmax over dim=1:\\n\", softmax_A_dim1)\n",
        "\n",
        "# Apply softmax with dim=0\n",
        "softmax_A_dim0 = F.softmax(A, dim=0)\n",
        "print(\"Softmax over dim=0:\\n\", softmax_A_dim0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JudF5U7ksjW4",
        "outputId": "4dbac40e-3f32-4bc7-b706-8318efb75f3c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax over dim=1:\n",
            " tensor([[0.0900, 0.2447, 0.6652],\n",
            "        [0.0900, 0.2447, 0.6652]])\n",
            "Softmax over dim=0:\n",
            " tensor([[0.0474, 0.0474, 0.0474],\n",
            "        [0.9526, 0.9526, 0.9526]])\n"
          ]
        }
      ]
    }
  ]
}