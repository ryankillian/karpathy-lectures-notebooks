{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This Colab notebook serves as a personal workbook to guide you (and me) through coding micrograd from scratch.\n",
        "\n",
        "I like to try to code some of Andrej Karpathy's 'Zero-to-Hero' projects from scratch every now and again.\n",
        "\n",
        "By following this workbook, you can regularly practice and internalize the process of building micrograd from the ground up.\n",
        "\n",
        "Each time you start a new Colab, simply copy over the instructions cell and build upon it.\n",
        "\n",
        "[Go to micrograd](https://github.com/karpathy/micrograd)"
      ],
      "metadata": {
        "id": "BWCcYcyV1nUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions"
      ],
      "metadata": {
        "id": "lcCitUaUz3hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Byct8E_61p-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5z48Ot_M74ih"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------\n",
        "\n",
        "# There are 3 'phases' to coding micrograd from scratch\n",
        "\n",
        "# 1. Value class (micrograd engine)\n",
        "# 2. Neural Network\n",
        "# 3. Training\n",
        "\n",
        "\n",
        "# 1. Value class.\n",
        "\n",
        "# __init__\n",
        "# __add__\n",
        "# __repl__\n",
        "# __radd__\n",
        "# __mul__\n",
        "# __rmul__\n",
        "# __pow__\n",
        "# __neg__\n",
        "# __sub__\n",
        "# __rsub__\n",
        "# __truediv__\n",
        "# __rtruediv__\n",
        "\n",
        "# __relu__\n",
        "# __tanh__\n",
        "\n",
        "# backward() - topological sort\n",
        "\n",
        "# Compare to Pytorch with micrograd/test/test_engine.py\n",
        "\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Neural Network Library\n",
        "\n",
        "# Module\n",
        "# Neuron\n",
        "# Layer\n",
        "# MLP\n",
        "\n",
        "\n",
        "# ------------------------------------\n",
        "\n",
        "# 3. Training\n",
        "\n",
        "# test_mlp()\n",
        "# test_train()\n",
        "\n",
        "# Initiate model\n",
        "# Test model with one batch\n",
        "# Test data (xs, ys)\n",
        "# Training loop\n",
        "# mse loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Micrograd Engine"
      ],
      "metadata": {
        "id": "DMcxtW0Uz9jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=\"\"):\n",
        "        self.data = data\n",
        "        self.grad = 0.0\n",
        "\n",
        "        self._prev = set(_children)\n",
        "        self._backward = lambda: None\n",
        "        self._op = _op\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), \"+\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), \"*\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "\n",
        "        out = Value(self.data**other, (self,), f'**{other}' )\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0.0 if self.data <0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def tanh(self):\n",
        "        x = self.data\n",
        "        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
        "        out = Value(t, (self,), 'tanh')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1-t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def dfs(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    dfs(child)\n",
        "                topo.append(v)\n",
        "        dfs(self)\n",
        "\n",
        "        self.grad = 1.0\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return self + other\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return self * other\n",
        "\n",
        "    def __neg__(self):\n",
        "        return self * -1\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        return other + (-self)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return self * (other**-1)\n",
        "\n",
        "    def __rtruediv(self, other):\n",
        "        return other * (self**-1)\n"
      ],
      "metadata": {
        "id": "Q30LizPr-WsD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(5.0)\n",
        "b = Value(3.0)\n",
        "c = a.tanh()\n",
        "print(c)\n",
        "c.grad=1.0\n",
        "c._backward()\n",
        "print(a.grad)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iIv7DJE-sQF",
        "outputId": "da73ef97-fbea-4fe7-a4cf-4aa8a0fd82ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=0.9999092042625951, grad=0.0)\n",
            "0.0001815832309438603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare micrograd with pytorch (simple graph)"
      ],
      "metadata": {
        "id": "ukNYrElqzsca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = Value(2.0)\n",
        "x2 = Value(0.0)\n",
        "w1 = Value(-3.0)\n",
        "w2 = Value(1.0)\n",
        "b = Value(6.8813735870195432)\n",
        "\n",
        "x1w1 = x1*w1\n",
        "x2w2 = x2*w2\n",
        "x1w1x2w2 = x1w1 + x2w2\n",
        "n = x1w1x2w2 + b\n",
        "o = n.tanh()\n",
        "print(o)\n",
        "o.backward()\n",
        "\n",
        "\n",
        "print(x2.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_drtjccssnl",
        "outputId": "233b2a82-a4e8-44a9-821d-acad3200dadb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=0.7071067811865476, grad=0.0)\n",
            "0.4999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "SW8K2HhbyDLI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x1 = torch.tensor([2.0], dtype=torch.double) ; x1.requires_grad=True\n",
        "x2 = torch.tensor([0.0], dtype=torch.double) ; x2.requires_grad=True\n",
        "w1 = torch.tensor([-3.0], dtype=torch.double) ; w1.requires_grad=True\n",
        "w2 = torch.tensor([1.0], dtype=torch.double) ; w2.requires_grad=True\n",
        "b = torch.tensor([6.8813735870195432], dtype=torch.double) ; b.requires_grad=True\n",
        "\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.tanh(n)\n",
        "print(o.item())\n",
        "o.backward()\n",
        "\n",
        "print(x2.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--y9NoIouiy9",
        "outputId": "19b7bdd8-0b0a-4fed-f63c-a2cf56080244"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7071067811865476\n",
            "tensor([0.5000], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Library"
      ],
      "metadata": {
        "id": "YB9z5NLM2ABR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "        out = act.tanh()\n",
        "        return out\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout):\n",
        "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outs = [n(x) for n in self.neurons]\n",
        "        return outs[0] if len(outs)== 1 else outs\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
        "\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n"
      ],
      "metadata": {
        "id": "cMQGBUMR2XT7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "2EMIWUdpUv45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [ 2.0, 3.0, -1.0]\n",
        "\n",
        "n = MLP(3, [4, 4, 1])\n",
        "\n",
        "out = n(x)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJViu0izvfuQ",
        "outputId": "17c1db81-c334-429b-9e24-85381b4ced17"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=0.6569427405516585, grad=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [[ 2.0, 3.0, -1.0],\n",
        "      [ 1.0, -5.0, 1.0],\n",
        "      [ -2.0, 2.0, -1.0],\n",
        "      [ 4.0, 0.0, -2.0]]\n",
        "\n",
        "ys = [1.0, -1.0, -1.0, 1.0]\n",
        "\n"
      ],
      "metadata": {
        "id": "6DwVn29Jv5iU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(200):\n",
        "\n",
        "    ypred = [n(x) for x in xs]\n",
        "\n",
        "    loss = sum((ygt - yout)**2 for ygt, yout in zip(ys, ypred))\n",
        "\n",
        "    if i % 10 ==0:\n",
        "        print(f\"step: {i}, loss: {loss.data}\")\n",
        "\n",
        "    n.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in n.parameters():\n",
        "        p.data += -0.1 * p.grad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgdODcrxwTHD",
        "outputId": "5da52893-7d7a-48c3-b492-16f71247e9e2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 0.0074704852292340595\n",
            "step: 10, loss: 0.005207816272289852\n",
            "step: 20, loss: 0.004016421118811813\n",
            "step: 30, loss: 0.003277972254928078\n",
            "step: 40, loss: 0.002773866493847907\n",
            "step: 50, loss: 0.0024069798459459735\n",
            "step: 60, loss: 0.002127504748165722\n",
            "step: 70, loss: 0.0019072283361321052\n",
            "step: 80, loss: 0.001728954056751397\n",
            "step: 90, loss: 0.0015815916238825045\n",
            "step: 100, loss: 0.0014576608611143716\n",
            "step: 110, loss: 0.0013519290420868763\n",
            "step: 120, loss: 0.0012606230678682245\n",
            "step: 130, loss: 0.0011809519190026786\n",
            "step: 140, loss: 0.0011108054570466788\n",
            "step: 150, loss: 0.001048557916129813\n",
            "step: 160, loss: 0.0009929359005978148\n",
            "step: 170, loss: 0.0009429274300054031\n",
            "step: 180, loss: 0.000897717849936193\n",
            "step: 190, loss: 0.0008566437689616818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n"
      ],
      "metadata": {
        "id": "AOG4XVh3VRXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The benefits of learning micrograd\n",
        "\n",
        "Studying this 'homemade' autograd engine is beneficial for understanding the principles underlying more complex production autograd engines like PyTorch's. Here are some key features that make this code worth studying:\n",
        "\n",
        "### 1. **Core Concepts of Autograd:**\n",
        "   - **Automatic Differentiation:** The primary purpose of this code is to automatically compute gradients, which is crucial for training neural networks through backpropagation. Understanding this concept is foundational for comprehending how frameworks like PyTorch handle gradient computation.\n",
        "   - **Gradient Accumulation:** The code demonstrates how gradients are accumulated through the `_backward` method for various operations. This is essential for understanding how gradients flow through the computational graph.\n",
        "\n",
        "### 2. **Computational Graph Construction:**\n",
        "   - **Node Representation:** Each `Value` object represents a node in the computational graph, storing data, gradient, and references to parent nodes (`_prev`). This mirrors how tensors in PyTorch are tracked in the computational graph.\n",
        "   - **Operation Overloading:** By overloading operators (e.g., `__add__`, `__mul__`, `__pow__`), the code constructs the computational graph dynamically as operations are performed on `Value` instances. This is akin to PyTorch's dynamic graph construction.\n",
        "\n",
        "### 3. **Backward Propagation:**\n",
        "   - **Topological Sorting:** The `backward` method includes a topological sort to ensure that gradients are computed in the correct order. This is crucial for understanding how backward passes are structured in more complex frameworks.\n",
        "   - **Chain Rule Application:** The code explicitly applies the chain rule to compute gradients, providing a clear, step-by-step illustration of how this mathematical principle is implemented programmatically.\n",
        "\n",
        "### 4. **Extensibility and Debugging:**\n",
        "   - **Custom Operations:** The inclusion of a variety of operations (addition, multiplication, power, ReLU) and their corresponding gradient computations serves as a basis for extending the engine with more complex functions.\n",
        "   - **Debugging and Visualization:** The `_op` attribute and the `_backward` method provide hooks for debugging and visualizing the computational graph, which is useful for understanding and troubleshooting the behavior of the autograd system.\n",
        "\n",
        "### 5. **Simplicity and Clarity:**\n",
        "   - **Minimalist Design:** The simplicity of this code, with its minimal dependencies and straightforward implementation, makes it an excellent educational tool. It distills the core ideas of an autograd engine without the complexity of a full-fledged framework.\n",
        "   - **Clear and Readable:** The code is written in a clear and readable manner, making it accessible for learners who are new to the concept of automatic differentiation and computational graphs.\n",
        "\n",
        "### 6. **Practical Understanding:**\n",
        "   - **Hands-On Experience:** Implementing and modifying this code provides practical experience that can deepen one's understanding of how autograd systems work in practice. This hands-on approach can bridge the gap between theoretical knowledge and real-world application."
      ],
      "metadata": {
        "id": "5hLxkf_JVYqP"
      }
    }
  ]
}