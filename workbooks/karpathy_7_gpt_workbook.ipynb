{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6TyGyfE7yLG7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This workbook is based on Andrej Karpathy's YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "\n",
        "## Purpose of this Workbook\n",
        "This notebook serves as a personal educational tool to reinforce learning and practice building a transformer from scratch.\n",
        "\n",
        "## Suggested use\n",
        "Delete everything but the list of instructions below, and good luck!!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3VhVGiPjWA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of instructions"
      ],
      "metadata": {
        "id": "pMifN8YNyDBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNF72tZHFHhk"
      },
      "outputs": [],
      "source": [
        "# 1. Imports & config\n",
        "# 2. Download dataset\n",
        "# 3. Vocabulary\n",
        "# 4. Tokenizer (encode and decode)\n",
        "# 5. Train and Test Splits\n",
        "# 6. Dataloader *\n",
        "# 7. Model: Embedding Layer and Output Linear Transformation\n",
        "# 8. Generate Function\n",
        "# 9. Evaluation Loop\n",
        "# 10. Training Loop\n",
        "# 11. Model: Positional Embeddings\n",
        "# 12. Model: Single Attention Head\n",
        "# 13. Model: Multi-head Attention\n",
        "# 14. Model: Multihead Attention Projection Layer\n",
        "# 15. Model: MLP\n",
        "# 16: Model: Transformer block\n",
        "# 17: Model: Skip connections\n",
        "# 18: Model: Layer Normalization\n",
        "# 19: Model: Dropout\n",
        "\n",
        "# Extras\n",
        "# 1. Self Attention from first principles"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "6TyGyfE7yLG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & config\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "6SLsb3C7HZex"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cgZssnxIioo",
        "outputId": "b940317a-b9f1-402c-defb-fd9c30832a19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-18 15:50:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-18 15:50:42 (32.2 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Vocabulary\n",
        "\n",
        "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCZwWJG9IvDh",
        "outputId": "f28bceee-2e36-495c-ad11-d879d76df1ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenizer (encode and decode)\n",
        "\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode('hello world!'))\n",
        "print(decode(encode('hello world!')))\n",
        "\n",
        "# def decode: lambda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J2AGaYkMhgV",
        "outputId": "1b0c5878-d9da-4a1e-c917-9fc2f55cfe4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
            "hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train and Test Splits\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WXb2gSjPUuW",
        "outputId": "c11c5c87-7a73-4ddb-cc64-dd90cc7f5565"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Dataloader\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split=='train' else val_data\n",
        "\n",
        "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i: i+ block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1: i+ block_size+1] for i in ix], dim=0)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "print(get_batch('train'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv2ijHYqTER3",
        "outputId": "496dc33b-5a57-4036-c068-8d41179dbc87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]]), tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Model: Embedding Layer and Output Linear Transformation\n",
        "# 8. Generate Function\n",
        "\n",
        "n_embed = 32\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        logits = self.lm_head(tok_emb) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQyS9lzFajzj",
        "outputId": "a726a2b9-965f-4019-a8f2-4ba003552c47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation Loop\n",
        "# 10. Training Loop\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "learning_rate=1e-3\n",
        "training_iters=1000\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3MsgzKieVIb",
        "outputId": "dc8a74a8-7343-4d6e-8490-5dbb6e393e74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " training loss: 4.396236419677734, eval loss 4.392512321472168\n",
            " training loss: 3.7682690620422363, eval loss 3.8132052421569824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbEGDWiPvWpU",
        "outputId": "124b849f-ab6a-43ac-d690-da082b48ef29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "nGFrSwIrsSp! BeMn!yf,Mtb l!spYfNiebNw  tsxBonlrp!,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Model: Positional Embeddings\n",
        "\n",
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = tok_emb\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNJZ0iHJ1PDE",
        "outputId": "0b4cfe2f-be01-4c9f-eb1f-9dde7a69994f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            " training loss: 4.284838676452637, eval loss 4.372692108154297\n",
            " training loss: 2.9556362628936768, eval loss 2.9893593788146973\n",
            " training loss: 2.6617894172668457, eval loss 2.75193190574646\n",
            " training loss: 2.535144090652466, eval loss 2.739504337310791\n",
            " training loss: 2.560425043106079, eval loss 2.6136131286621094\n",
            " training loss: 2.4922423362731934, eval loss 2.6709134578704834\n",
            "\n",
            "Mje p st; mabind d pr, bafotauth ny l baru,\n",
            "Ae hes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Model: Single Attention Head\n",
        "\n",
        "n_embed = 32\n",
        "head_size = 32\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = Head(head_size)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOSROs_OI8l-",
        "outputId": "25e554f4-7335-464b-bf0d-76a6b80decbc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): Head(\n",
            "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.191686153411865, eval loss 4.202771186828613\n",
            " training loss: 3.0024924278259277, eval loss 2.986266851425171\n",
            " training loss: 2.6651840209960938, eval loss 2.7015931606292725\n",
            " training loss: 2.7183470726013184, eval loss 2.6373283863067627\n",
            " training loss: 2.623220443725586, eval loss 2.5721213817596436\n",
            " training loss: 2.5711042881011963, eval loss 2.5833258628845215\n",
            "\n",
            "---------------\n",
            "\n",
            "Hacounesre d;\n",
            "Bar taud tpilptit omig ty II tiveang\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Model: Multi-head Attention\n",
        "# 14. Model: Multihead Attention Projection Layer\n",
        "\n",
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for h in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embed//n_head)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKwaLSjYZG6A",
        "outputId": "a0c63444-bad4-40be-9a4e-449d6b25f611"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.203196048736572, eval loss 4.1950364112854\n",
            " training loss: 2.936199426651001, eval loss 2.9405248165130615\n",
            " training loss: 2.6581954956054688, eval loss 2.734909772872925\n",
            " training loss: 2.5741524696350098, eval loss 2.6650893688201904\n",
            " training loss: 2.6502718925476074, eval loss 2.5795183181762695\n",
            " training loss: 2.439868211746216, eval loss 2.5682873725891113\n",
            "\n",
            "---------------\n",
            "\n",
            "rfler the icot hathinlt non trit yoce chirese thos\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 8577\n",
            "Trainable Parameters: 8577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Model: MLP\n",
        "\n",
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for h in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.out_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.in_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.out_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.in_proj(x)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embed//n_head)\n",
        "        self.mlp = MLP(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A5lGN00lesp",
        "outputId": "1340a345-a2df-419d-f9ca-bb0dd525d2a0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (mlp): MLP(\n",
            "    (out_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "    (in_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (act): ReLU()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.203790187835693, eval loss 4.198258399963379\n",
            " training loss: 2.8171744346618652, eval loss 2.7808096408843994\n",
            " training loss: 2.673691987991333, eval loss 2.642836093902588\n",
            " training loss: 2.6495823860168457, eval loss 2.461919069290161\n",
            " training loss: 2.421330213546753, eval loss 2.4901633262634277\n",
            " training loss: 2.453585147857666, eval loss 2.443708658218384\n",
            "\n",
            "---------------\n",
            "\n",
            "Paingemt reanu tarl shir rarlostee.\n",
            "\n",
            "\n",
            "ADO:\n",
            "Tyo why\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 16929\n",
            "Trainable Parameters: 16929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16: Model: Transformer block\n",
        "\n",
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "n_layers = 2\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_head * head_size, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.up_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.down_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.mlp = MLP(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attn(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.layers = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layers) ])\n",
        "        # self.layers = nn.Sequential(\n",
        "        #     Block(n_embed, n_head=4),\n",
        "        #     Block(n_embed, n_head=4)\n",
        "        # )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.layers(x)\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtwAWZtUnFrs",
        "outputId": "ddf21593-5f7f-4c5f-d071-cd0099180bb4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.218210220336914, eval loss 4.217452049255371\n",
            " training loss: 3.169309139251709, eval loss 3.3483188152313232\n",
            " training loss: 3.014867067337036, eval loss 2.9642093181610107\n",
            " training loss: 2.7593791484832764, eval loss 2.700700283050537\n",
            " training loss: 2.706625461578369, eval loss 2.64189076423645\n",
            " training loss: 2.5703539848327637, eval loss 2.645573616027832\n",
            "\n",
            "---------------\n",
            "\n",
            "Whhaltng or my an iuln houlorp'j cho out?\n",
            "\n",
            "Wpincii\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 29377\n",
            "Trainable Parameters: 29377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17: Model: Skip connections\n",
        "# 18: Model: Layer Normalization\n",
        "# 19: Model: Dropout\n",
        "\n",
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "n_layers = 2\n",
        "\n",
        "dropout=0.1\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_head * head_size, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.up_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.down_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.mlp = MLP(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.layers = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layers) ])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.layers(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HD-TRscuuGR",
        "outputId": "3a91f0b5-0a4b-4326-9620-b99337926484"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.416268348693848, eval loss 4.4342756271362305\n",
            " training loss: 2.77500581741333, eval loss 2.7138144969940186\n",
            " training loss: 2.6020607948303223, eval loss 2.6608541011810303\n",
            " training loss: 2.368614673614502, eval loss 2.4679877758026123\n",
            " training loss: 2.5281529426574707, eval loss 2.4147896766662598\n",
            " training loss: 2.4049906730651855, eval loss 2.423767566680908\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "HETO:\n",
            "W iee sodl thelie oln ja;\n",
            "Thy, nold kest hi\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 29697\n",
            "Trainable Parameters: 29697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "tJQ1L4YDZZMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## improvements for get_batch()\n",
        "\n",
        "Here's a revised version of your code with improvements in variable naming, inline comments for clarity, and some additional refinements to make it more understandable and efficient:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "batch_size = 4\n",
        "sequence_length = 8  # Renamed for clarity\n",
        "\n",
        "def get_batch(data_split):\n",
        "    \"\"\"\n",
        "    Fetches a batch of data sequences for training or validation.\n",
        "\n",
        "    Args:\n",
        "    data_split (str): Either 'train' or 'val' to specify which dataset to use.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Two tensors (input sequences and target sequences) moved to the appropriate device.\n",
        "    \"\"\"\n",
        "    # Select the appropriate dataset based on the data split\n",
        "    dataset = train_data if data_split == 'train' else val_data\n",
        "\n",
        "    # Randomly choose starting indices for the sequences\n",
        "    start_indices = torch.randint(len(dataset) - sequence_length, (batch_size,))\n",
        "\n",
        "    # Extract input sequences using the starting indices\n",
        "    input_sequences = torch.stack([dataset[start: start + sequence_length] for start in start_indices], dim=0)\n",
        "\n",
        "    # Extract target sequences which are the next characters following the input sequences\n",
        "    target_sequences = torch.stack([dataset[start + 1: start + sequence_length + 1] for start in start_indices], dim=0)\n",
        "\n",
        "    # Move the input and target sequences to the designated computing device\n",
        "    input_sequences = input_sequences.to(device)\n",
        "    target_sequences = target_sequences.to(device)\n",
        "\n",
        "    return input_sequences, target_sequences\n",
        "```\n",
        "\n",
        "### Improvements made:\n",
        "\n",
        "1. **Variable Names**: Improved variable names like `batch_size` and `sequence_length` to enhance code readability.\n",
        "2. **Function Parameters**: Changed `split` to `data_split` to clearly indicate that it specifies which part of the dataset to use.\n",
        "3. **Documentation**: Added a docstring to the function explaining what it does, its parameters, and what it returns.\n",
        "4. **Usage of Start Indices**: Renamed `ix` to `start_indices` for clarity, indicating that these are starting points for the sequences.\n",
        "5. **Comments**: Added inline comments to explain important steps in the function.\n",
        "\n",
        "Make sure to define `train_data`, `val_data`, and `device` outside this function to avoid errors when it's called. These should be set according to your dataset and PyTorch device configuration."
      ],
      "metadata": {
        "id": "k6w0dhIdZbdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `.backward()` function in PyTorch\n",
        "\n",
        "The `.backward()` function in PyTorch is a fundamental method for performing backpropagation, which is critical for training neural networks. It automates the calculation of gradients for all tensors in the network that have `requires_grad` set to `True`. This enables the model to update its parameters according to the gradient descent algorithm during training.\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "In PyTorch, each tensor has a `.grad` attribute that holds the gradients computed during backpropagation. When you call `.backward()` on a tensor, PyTorch computes the gradients of that tensor with respect to all tensors that have `requires_grad=True`.\n",
        "\n",
        "For a typical loss tensor computed by comparing the model’s predictions to the true values, calling `loss.backward()` will calculate the gradients of the loss with respect to all model parameters. This is because the loss is the endpoint of the computation graph, and backpropagation needs to trace gradients from this endpoint back to the inputs.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "- **Computational Graph**: PyTorch builds a dynamic computational graph as your code executes. This graph contains all the operations performed on tensors, and it is used to compute gradients during backpropagation.\n",
        "- **Gradient Accumulation**: PyTorch accumulates gradients every time `.backward()` is called. This means that if `.backward()` is called multiple times without resetting the gradients, the gradients from each call will add up in the `.grad` attributes. This is particularly useful for scenarios like accumulating gradients over multiple batches of data.\n",
        "- **Zeroing Gradients**: Because gradients accumulate, you usually need to manually set the gradients to zero before each optimization step using `optimizer.zero_grad()` or `tensor.grad.zero_()`.\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- **gradient**: This is an optional parameter that can be passed to `.backward()`. It allows for weighting the gradient during the computation. For example, if the tensor on which `.backward()` is called is not a scalar (i.e., it has more than one element), you must pass a `gradient` argument specifying the tensor of weights.\n",
        "- **retain_graph**: By default, PyTorch frees the computational graph after backpropagation is done (to save memory). However, if you need to do several backward passes on the same graph, you must set `retain_graph=True`.\n",
        "\n",
        "### Example Code\n",
        "\n",
        "Here’s a simple example illustrating the use of `.backward()`:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create tensors.\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "\n",
        "# Perform an operation.\n",
        "z = x * y\n",
        "\n",
        "# Sum to get a scalar output.\n",
        "s = z.sum()\n",
        "\n",
        "# Backpropagate.\n",
        "s.backward()\n",
        "\n",
        "# Print gradients.\n",
        "print(x.grad)  # Output will be the values of y because ds/dx = y.\n",
        "print(y.grad)  # Output will be the values of x because ds/dy = x.\n",
        "```\n",
        "\n",
        "In this example, `s.backward()` computes the gradients of `s` with respect to `x` and `y`, which are stored in `x.grad` and `y.grad`, respectively.\n",
        "\n",
        "Understanding `.backward()` and its role in gradient computation and backpropagation is crucial for effectively training models using PyTorch."
      ],
      "metadata": {
        "id": "V_qj4Mt4w8Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors.\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "\n",
        "# Perform an operation.\n",
        "z = x * y\n",
        "\n",
        "# Sum to get a scalar output.\n",
        "s = z.sum()\n",
        "\n",
        "# Backpropagate.\n",
        "s.backward()\n",
        "\n",
        "# Print gradients.\n",
        "print(x.grad)  # Output will be the values of y because ds/dx = y.\n",
        "print(y.grad)  # Output will be the values of x because ds/dy = x.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-8svEew1iJ",
        "outputId": "1dfca205-186b-4216-b1de-f6cad1d3ffe1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4., 5., 6.])\n",
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    }
  ]
}