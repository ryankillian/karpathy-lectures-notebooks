{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook follows Andrej Karpathy's [Let's build the GPT Tokenizer\n",
        "](https://www.youtube.com/watch?v=zduSFxRajkE) Youtube video, with some interesting links and plenty of explanatory notes generated by ChatGPT\n",
        "\n",
        "[Andrej Karpathy's colab](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing#scrollTo=pkAPaUCXOhvW)\n",
        "\n",
        "As always, you should watch the video and work through it yourself for maximum benefit.\n",
        "\n",
        "\n",
        "Other Links:\n",
        "\n",
        "\n",
        "[GPT-2 paper: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "\n",
        "[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)"
      ],
      "metadata": {
        "id": "053KdegDTj3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization webapp\n",
        "\n",
        "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n"
      ],
      "metadata": {
        "id": "dykoPb4hXAXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unicode\n",
        "\n",
        "[A Programmer’s Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
        "\n",
        "[UTF-8 Everywhere](https://utf8everywhere.org/)"
      ],
      "metadata": {
        "id": "q8SZd__HbykK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bTZSfMGTJDf",
        "outputId": "d9c932a3-532e-4886-c345-0479025dfc67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104, 101, 108, 108, 111]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "[ord(x)for x in \"hello\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chr(104)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RegvheBBYksR",
        "outputId": "c59f2350-2bcc-4cdb-a421-59271ee9aed8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(\"h\".encode(\"utf-8\")))\n",
        "print(list(\"h\".encode(\"utf-16\")))\n",
        "print(list(\"h\".encode(\"utf-32\")))\n",
        "print(list(\"hello\".encode(\"utf-8\")))\n",
        "print(list(\"hello\".encode(\"utf-16\")))\n",
        "print(list(\"hello\".encode(\"utf-32\")))\n",
        "print(list(\"😄\".encode(\"utf-8\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i7VaGN2oo0d",
        "outputId": "6e25fa27-6d3f-4df5-e5e3-d4331c6a6dc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104]\n",
            "[255, 254, 104, 0]\n",
            "[255, 254, 0, 0, 104, 0, 0, 0]\n",
            "[104, 101, 108, 108, 111]\n",
            "[255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0]\n",
            "[255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0]\n",
            "[240, 159, 152, 132]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"😄 hello\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(list(tokens))\n",
        "print('---')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xataaAp_09AQ",
        "outputId": "e8d3ded2-e9b8-4113-a412-96c79b5f8bbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "[240, 159, 152, 132, 32, 104, 101, 108, 108, 111]\n",
            "---\n",
            "😄 hello\n",
            "length: 7\n",
            "---\n",
            "[240, 159, 152, 132, 32, 104, 101, 108, 108, 111]\n",
            "length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLh4pn04037z",
        "outputId": "7ae6668b-08e7-4d20-d72d-ee524cd631bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
            "length: 533\n",
            "---\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte-pair encoding\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Byte_pair_encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)"
      ],
      "metadata": {
        "id": "-dLqgR3ErMOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "# print(stats)\n",
        "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX2xrWTg55J9",
        "outputId": "26213284-026c-451c-c984-a7365fa87583"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_pair = max(stats, key=stats.get)\n",
        "top_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9bpOUeV6E7q",
        "outputId": "5e9cf979-c9f8-4942-c30d-186a56206471"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(ids, pair, idx):\n",
        "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    # if we are not at the very last position AND the pair matches, replace it\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
        "\n",
        "tokens2 = merge(tokens, top_pair, 256)\n",
        "print(tokens2)\n",
        "print(\"length:\", len(tokens2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGJveTHq6Klo",
        "outputId": "afa763e1-606f-41a9-840e-dc073cf499de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 99, 9, 1]\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Full tokenizer training text\n",
        "\n",
        "# making the training text longer to have more representative token statistics\n",
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\"\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xr1lcbspM2vF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the tokenizer using Byte-Pair Encoding"
      ],
      "metadata": {
        "id": "ZlAQJ9nz5zjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 276 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rSDCZcRNAVm",
        "outputId": "b848bec1-fe8c-425a-bf1e-9c3eb4ffe877"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into a new token 256\n",
            "merging (105, 110) into a new token 257\n",
            "merging (115, 32) into a new token 258\n",
            "merging (116, 104) into a new token 259\n",
            "merging (101, 114) into a new token 260\n",
            "merging (99, 111) into a new token 261\n",
            "merging (116, 32) into a new token 262\n",
            "merging (226, 128) into a new token 263\n",
            "merging (44, 32) into a new token 264\n",
            "merging (97, 110) into a new token 265\n",
            "merging (111, 114) into a new token 266\n",
            "merging (100, 32) into a new token 267\n",
            "merging (97, 114) into a new token 268\n",
            "merging (101, 110) into a new token 269\n",
            "merging (257, 103) into a new token 270\n",
            "merging (261, 100) into a new token 271\n",
            "merging (121, 32) into a new token 272\n",
            "merging (46, 32) into a new token 273\n",
            "merging (97, 108) into a new token 274\n",
            "merging (259, 256) into a new token 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdF8hQEBNOXM",
        "outputId": "706daec6-4693-425a-db36-8c76dd3b9ee4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 24597\n",
            "ids length: 19438\n",
            "compression ratio: 1.27X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab"
      ],
      "metadata": {
        "id": "iMVaKu3OMr53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RV6X9wnRLmE",
        "outputId": "58bd6baa-c9d4-4a86-ae12-7fb265a59a63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(101, 32): 256, (105, 110): 257, (115, 32): 258, (116, 104): 259, (101, 114): 260, (99, 111): 261, (116, 32): 262, (226, 128): 263, (44, 32): 264, (97, 110): 265, (111, 114): 266, (100, 32): 267, (97, 114): 268, (101, 110): 269, (257, 103): 270, (261, 100): 271, (121, 32): 272, (46, 32): 273, (97, 108): 274, (259, 256): 275}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a sequence of integers in the range [0, vocab_size], what is the text?\n",
        "\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNz7o21wMtrT",
        "outputId": "11d78d58-33f0-4262-9b37-1774256cb002"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidlKHN8OZ2X",
        "outputId": "70b0b018-e7f1-4d8e-84ab-a262b3dd1ea9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b'in', 258: b's ', 259: b'th', 260: b'er', 261: b'co', 262: b't ', 263: b'\\xe2\\x80', 264: b', ', 265: b'an', 266: b'or', 267: b'd ', 268: b'ar', 269: b'en', 270: b'ing', 271: b'cod', 272: b'y ', 273: b'. ', 274: b'al', 275: b'the '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b'a'+b'c')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR68y1izUdH4",
        "outputId": "9ec8991c-1509-459f-a41a-c5e3d34b6f72"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'ac'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "rNakdZNgkltp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids):\n",
        "  # given ids (list of integers), return Python string\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "print(decode([269, 265]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tCMNLlrOLAV",
        "outputId": "667dbdc1-3fbb-4a27-a194-68da906cb8be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "K0kQkevviNtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMjNi-EeiPEe",
        "outputId": "64576897-e50d-43c0-88f8-148b5418b4a2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (105, 110): 257,\n",
              " (115, 32): 258,\n",
              " (116, 104): 259,\n",
              " (101, 114): 260,\n",
              " (99, 111): 261,\n",
              " (116, 32): 262,\n",
              " (226, 128): 263,\n",
              " (44, 32): 264,\n",
              " (97, 110): 265,\n",
              " (111, 114): 266,\n",
              " (100, 32): 267,\n",
              " (97, 114): 268,\n",
              " (101, 110): 269,\n",
              " (257, 103): 270,\n",
              " (261, 100): 271,\n",
              " (121, 32): 272,\n",
              " (46, 32): 273,\n",
              " (97, 108): 274,\n",
              " (259, 256): 275}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "    # Encode the input text as a list of UTF-8 bytes (integers).\n",
        "    # This initial list represents the initial tokens, each corresponding to a single character.\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "    # Continue attempting to merge tokens as long as there are at least two tokens left.\n",
        "    # This allows for the possibility of further merging.\n",
        "    while len(tokens) >= 2:\n",
        "        # Calculate the frequency of adjacent pairs of tokens in the current list of tokens.\n",
        "        stats = get_stats(tokens)\n",
        "\n",
        "        # Find the pair with the lowest index in the 'merges' dictionary, indicating the next merge.\n",
        "        # If a pair doesn't exist in 'merges', assign it an infinite index to prioritize pairs that can be merged.\n",
        "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "\n",
        "        # If the chosen pair does not exist in the merges dictionary, no further merging is possible.\n",
        "        # Exit the loop to prevent infinite iteration.\n",
        "        if pair not in merges:\n",
        "            break\n",
        "\n",
        "        # Retrieve the index that represents the merged pair from the merges dictionary.\n",
        "        idx = merges[pair]\n",
        "\n",
        "        # Merge the identified pair throughout the tokens, replacing occurrences of the pair with the new token.\n",
        "        tokens = merge(tokens, pair, idx)\n",
        "\n",
        "    # Return the final list of tokens after all possible merges have been applied.\n",
        "    return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "viinutZbiQSm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxVk_-QPiWNd",
        "outputId": "586fac06-c5f8-4e9e-cf94-fbc5b6253b16"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = decode(encode(text))\n",
        "print(text2 == text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OtPwOcNi3eV",
        "outputId": "9d52d780-4301-4116-dfeb-7f80214d5862"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining GPT-2 tokenizer implementation\n",
        "\n",
        "\n",
        "[GPT-2 paper: Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "\n",
        "[GPT-2 Tokenizer implementation](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
        "\n"
      ],
      "metadata": {
        "id": "oijZjXvbmZm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZJ8o5qrr2YS",
        "outputId": "3a5fa569-7703-4192-f7d8-5672a1dde7d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"\n",
        "for i in range(1, 101):\n",
        "    if i % 3 == 0 and i % 5 == 0:\n",
        "        print(\"FizzBuzz\")\n",
        "    elif i % 3 == 0:\n",
        "        print(\"Fizz\")\n",
        "    elif i % 5 == 0:\n",
        "        print(\"Buzz\")\n",
        "    else:\n",
        "        print(i)\n",
        "\"\"\"\n",
        "print(re.findall(gpt2pat, example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlHaENjlr9Sh",
        "outputId": "b85f7c6b-a0cd-42b0-e9ba-3498df7f43f1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiktoken library from OpenAI\n",
        "\n",
        "https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py"
      ],
      "metadata": {
        "id": "oGhQVYTQsP-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken # added for colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_7GRpFcsDBU",
        "outputId": "d56d00f8-7e9c-4a62-f752-12af12aae54c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# GPT-2 (does not merge spaces)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"    hello world!!!\"))\n",
        "\n",
        "# GPT-4 (merges spaces)\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"    hello world!!!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VlB1QFCsH0A",
        "outputId": "8117576d-97b2-4ab8-f540-e6fab2e6572b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 220, 23748, 995, 10185]\n",
            "[262, 24748, 1917, 12340]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Vocab and Merges\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7KHjtBWz3-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OhDPMdSz86b",
        "outputId": "90fe1f22-7ca3-49a2-9f9c-841fb1f72914"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-22 09:52:01--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.209.18.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.209.18.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‘vocab.bpe’\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K  1.59MB/s    in 0.3s    \n",
            "\n",
            "2024-03-22 09:52:01 (1.59 MB/s) - ‘vocab.bpe’ saved [456318/456318]\n",
            "\n",
            "--2024-03-22 09:52:01--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.209.18.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.209.18.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘encoder.json’\n",
            "\n",
            "encoder.json        100%[===================>]   1018K  2.35MB/s    in 0.4s    \n",
            "\n",
            "2024-03-22 09:52:02 (2.35 MB/s) - ‘encoder.json’ saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "with open('encoder.json', 'r') as f:\n",
        "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
        "\n",
        "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "    bpe_data = f.read()\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "# ^---- ~equivalent to our \"merges\""
      ],
      "metadata": {
        "id": "7DpGiCC_0E-y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Special Tokens"
      ],
      "metadata": {
        "id": "XbqidY3e1hO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoder) # 256 raw byte tokens. 50,000 merges. +1 special token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypuKab4e1kIs",
        "outputId": "7b21b4cb-db10-4a80-d7ff-90d42edfa6f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder['<|endoftext|>'] # the only special token in use for the GPT-2 base model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqgSjngDk669",
        "outputId": "505a189c-ef09-4441-c6c7-38089cbea3a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/openai/tiktoken/blob/main/src/lib.rs\n"
      ],
      "metadata": {
        "id": "Kpb8Iia7mCU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minbpe\n",
        "\n",
        "[https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)"
      ],
      "metadata": {
        "id": "NSfrmnn4mfnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # GPT-4 tokenizer\n",
        "print(enc.encode(\"안녕하세요 👋 (hello in Korean!)\"))\n",
        "print(enc.decode(enc.encode(\"안녕하세요 👋 (hello in Korean!)\")) == \"안녕하세요 👋 (hello in Korean!)\")\n",
        "# match the above for your own tokenizer, and also implement a train() function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSWtNocQE-K6",
        "outputId": "e1d8caf9-f13b-4721-cf52-960712d370af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31495, 230, 75265, 243, 92245, 62904, 233, 320, 15339, 304, 16526, 16715]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Piece\n",
        "\n",
        "[https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)\n",
        "\n"
      ],
      "metadata": {
        "id": "XPNtw3pPFWds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "vsdgyRpSFbvE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a toy.txt file with some random text\n",
        "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
      ],
      "metadata": {
        "id": "RZMDFHOUGMdM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)\n"
      ],
      "metadata": {
        "id": "YIY3MTADGV1w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Zz0Y4TGbF9",
        "outputId": "ef149abd-52d2-4c20-c598-933f96e8367b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['en', 259],\n",
              " ['▁t', 260],\n",
              " ['ce', 261],\n",
              " ['in', 262],\n",
              " ['ra', 263],\n",
              " ['▁a', 264],\n",
              " ['de', 265],\n",
              " ['er', 266],\n",
              " ['▁s', 267],\n",
              " ['ent', 268],\n",
              " ['or', 269],\n",
              " ['pr', 270],\n",
              " ['▁m', 271],\n",
              " ['▁u', 272],\n",
              " ['ing', 273],\n",
              " ['▁th', 274],\n",
              " ['ence', 275],\n",
              " ['entence', 276],\n",
              " ['Pi', 277],\n",
              " ['ed', 278],\n",
              " ['em', 279],\n",
              " ['ex', 280],\n",
              " ['is', 281],\n",
              " ['iz', 282],\n",
              " ['la', 283],\n",
              " ['on', 284],\n",
              " ['st', 285],\n",
              " ['▁S', 286],\n",
              " ['Pie', 287],\n",
              " ['end', 288],\n",
              " ['ext', 289],\n",
              " ['▁an', 290],\n",
              " ['▁pr', 291],\n",
              " ['▁to', 292],\n",
              " ['▁un', 293],\n",
              " ['▁the', 294],\n",
              " ['Piece', 295],\n",
              " ['▁Sentence', 296],\n",
              " ['▁SentencePiece', 297],\n",
              " ['.]', 298],\n",
              " ['Ne', 299],\n",
              " ['ag', 300],\n",
              " ['do', 301],\n",
              " ['ec', 302],\n",
              " ['gu', 303],\n",
              " ['ic', 304],\n",
              " ['ir', 305],\n",
              " ['it', 306],\n",
              " ['ly', 307],\n",
              " ['to', 308],\n",
              " ['▁(', 309],\n",
              " ['▁[', 310],\n",
              " ['▁f', 311],\n",
              " ['▁n', 312],\n",
              " ['▁w', 313],\n",
              " ['.])', 314],\n",
              " ['age', 315],\n",
              " ['del', 316],\n",
              " ['ion', 317],\n",
              " ['ken', 318],\n",
              " ['lan', 319],\n",
              " ['ral', 320],\n",
              " ['wor', 321],\n",
              " ['yst', 322],\n",
              " ['▁Ne', 323],\n",
              " ['▁al', 324],\n",
              " ['▁de', 325],\n",
              " ['▁is', 326],\n",
              " ['▁ma', 327],\n",
              " ['▁mo', 328],\n",
              " ['izer', 329],\n",
              " ['rain', 330],\n",
              " ['ural', 331],\n",
              " ['▁and', 332],\n",
              " ['▁lan', 333],\n",
              " ['▁pre', 334],\n",
              " ['guage', 335],\n",
              " ['ystem', 336],\n",
              " ['▁text', 337],\n",
              " ['▁model', 338],\n",
              " ['▁train', 339],\n",
              " ['kenizer', 340],\n",
              " ['▁system', 341],\n",
              " ['▁language', 342],\n",
              " ['▁training', 343],\n",
              " ['.,', 344],\n",
              " ['BP', 345],\n",
              " ['Ku', 346],\n",
              " ['ab', 347],\n",
              " ['as', 348],\n",
              " ['at', 349],\n",
              " ['by', 350],\n",
              " ['co', 351],\n",
              " ['es', 352],\n",
              " ['et', 353],\n",
              " ['if', 354],\n",
              " ['ig', 355],\n",
              " ['im', 356],\n",
              " ['ke', 357],\n",
              " ['lo', 358],\n",
              " ['nr', 359],\n",
              " ['oc', 360],\n",
              " ['e', 361],\n",
              " ['▁', 362],\n",
              " ['n', 363],\n",
              " ['t', 364],\n",
              " ['i', 365],\n",
              " ['r', 366],\n",
              " ['a', 367],\n",
              " ['o', 368],\n",
              " ['s', 369],\n",
              " ['d', 370],\n",
              " ['c', 371],\n",
              " ['l', 372],\n",
              " ['u', 373],\n",
              " ['g', 374],\n",
              " ['m', 375],\n",
              " ['p', 376],\n",
              " ['.', 377],\n",
              " ['h', 378],\n",
              " ['-', 379],\n",
              " ['w', 380],\n",
              " ['y', 381],\n",
              " ['P', 382],\n",
              " ['S', 383],\n",
              " ['b', 384],\n",
              " ['f', 385],\n",
              " ['k', 386],\n",
              " [')', 387],\n",
              " ['x', 388],\n",
              " ['z', 389],\n",
              " ['(', 390],\n",
              " ['N', 391],\n",
              " ['[', 392],\n",
              " [']', 393],\n",
              " ['v', 394],\n",
              " [',', 395],\n",
              " ['/', 396],\n",
              " ['B', 397],\n",
              " ['E', 398],\n",
              " ['K', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello 안녕하세요\")\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gya-UGm1GkxI",
        "outputId": "9e70f596-0b05-43f7-e15c-59ce5536f1d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLGTh64GGnaF",
        "outputId": "b8343880-10fb-4a96-a1c8-029f23cdf9a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'h', 'e', 'l', 'lo', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "5L-C8RW6jsIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a feature of UTF-8 that is particularly beneficial for software development\n",
        "\n",
        "The author is explaining a feature of UTF-8, a character encoding system, that is particularly beneficial for software development, especially when dealing with strings (sequences of characters) and files. Let's break down the points made with examples for clarity.\n",
        "\n",
        "### 1. ASCII Compatibility\n",
        "\n",
        "- **UTF-8 Property**: In UTF-8, characters from the ASCII set (which includes English letters, digits, and some common symbols) are represented as single bytes, just like in the ASCII encoding. These characters have values ranging from 0 to 127. Meanwhile, characters that are not part of the ASCII set are encoded using sequences of two or more bytes, each within the range of 128 to 255.\n",
        "\n",
        "- **Example**: The character 'A' is represented in ASCII (and thus also in UTF-8) as the decimal value 65. In contrast, the character '€' (Euro sign) is not in the ASCII set and is represented in UTF-8 by a sequence of bytes, specifically three bytes with values 226, 130, and 172.\n",
        "\n",
        "### 2. No Conversion Required for ASCII\n",
        "\n",
        "- **Point Explained**: Since UTF-8 and ASCII represent characters from the ASCII set in the same way, any file or string that is encoded in ASCII does not need to be converted when it is treated as UTF-8. This ensures compatibility and simplifies handling text data that only contains these characters.\n",
        "\n",
        "- **Example**: If you have a text file containing only English letters and punctuation (all within the ASCII set), it can be opened, read, and processed by a program expecting UTF-8 encoded text without any issues or special handling.\n",
        "\n",
        "### 3. Programming Idioms Work as Expected\n",
        "\n",
        "- **Idioms Mentioned**: Programming idioms related to strings like null termination (using a byte with value 0 to mark the end of a string) and delimiters (using specific characters like newlines, tabs, commas, etc., to separate data) work seamlessly with UTF-8.\n",
        "\n",
        "- **Why They Work**: In UTF-8, the bytes representing ASCII characters (including control characters used as delimiters or terminators) cannot appear as part of the sequence representing a non-ASCII character. This means that when a program searches a UTF-8 string for a specific ASCII character (like a newline or a null byte), it can be sure that it's found an actual delimiter or end of the string, not just part of a different, multi-byte character.\n",
        "\n",
        "- **Example**: Consider a UTF-8 encoded text file with a mix of English text and emoji. Even though emojis are encoded with multiple bytes, you can still reliably use a single byte representing the newline character (value 10) to find the end of lines, because this byte value will not be part of any multi-byte sequence representing an emoji.\n",
        "\n",
        "In summary, the author highlights UTF-8's backward compatibility with ASCII and how this compatibility ensures that traditional string handling techniques continue to work as expected, even in a system that supports a much wider range of characters."
      ],
      "metadata": {
        "id": "HfLqXk53jtp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte-Pair Encoding (BPE) as a solution to the challenges of tokenizing text data for language models\n",
        "\n",
        "The lecturer introduces Byte-Pair Encoding (BPE) as a solution to the challenges of tokenizing text data for language models. The intuition behind this approach stems from a few key considerations:\n",
        "\n",
        "1. **Need for Tokenization**: Language models, particularly those based on the Transformer architecture, require input in the form of integers, not raw strings. These integers correspond to tokens derived from the text, which in turn map to vectors in a lookup table. The vectors are what actually get fed into the Transformer.\n",
        "\n",
        "2. **Support for Multilingual and Special Characters**: The goal is to accommodate a wide range of languages and special characters (including emojis), not just the English alphabet. This requires a system that can handle the vast array of Unicode characters, which are standardized representations of text characters globally.\n",
        "\n",
        "3. **Challenges with Direct Unicode Utilization**: While Unicode provides a comprehensive set of over 150,000 characters across various languages and symbols, using their code points (unique numbers assigned to each character) directly as tokens is impractical. The reasons are twofold: the Unicode standard is continuously evolving, adding new characters and thus making the set of code points unstable over time; and directly using Unicode code points would result in an excessively large vocabulary size for the model to handle efficiently.\n",
        "\n",
        "4. **Encoding Methods**: UTF-8, UTF-16, and UTF-32 are different methods of encoding Unicode characters into binary data. UTF-8 is preferred for its efficiency and compatibility with ASCII, but using it (or any encoding scheme) directly for tokenization presents issues. Specifically, UTF-8 encodes characters into a variable number of bytes, which would lead to an impractically small vocabulary if each byte were treated as a separate token. This would also result in excessively long sequences of tokens for relatively short strings of text, making processing by the Transformer inefficient.\n",
        "\n",
        "5. **Byte-Pair Encoding as a Solution**: BPE addresses these challenges by compressing sequences of bytes into tokens in a way that balances the size of the vocabulary with the length of the tokenized sequences. It operates by iteratively merging the most frequent pairs of bytes (or byte sequences) in the text data into single new tokens. This method allows for efficient encoding of common sequences (including those representing multilingual text and special characters) into a manageable number of tokens, thereby facilitating efficient processing by language models without sacrificing the ability to represent a wide range of text.\n",
        "\n",
        "In summary, the lecturer's intuition for introducing BPE is to find a pragmatic middle ground that enables language models to process diverse text efficiently, by overcoming the limitations of direct Unicode utilization and taking advantage of the flexibility and efficiency of UTF-8, all while maintaining a manageable vocabulary size and sequence length."
      ],
      "metadata": {
        "id": "0OV9Q5HKqeb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The lecturer's explanation on Byte-Pair Encoding (BPE)\n",
        "\n",
        "The lecturer's explanation on Byte-Pair Encoding (BPE) and its implementation for compressing text sequences into a manageable size for language models illuminates several main goals and intuitions:\n",
        "\n",
        "### 1. **Efficiency in Text Representation**:\n",
        "- **Goal**: To represent a lengthy sequence of characters or bytes in a more compact form without losing information.\n",
        "- **Intuition**: By merging frequently occurring pairs of characters (or bytes) into single new tokens and adding these to the vocabulary, the original sequence can be significantly compressed. This process reduces the length of the input sequence while slightly expanding the vocabulary.\n",
        "\n",
        "### 2. **Adaptability to Language Variability**:\n",
        "- **Goal**: To handle the vast diversity of languages and special characters (including emojis) efficiently in text processing for language models.\n",
        "- **Intuition**: BPE's dynamic vocabulary construction through the iterative merging process is adaptable to the peculiarities of different languages and symbols. This flexibility ensures that the encoding scheme can efficiently represent varied linguistic elements without overly inflating the sequence length or vocabulary size.\n",
        "\n",
        "### 3. **Optimization of Computational Resources**:\n",
        "- **Goal**: To manage the trade-off between vocabulary size and sequence length for the sake of computational efficiency in training and using language models.\n",
        "- **Intuition**: A balanced approach to tokenization, where the vocabulary size is not too large to handle but the sequence length is not excessively long, optimizes the use of computational resources. It ensures that language models can process data efficiently, benefiting from both a rich vocabulary and manageable sequence lengths.\n",
        "\n",
        "### 4. **Preparation for Language Model Training**:\n",
        "- **Goal**: To preprocess text data in a way that makes it suitable for training state-of-the-art language models, particularly those based on Transformer architectures.\n",
        "- **Intuition**: By converting raw text into a sequence of tokens that a language model can understand and process, BPE facilitates the efficient training and operation of these models. This preprocessing stage is crucial for enabling models to learn from and generate human-like text.\n",
        "\n",
        "### 5. **Support for Dynamic and Evolving Language Use**:\n",
        "- **Goal**: To ensure the tokenization scheme can evolve with the language, accommodating new characters, symbols, or usage patterns that emerge over time.\n",
        "- **Intuition**: The iterative nature of BPE allows for continuous updates to the vocabulary in response to changes in language use. This adaptability is key to maintaining the relevance and effectiveness of language models trained on diverse and evolving text data.\n",
        "\n",
        "In conclusion, the lecturer's exploration of BPE and its implementation underscores a strategic approach to text tokenization aimed at enhancing the performance and efficiency of language models. This methodology is pivotal for dealing with the complexity and diversity of human language in computational contexts."
      ],
      "metadata": {
        "id": "vgPQ9iJ27Cw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a byte string in Python\n",
        "\n",
        "The notation `b'the '` represents a byte string in Python. A byte string is a sequence of bytes – essentially, a sequence of 8-bit values – that is used for storing binary data, including text that is encoded in a specific encoding scheme like UTF-8, ASCII, etc.\n",
        "\n",
        "### Byte-String Explained\n",
        "\n",
        "- **Syntax**: In Python, a byte string is denoted by a leading `b` before the opening quote of the string literal, e.g., `b'the '`. This indicates that what follows is not a standard string (which in Python 3 is Unicode) but a sequence of bytes.\n",
        "- **Content**: For the example `b'the '`, it contains the bytes that represent the characters `t`, `h`, `e`, and a space in ASCII encoding. The ASCII encoding represents each character as a single byte, so this byte string is composed of 4 bytes in total.\n",
        "\n",
        "### Comparison with Standard Strings\n",
        "\n",
        "- **Standard String (`'the '`)**: In Python 3, a standard string is a sequence of Unicode characters. It can include characters from practically any writing system in the world, from Latin letters to Chinese characters and emojis. These characters are encoded internally by Python using a variable-length encoding, which can require one or more bytes per character.\n",
        "- **Byte String (`b'the '`)**: A byte string is explicitly a sequence of bytes, not characters. Each element in a byte string is a byte (an integer in the range 0-255), and it does not inherently carry any encoding for converting it into characters. When you use a byte string to store text, you must know what encoding was used (e.g., ASCII, UTF-8) to interpret it correctly back into text.\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **Type**: Standard strings are of type `str`, while byte strings are of type `bytes` in Python 3.\n",
        "- **Usage**: Standard strings are used for text processing and manipulation in Python, taking advantage of Python's rich set of text-handling features. Byte strings are used when dealing with raw binary data, encoding-specific data, or interfacing with systems and files that require a specific byte format.\n",
        "- **Encoding**: Standard strings can represent text in any language supported by Unicode, using a uniform encoding (UTF-8, UTF-16, etc.) that Python manages internally. Byte strings, on the other hand, represent raw bytes, which can be used to store encoded text (in UTF-8, ASCII, etc.) but are also used for binary data that is not text at all.\n",
        "\n",
        "In summary, `b'the '` is a byte string containing the ASCII-encoded bytes for the text \"the \", and it differs from the standard string `'the '` in that it explicitly represents raw bytes rather than Unicode text characters."
      ],
      "metadata": {
        "id": "tYpoWEl2U1BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adjacent byte pairs\n",
        "\n",
        "Yes, in the context of a byte-pair encoding (BPE) vocabulary, it's entirely expected to see merged values that consist of varying numbers of characters, which correspondingly vary in the number of bytes. This is indeed related to UTF-8 and the nature of byte-pair encoding itself. Here’s why:\n",
        "\n",
        "### Understanding UTF-8 Encoding\n",
        "\n",
        "UTF-8 is a variable-width character encoding for Unicode. It can encode a single character using one to four bytes, depending on the character:\n",
        "\n",
        "- **ASCII characters** (the most common in English text) are encoded using a single byte.\n",
        "- **Characters from other alphabets and symbols** might require two, three, or even four bytes.\n",
        "\n",
        "### Byte-Pair Encoding Merges\n",
        "\n",
        "In BPE, merges are based on the frequency of adjacent byte pairs. Initially, each byte in the text is considered a separate token, and BPE doesn't differentiate between single-byte characters and multi-byte character sequences:\n",
        "\n",
        "- **Single-byte characters**: These are typically ASCII characters, which can be directly represented by a single byte (e.g., `b'e'`, `b's'`).\n",
        "- **Multi-byte characters or sequences**: These result from characters outside the basic ASCII range or from combining multiple bytes (characters) into a single token through the BPE process. For example, `b'\\xe2\\x80'` represents a two-byte sequence that is part of a character or punctuation in UTF-8 encoding that requires more than one byte.\n",
        "\n",
        "### Examples of Merges\n",
        "\n",
        "- **Two ASCII characters**: For example, `b'in'`, `b'th'`. Each character is a single byte, so these merges result in two-byte sequences.\n",
        "- **Combination involving non-ASCII**: The merge `b'\\xe2\\x80'` involves bytes that individually do not represent ASCII characters but together might represent a single character or part of a character in UTF-8. For instance, `\\xe2\\x80` are the beginning bytes of characters like the em dash or quotation marks in their UTF-8 encoding.\n",
        "- **Longer sequences**: Merges like `b'ing'`, `b'the '` involve more than two characters because, as the BPE algorithm progresses, it doesn't just merge single bytes but also previously merged sequences. This is why you see some merges resulting in three or more bytes, as they combine longer sequences of characters that were frequent in the dataset.\n",
        "\n",
        "### Is This Expected?\n",
        "\n",
        "Absolutely. The variability in the number of bytes per merge reflects the algorithm’s adaptation to the specific frequencies of character (byte) sequences in the text data. It's optimizing the encoding to be more efficient for common sequences, which can include both simple character pairs and more complex combinations resulting from UTF-8's variable-length encoding of characters.\n",
        "\n",
        "This variability is a key feature of UTF-8 and BPE working together: UTF-8 allows for the efficient encoding of a vast range of characters using variable-length bytes, while BPE leverages this by dynamically merging frequent sequences, regardless of their original length in bytes."
      ],
      "metadata": {
        "id": "xEf2ZNaeXKdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Is the vocab a mapping from integers to byte strings?\n",
        "\n",
        "Yes, in the context of the byte-pair encoding (BPE) implementation you've provided, the vocabulary (`vocab`) is indeed a mapping from integers to byte strings. Let's break down your questions for clarity:\n",
        "\n",
        "### Is the vocab a mapping from integers to byte strings?\n",
        "\n",
        "- **Yes.** The vocabulary (`vocab`) is a dictionary where each key is an integer representing a token's unique ID, and the associated value is a byte string representing the token's actual content. Initially, the vocabulary is populated with single-byte tokens (for bytes 0 through 255), directly mapping each possible byte value to a corresponding byte string. As the BPE process merges pairs of tokens, new entries are added to the vocabulary, mapping newly created token IDs (integers starting from 256) to their corresponding byte strings, which represent merged sequences of bytes.\n",
        "\n",
        "### Are these byte strings also known as tokens?\n",
        "\n",
        "- **Yes.** In the context of this implementation and in general terms within text processing and NLP, the byte strings in the `vocab` can indeed be considered tokens. A token, in this sense, is a unit of text, which can range from a single character to more complex sequences of characters. Initially, tokens correspond directly to single bytes (characters in ASCII or start of UTF-8 sequences), but as the algorithm progresses, tokens represent longer sequences of bytes that reflect merged character sequences.\n",
        "\n",
        "### Are the tokens byte strings rather than bytes?\n",
        "\n",
        "- **Yes, with clarification.** Initially, each token corresponds to a single byte and is represented as a byte string of length 1. However, as the BPE algorithm merges tokens, new tokens represent sequences of bytes, thus becoming byte strings of length greater than 1. It's important to note the distinction between a single byte and a byte string:\n",
        "    - A **single byte** is an 8-bit value.\n",
        "    - A **byte string** is a sequence of bytes. In Python, this is represented by the `bytes` type, which can contain one or more bytes.\n",
        "\n",
        "The process of encoding text into tokens involves converting the original text into a sequence of these byte strings (tokens), each of which can represent one or more original bytes/characters, especially after merges. The decoding process, as implemented in the `decode` function, reverses this by concatenating the byte strings corresponding to a sequence of token IDs and then decoding the resulting byte string into a UTF-8 encoded text string.\n",
        "\n",
        "In summary, the BPE process here creates a dynamic mapping of integers to byte strings (tokens), where these tokens can represent increasingly complex sequences of characters as the algorithm progresses, starting from single bytes and potentially growing to represent entire syllables or commonly occurring word parts."
      ],
      "metadata": {
        "id": "ttkjtXw5fUTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: GPT-2 tokenizer\n",
        "\n",
        "The transcript discusses the nuances of how the GPT-2 tokenizer handles text and why specific design choices were made to optimize its performance. Here are the key points to note, especially in the context of Byte-Pair Encoding (BPE) work:\n",
        "\n",
        "1. **Use of BPE on Byte-Level Representation**: The transcript confirms that GPT-2 employs the Byte-Pair Encoding algorithm but operates at the byte level of UTF-8 encoded text. This approach allows GPT-2 to handle a wide variety of languages and symbols efficiently.\n",
        "\n",
        "2. **Avoiding Suboptimal Merges**: A critical insight shared is the potential suboptimality of merging common words with adjacent punctuation (e.g., \"dog,\" \"dog!\"), leading to an inflated vocabulary with semantically similar but technically different tokens. The GPT-2 team sought to avoid such merges, which could otherwise cluster tokens in a way that mixes semantic content with punctuation, potentially hampering the model's ability to understand and generate text accurately.\n",
        "\n",
        "3. **Manual Enforcement of Merging Rules**: To address the issue of suboptimal merges, GPT-2 introduces manual rules that prevent certain types of characters from being merged together. This approach aims to maintain a clear distinction between different types of tokens (e.g., letters, numbers, punctuation) and ensure that merges are meaningful and contribute positively to the model's performance.\n",
        "\n",
        "4. **Use of Regex Patterns**: The transcript dives into the implementation details, showing how regular expressions (regex) are used to enforce the aforementioned rules. The pattern matching is designed to segment the text into chunks that can be independently processed by the tokenizer, ensuring that merges do not occur across predefined boundaries (e.g., between letters and punctuation).\n",
        "\n",
        "5. **Extension of Python's `re` Module**: It's noted that GPT-2 uses the `regex` module (imported as `re` for familiarity), which is an extension of Python's standard `re` module, offering more powerful pattern matching capabilities. This choice allows for more complex regex patterns that are crucial for implementing GPT-2's specific tokenization rules.\n",
        "\n",
        "6. **Tokenization and Encoding Details**: The process described in the transcript involves first breaking down the input text into chunks based on the regex pattern, then encoding these chunks into tokens using the BPE algorithm, and finally concatenating the results. This method ensures that certain merges, deemed suboptimal, are explicitly prevented.\n",
        "\n",
        "7. **Inference vs. Training Code**: The transcript mentions that the code discussed pertains to the inference phase of the tokenizer, not the training phase. The training code, which would detail exactly how the tokenizer was initially built and trained, was not released by OpenAI. This distinction highlights that the inference code applies pre-determined merges to new text, while the training code, which determined those merges, remains proprietary.\n",
        "\n",
        "In summary, the transcript sheds light on the thoughtful considerations and technical strategies behind GPT-2's tokenizer design, particularly how it balances the flexibility of BPE with the need for structured and meaningful tokenization through manual rules and advanced regex patterns."
      ],
      "metadata": {
        "id": "h226tMBxyrHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: from GPT-2 to GPT-4 tokenizer\n",
        "\n",
        "The transcript discusses the transition from GPT-2 to GPT-4 tokenizer implementations by OpenAI, highlighting the changes made in the latter to improve tokenization. Here are the key points:\n",
        "\n",
        "1. **TikToken Library Introduction**: OpenAI's TikToken library is introduced as the official tool for tokenization tasks related to their language models, particularly GPT-4. This library provides inference capabilities for tokenization, meaning it can convert text into tokens based on pre-trained models but is not designed for training new tokenization models.\n",
        "\n",
        "2. **Whitespaces Handling Changes**: A notable difference between GPT-2 and GPT-4 tokenizers is how they handle whitespace. In GPT-4, whitespace characters are merged more aggressively, a change from the approach in GPT-2 where whitespace remained largely unmerged. This adjustment potentially impacts how text is segmented and could influence the model's understanding of text structure.\n",
        "\n",
        "3. **Regular Expression (Regex) Modifications**: The transcript points out that GPT-4's tokenizer utilizes a modified regex pattern for chunking text compared to GPT-2. This change is central to the new tokenizer's performance and affects how text is preprocessed for tokenization.\n",
        "\n",
        "4. **Case Insensitivity and Apostrophe Handling**: An important update in GPT-4's regex pattern is the introduction of case-insensitive matching, indicated by the 'i' flag. This alteration ensures that contractions and possessives using apostrophes are consistently recognized regardless of letter casing, addressing a limitation observed in the GPT-2 tokenizer.\n",
        "\n",
        "5. **Numeric Sequences Treatment**: GPT-4's tokenizer limits the merging of numeric sequences to those with up to three digits. This decision prevents the formation of tokens from longer numeric sequences, which could be less useful and more sparsely represented in the training data. This change suggests an optimization aimed at improving token efficiency and model performance on numerical data.\n",
        "\n",
        "6. **Documentation and Transparency**: The transcript expresses a concern over the lack of detailed documentation and rationale behind specific changes and decisions in the tokenizer's development. It underscores the challenges in understanding the motivations and implications of these updates without official explanations from OpenAI.\n",
        "\n",
        "7. **Vocabulary Size Increase**: GPT-4's tokenizer expands the vocabulary size from approximately 50,000 to around 100,000 tokens. This significant increase allows for a richer representation of text, potentially enabling more nuanced understanding and generation capabilities in the GPT-4 model.\n",
        "\n",
        "8. **Exploration and Analysis Approach**: The transcript conveys a hands-on approach to exploring the tokenizer's behavior and changes through examination of the code and regex patterns. However, it also highlights the complexity and nuanced understanding required to fully grasp the tokenizer's inner workings, suggesting the use of tools like ChatGPT and regex documentation for deeper analysis.\n",
        "\n",
        "In summary, the transition from GPT-2 to GPT-4 tokenizers involves strategic adjustments to whitespace handling, regex patterns, and numeric sequence treatment, alongside an increase in vocabulary size. These changes reflect OpenAI's ongoing efforts to refine and optimize their models' language processing capabilities."
      ],
      "metadata": {
        "id": "CP546_V1ycqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: GPT-2 vocab and merges\n",
        "\n",
        "The transcript provides a walkthrough of the GPT-2 `encoder.py` implementation by OpenAI, focusing on how the encoding and decoding processes work within the GPT-2 framework. Here are the key points to note:\n",
        "\n",
        "1. **File Structure and Initial Setup**: The GPT-2 encoder setup involves loading two critical files: `encoder.json` and `vocab.bpe`. These files are essential for constructing the tokenizer. The `encoder.json` maps tokens to their corresponding integers, similar to a vocabulary object, while `vocab.bpe` contains the merge operations used during Byte Pair Encoding (BPE) tokenization.\n",
        "\n",
        "2. **Encoder and Decoder Objects**: The core of the tokenization process is encapsulated in the `Encoder` class. This class handles the conversion between text and a sequence of token IDs (encoding) and vice versa (decoding). It employs a dictionary for encoding and its reverse for decoding, facilitating efficient translation between text and token IDs.\n",
        "\n",
        "3. **Byte Encoder and Decoder**: Apart from the standard tokenizer, GPT-2 employs an additional layer of byte encoding and decoding. This layer converts between UTF-8 bytes and unicode strings, aiming to ensure a reversible process that doesn't rely solely on the standard Unicode characters. This mechanism helps to manage the vast array of characters and symbols that GPT-2 might encounter in diverse datasets.\n",
        "\n",
        "4. **BPE Mechanism**: The Byte Pair Encoding process is detailed in the `bpe` function within the `Encoder` class. This function iteratively merges the most frequent pairs of symbols (initially individual characters) into single new symbols, following the ranks determined during the BPE training phase. This process reduces the size of the input text representation by combining commonly occurring sequences into single tokens.\n",
        "\n",
        "5. **Regex Pattern for Tokenization**: GPT-2 uses a complex regex pattern to identify tokens within the text. This pattern aims to segment text into manageable pieces, including handling apostrophes, letters, numbers, and various punctuation marks, ensuring that the tokenization process respects linguistic and structural boundaries.\n",
        "\n",
        "6. **Encoding and Decoding Functions**: The `encode` and `decode` functions illustrate how text is converted into a sequence of token IDs and how these IDs are mapped back to text, respectively. The encoding process involves finding matches for the regex pattern, byte encoding the text, and then applying the BPE process. The decoding process reverses this, turning token IDs back into text.\n",
        "\n",
        "7. **Implementation Nuances**: The transcript points out that while the GPT-2 code might appear somewhat \"messy\" and includes some implementation details that might not seem immediately relevant (such as the additional layer of byte encoding/decoding), understanding the core algorithmic components—particularly the BPE process and how encoding/decoding operates—is crucial for grasping how GPT-2's tokenizer functions.\n",
        "\n",
        "In summary, the GPT-2 `encoder.py` file showcases a sophisticated approach to text tokenization, leveraging Byte Pair Encoding and additional encoding layers to handle a wide range of text data. Understanding this implementation provides valuable insights into the mechanisms behind GPT-2's language processing capabilities."
      ],
      "metadata": {
        "id": "Ih-ATOds028Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: Special Tokens\n",
        "\n",
        "[https://github.com/openai/tiktoken/blob/main/src/lib.rs](https://github.com/openai/tiktoken/blob/main/src/lib.rs)\n",
        "\n",
        "[Efficient Training of Language Models to\n",
        "Fill in the Middle](https://arxiv.org/pdf/2207.14255.pdf)\n",
        "\n",
        "The transcript elaborates on the role and handling of special tokens in GPT-2 and GPT-4, detailing the intricacies of their implementation and their significance in language modeling and tokenization. Here are the essential takeaways:\n",
        "\n",
        "1. **Purpose of Special Tokens**: Special tokens serve to delimit different parts of data or to introduce a special structure within the token streams. They play a crucial role in signaling to the language model the boundaries between distinct sections of text, such as the end of a document or the delineation of conversational turns.\n",
        "\n",
        "2. **Special Token in GPT-2**: GPT-2 incorporates a specific special token, notably the \"end of text\" token, which is used to mark the boundary between separate documents in the training dataset. This token is crucial for training as it informs the model when one piece of content ends, and another begins, ensuring that content is treated as distinct and unrelated.\n",
        "\n",
        "3. **Handling of Special Tokens**: Special tokens are managed outside the regular byte pair encoding (BPE) process. They are recognized and inserted explicitly into the token stream at appropriate places, such as between documents or conversational messages. The handling of these tokens involves special case instructions in the tokenization code, which can identify and replace specific string patterns with the corresponding special token IDs.\n",
        "\n",
        "4. **Special Tokens in GPT-4**: The GPT-4 tokenizer introduces additional special tokens beyond the \"end of text\" token found in GPT-2. These include tokens designed to support more complex structures, such as \"fill in the middle\" (FIM) and tokens for prefix, middle, and suffix positions. These enhancements reflect a broader range of uses for special tokens, enabling more nuanced control over the model's input and output.\n",
        "\n",
        "5. **Extending Tokenizers with Special Tokens**: The TikToken library and related tooling from OpenAI allow for the extension of base tokenizers by adding new special tokens. This flexibility is crucial for fine-tuning and adapting pre-trained models to specific tasks or conversational formats, where additional structural markers can significantly enhance performance.\n",
        "\n",
        "6. **Model Surgery for Special Tokens**: Incorporating new special tokens into a pre-trained model requires adjustments to the model's architecture, specifically the embedding matrix and the final classifier layer. This process, often referred to as \"model surgery,\" involves extending these components to accommodate the additional tokens, typically initializing them with small random values to integrate smoothly into the existing model structure.\n",
        "\n",
        "In summary, special tokens are a fundamental aspect of modern language models like GPT-2 and GPT-4, enabling more sophisticated handling of textual data and structural nuances. Their implementation and the ability to extend tokenizers with new special tokens provide significant flexibility in customizing models for a wide range of applications, from basic language understanding to complex conversational AI.\n",
        "\n",
        "---\n",
        "\n",
        "The discussion on the GPT-2 tokenizer and byte-pair encoding (BPE) alongside the vocabulary, including special tokens, offers several insightful facets:\n",
        "\n",
        "1. **Complex Vocabulary Structure**: The GPT-2 tokenizer's vocabulary size is 50,257. This includes 256 raw byte tokens that directly map to single ASCII characters, 50,000 tokens generated through the BPE merges process, and one special token, the \"end of text\" token. The precise structure and size of this vocabulary reflect a sophisticated approach to encoding textual data, balancing between raw character representation and merged sequences to efficiently cover the linguistic patterns observed in the training data.\n",
        "\n",
        "2. **Role of Special Tokens**: Special tokens, particularly the \"end of text\" token in GPT-2, play a pivotal role in delineating boundaries within the training data. By inserting this token between documents, it signals to the model the conclusion of one document and the start of another, unrelated piece of text. This organization helps the model learn contextual boundaries and manage sequence dependencies effectively, improving its ability to generate coherent and contextually appropriate text.\n",
        "\n",
        "3. **Encoding and Decoding Process**: The encoding process involves converting text to a sequence of integers representing tokens, while decoding reverses this process. The addition of byte encoding and decoding layers introduces an additional step in both processes, converting between UTF-8 bytes and unicode strings. This layer helps manage the wide array of characters and symbols, ensuring reversible and robust encoding.\n",
        "\n",
        "4. **Special Case Instructions for Handling Special Tokens**: Beyond the typical BPE process, the tokenizer includes special case instructions for handling special tokens. These instructions ensure that tokens like \"end of text\" are recognized and handled correctly during tokenization, maintaining their intended function as delimiters or structural markers within the text data.\n",
        "\n",
        "5. **Extensibility with Special Tokens in GPT-4**: Moving to GPT-4, the tokenizer evolves to include more special tokens and changes in regex patterns for chunking text. This evolution indicates ongoing efforts to refine the tokenizer's capability to understand and manage text structure more effectively. The ability to add and handle new special tokens allows for customization and fine-tuning of the tokenizer to specific tasks or conversational formats, enhancing the model's applicability and performance.\n",
        "\n",
        "6. **Model Surgery for Adding Special Tokens**: Incorporating new special tokens into a pre-trained model necessitates adjustments to the model's architecture, such as extending the embedding matrix and the final classifier layer. This \"model surgery\" underscores the dynamic nature of LLMs and their tokenizers, where modifications can be made to tailor the model's understanding and generation capabilities for specific applications.\n",
        "\n",
        "In conclusion, the intricacies of the GPT-2 and GPT-4 tokenizers, from their comprehensive vocabularies to the handling of special tokens, reflect a nuanced approach to text tokenization. This complexity enables the models to capture and generate human language with remarkable coherence and relevance, showcasing the depth of technology behind OpenAI's language models."
      ],
      "metadata": {
        "id": "v5jM2jdLhGJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens\n",
        "\n",
        "Special tokens in tokenizers and Large Language Models (LLMs) serve as crucial markers or signals that help the models understand and manage the structure and context of the text they process. These tokens are not part of the natural language text itself but are inserted into the text data to convey specific meanings or instructions to the model. Here's a breakdown of their roles:\n",
        "\n",
        "1. **Segmentation and Delimitation**: Special tokens often mark the beginning or end of a segment of text. For example, an \"end of text\" token can signal that a document or an input sequence has concluded, helping the model distinguish between separate pieces of text. This is particularly important in training and inference to prevent the model from blending the context of unrelated text segments.\n",
        "\n",
        "2. **Providing Contextual Cues**: In conversational AI or task-specific models (like question-answering systems), special tokens can indicate the start of a question, the end of an answer, or transitions between turns in dialogue. This helps the model understand the structure of the conversation and respond appropriately.\n",
        "\n",
        "3. **Control and Instructions**: Some special tokens are used to control the behavior of the model or instruct it on how to generate text. For instance, tokens might be used to switch between languages, dictate a certain style or tone, or prompt the model to perform a specific task like summarization.\n",
        "\n",
        "4. **Enhancing Model Performance**: By clearly delineating different parts of the data, special tokens can improve a model's ability to learn from and generate coherent and contextually appropriate responses. They help in managing sequence lengths and ensuring that the model's attention mechanism focuses on relevant parts of the input.\n",
        "\n",
        "5. **Facilitating Fine-tuning and Customization**: Special tokens allow developers to fine-tune pre-trained models for specific applications. By adding new special tokens and adjusting the model accordingly, developers can tailor the model's understanding and generation capabilities to suit particular needs or domains.\n",
        "\n",
        "6. **Supporting Meta-information**: In some cases, special tokens embed meta-information within the text data, such as speaker IDs in dialogues or tags indicating the genre of text. This meta-information can be leveraged by the model to generate more nuanced and context-aware outputs.\n",
        "\n",
        "In essence, special tokens are a versatile tool in the toolkit of NLP practitioners, offering a way to enhance and direct the capabilities of tokenizers and LLMs. Their strategic use enables more sophisticated handling of text data, supporting a wide range of applications from general-purpose language understanding to specialized conversational agents and beyond."
      ],
      "metadata": {
        "id": "9VqEVVDRjmHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: Fill in the Middle\n",
        "\n",
        "The concept of \"Fill in the Middle\" (FIM) in Large Language Models (LLMs) refers to a training and inference strategy where the model is tasked with generating text that logically connects two given text segments. Unlike traditional language model tasks, which typically focus on predicting the next word or sentence given a preceding context (autoregressive modeling) or completing a text given a prompt (text completion), FIM specifically aims to understand the relationship between two separate pieces of text and generate coherent content that fits between them.\n",
        "\n",
        "### Key Points of Fill in the Middle:\n",
        "\n",
        "1. **Contextual Bridging**: The model receives two pieces of text as input: an initial segment and a concluding segment. Its task is to generate one or more sentences that bridge these segments in a logical, coherent, and contextually appropriate manner. This requires a deep understanding of the content and context of both input segments.\n",
        "\n",
        "2. **Training Methodology**: To train a model on the FIM task, the training data must be structured to support this kind of generation. This could involve taking longer passages of text and segmenting them into beginning and ending parts, with the middle portion serving as the target for generation during training. The model learns to predict this middle section based on the surrounding context.\n",
        "\n",
        "3. **Applications**: FIM can be particularly useful in scenarios where generating contextually rich and coherent content is necessary. This includes creative writing assistance, where a writer needs to connect two plot points; content generation, where an article requires a detailed explanation between an introduction and conclusion; and conversational AI, where the model needs to maintain the flow of dialogue.\n",
        "\n",
        "4. **Enhanced Contextual Understanding**: FIM tasks encourage models to develop a more nuanced understanding of context and narrative structure. Instead of focusing solely on what comes next, the model must consider both what came before and what comes after the target generation, ensuring that the generated text serves as a natural bridge.\n",
        "\n",
        "5. **Challenges**: Successfully implementing a FIM approach in LLMs presents several challenges, including ensuring the relevance and coherence of the generated text, managing the potential for increased computational complexity, and developing training datasets that effectively support learning this task.\n",
        "\n",
        "In summary, \"Fill in the Middle\" represents an advanced use case for LLMs, pushing beyond straightforward predictive text generation to embrace more complex narrative and contextual understanding tasks. By focusing on bridging two text segments, FIM tasks help develop models that can generate more contextually rich, coherent, and nuanced content, offering promising applications in various domains of natural language processing and generation."
      ],
      "metadata": {
        "id": "GrFvjRKQkkM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `replace_control_characters` function\n",
        "\n",
        "The `replace_control_characters` function iterates over a given string `s` and replaces each control character with its Unicode escape sequence, while leaving all other characters unchanged. Control characters are non-printing characters that control or format the display of text, such as newline (`\\n`), carriage return (`\\r`), tab (`\\t`), and others defined in the Unicode standard under various categories starting with \"C\" (e.g., \"Cc\" for control characters, \"Cf\" for format characters).\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "- **Iterate Through Each Character**: The function loops through each character in the input string `s`.\n",
        "- **Check Character Category**: It uses the `unicodedata.category(ch)` function to determine the Unicode category of each character. If the first letter of the category is \"C\", the character is identified as a control character.\n",
        "- **Replace Control Characters**: For control characters, instead of adding them directly to the `chars` list, the function converts them into a Unicode escape sequence (`\\uxxxx`, where `xxxx` represents the Unicode code point of the character in hexadecimal format). This conversion uses the `ord(ch)` function to get the character's Unicode code point and formats it as a four-digit hexadecimal value.\n",
        "- **Build and Return the Modified String**: Non-control characters are added to the `chars` list unchanged. The function then returns a new string composed of the processed characters, effectively escaping all control characters while preserving the rest of the text.\n",
        "\n",
        "### Example Issue Addressed by This Function:\n",
        "\n",
        "Consider the string `Hello, world!\\nThis is a test.\\r\\n` containing both visible characters and control characters like newline (`\\n`) and carriage return plus newline (`\\r\\n`). If this string were to be displayed in a context where control characters could cause formatting issues (e.g., a single-line text box, logging system, or file names), it might lead to unwanted line breaks or other display problems.\n",
        "\n",
        "By applying the `replace_control_characters` function to this string, all control characters would be replaced with their Unicode escape sequences, resulting in a string like `Hello, world!\\\\u000aThis is a test.\\\\u000d\\\\u000a`. This transformed string can then be safely displayed or processed in environments where control characters could cause issues, as all control characters have been converted to a visible representation that doesn't affect text formatting."
      ],
      "metadata": {
        "id": "qkzACLQpkehP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face tokenizers\n",
        "\n",
        "In the context of Hugging Face tokenizers, several files are specifically associated with the tokenizer's configuration and operation. These files enable the tokenizer to encode input text into tokens understood by the model and decode tokens back into human-readable text. Here are the files related to the tokenizer and their purposes:\n",
        "\n",
        "1. **`vocab.json`**: This file contains the mapping from tokens to their integer IDs. It's essential for both encoding (text to tokens) and decoding (tokens to text) processes.\n",
        "\n",
        "2. **`merges.txt`**: In tokenizers that use Byte Pair Encoding (BPE) or similar algorithms (e.g., GPT-2), this file stores the merge operations that progressively combine individual characters or character sequences into tokens. It's crucial for the tokenizer to know which sequences of characters should be merged to form tokens.\n",
        "\n",
        "3. **`tokenizer.json`**: This is a comprehensive file that can include the tokenizer's vocabulary, merges, and additional configurations in a single JSON file. It's especially useful for portability and ease of use, as it encapsulates the tokenizer's entire configuration.\n",
        "\n",
        "4. **`tokenizer_config.json`**: This file contains configuration details specific to the tokenizer, such as whether to lower case the input text, the special tokens used, and other tokenizer settings. It helps in setting up the tokenizer correctly for encoding and decoding tasks.\n",
        "\n",
        "5. **`special_tokens_map.json`**: This file maps special tokens (like padding token, unknown token, etc.) to their string representations. Special tokens are used for specific purposes, such as separating sentences, marking the beginning or end of a text, and padding sequences to a uniform length.\n",
        "\n",
        "6. **`config.json`**: While primarily associated with the model's configuration, this file can also contain tokenizer-related settings, especially when the model and tokenizer are tightly integrated.\n",
        "\n",
        "The files `vocab.json`, `merges.txt`, `tokenizer.json`, `tokenizer_config.json`, and `special_tokens_map.json` are specifically associated with the tokenizer. They work together to ensure the tokenizer can accurately and efficiently convert text to and from the token representations needed for the model to process.\n",
        "\n",
        "Other files listed, like model weights (`pytorch_model.bin`, `tf_model.h5`, `flax_model.msgpack`, `rust_model.ot`), ONNX models, and TFLite models, are related to the model itself rather than the tokenizer. The `README.md` file provides documentation, and files like `config.json` and `generation_config.json` might contain both model and tokenizer configurations but are primarily associated with the model's operational parameters.\n",
        "\n",
        "---\n",
        "\n",
        "When training your own tokenizer or reusing an existing tokenizer with Hugging Face, the required files depend on the complexity of your tokenizer and whether you're making modifications or just leveraging it as-is. If you're fine-tuning a model like QLORA or any other model and plan to use an existing tokenizer, the minimal set of tokenizer-specific files you would need includes:\n",
        "\n",
        "1. **`vocab.json`**: This is essential as it provides the mapping of tokens to their respective IDs. Even if you're not modifying the tokenizer, you need this file to ensure that the text can be correctly tokenized into the format expected by the model.\n",
        "\n",
        "2. **`merges.txt`** (for BPE tokenizers like GPT-2 or GPT-3): If your tokenizer uses a Byte Pair Encoding mechanism or similar strategies, this file is necessary. It details how to merge pairs of characters or character sequences into larger tokens.\n",
        "\n",
        "3. **`tokenizer.json`**: For some tokenizers, this file can act as a standalone configuration file that includes the vocabulary, merges, and other tokenizer settings. In cases where `tokenizer.json` is provided, it might be the only file you need, as it can encapsulate the entire tokenizer setup.\n",
        "\n",
        "Optionally, if your tokenizer utilizes or defines special tokens beyond the standard set or if you've made modifications to how special tokens are handled:\n",
        "\n",
        "4. **`special_tokens_map.json`**: This file specifies any special tokens (such as start-of-sequence, end-of-sequence, padding, etc.) that your model expects. It's useful for ensuring that these tokens are consistently represented and treated within your pipeline.\n",
        "\n",
        "5. **`tokenizer_config.json`**: Contains additional configuration details specific to the tokenizer, such as whether to strip accents, use lowercasing, or other preprocessing steps. This file helps in customizing the tokenizer's behavior to match the model's requirements.\n",
        "\n",
        "For a fine-tuning task, especially when reusing an existing tokenizer, you may not need to alter `vocab.json`, `merges.txt`, or `tokenizer.json`, but ensuring their presence is crucial for the tokenizer to function correctly. However, depending on the specific changes you're making or the nature of your task, you might need to adjust `special_tokens_map.json` and `tokenizer_config.json` to better suit your fine-tuning or application needs.\n",
        "\n",
        "In summary, the minimal tokenizer-specific Hugging Face files needed would likely be `vocab.json` and possibly `merges.txt` if you're using a BPE tokenizer, with `tokenizer.json` serving as a comprehensive file that might replace the need for separate `vocab.json` and `merges.txt` files. `special_tokens_map.json` and `tokenizer_config.json` are optional and depend on whether you need to customize or modify the tokenizer's behavior and special tokens."
      ],
      "metadata": {
        "id": "MDAy1--ByVUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentencePiece\n",
        "\n",
        "[https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)\n",
        "\n",
        "[Sentencepiece training options](https://github.com/google/sentencepiece/blob/master/doc/options.md)\n",
        "\n",
        "Excellent articles for background and intuition:\n",
        "\n",
        "- [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
        "\n",
        "- [A Deep Dive into the Wonderful World of Preprocessing in NLP](https://web.archive.org/web/20201101005930/https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/)\n",
        "\n",
        "\n",
        "- [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "SentencePiece is a software library for unsupervised text tokenization and detokenization mainly used in neural network models for natural language processing (NLP). It's particularly popular in the context of machine translation, text generation, and other tasks that require handling text data efficiently at scale. One of the key features of SentencePiece is its ability to treat the input text as a raw stream of Unicode characters, which means it doesn't rely on pre-tokenized or whitespace-separated words. This approach offers several advantages:\n",
        "\n",
        "### Language Agnostic\n",
        "SentencePiece is designed to be language-agnostic, making it particularly useful for processing text from languages where whitespace doesn't serve as a clear word delimiter, such as in Chinese or Japanese. It can be used for virtually any language, facilitating the development of multilingual models.\n",
        "\n",
        "### Subword Tokenization\n",
        "SentencePiece supports subword tokenization methods like Byte Pair Encoding (BPE) and Unigram Language Model. These methods break down words into smaller units (subwords or characters), allowing the model to handle unknown words more gracefully, share representations among related words, and reduce the vocabulary size, which, in turn, can lead to more efficient training and inference.\n",
        "\n",
        "### Integration and Usage\n",
        "SentencePiece has been widely adopted in major NLP frameworks and projects. It's particularly favored in scenarios where controlling the tokenization process directly within the model training and inference pipeline is crucial. By integrating SentencePiece, developers can ensure consistency in how text is tokenized and detokenized, avoiding issues that might arise from discrepancies between training and deployment environments.\n",
        "\n",
        "### Benefits of Subword Tokenization\n",
        "- **Handling of Unknown Words**: By breaking words into subwords or characters that are seen during training, the model can handle words it has never seen before.\n",
        "- **Efficiency**: Reducing the vocabulary size to subwords or characters can significantly decrease the model's memory requirements.\n",
        "- **Flexibility**: It allows for fine-grained control over the tokenization process, making it suitable for a wide range of languages and tasks.\n",
        "\n",
        "### Customization\n",
        "SentencePiece allows for customization of its tokenization process, letting users specify the vocabulary size, character coverage, and the specific tokenization algorithm (e.g., BPE or Unigram). This level of control enables users to tailor the tokenization to the specific needs of their applications and datasets.\n",
        "\n",
        "### Conclusion\n",
        "SentencePiece represents a powerful and flexible tool in the NLP toolkit, enabling efficient and effective text processing for a wide range of languages and tasks. Its language-agnostic approach to tokenization, along with support for subword models, helps tackle common challenges such as unknown words and large vocabularies, making it a popular choice among researchers and practitioners in the NLP community."
      ],
      "metadata": {
        "id": "J5l--9IF5jfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a key distinction between TikToken and SentencePiece-based tokenizer\n",
        "\n",
        "The explanation highlights a key distinction between TikToken and SentencePiece-based tokenizers, particularly in how they handle the initial text representation and the granularity of tokenization:\n",
        "\n",
        "### TikToken Approach:\n",
        "- **Initial Representation**: TikToken operates at the byte level, initially encoding text using UTF-8, which results in a sequence of bytes. This is particularly useful for handling a wide variety of languages and special characters uniformly.\n",
        "- **Tokenization Granularity**: It merges bytes based on their frequency of occurrence next to each other in the training data. The Byte Pair Encoding (BPE) or similar algorithms are applied to these byte sequences, allowing for efficient handling of unknown words and subword tokenization.\n",
        "- **Character Coverage**: TikToken's method does not directly consider character coverage or rare code points, as the encoding and merging occur at the byte level.\n",
        "\n",
        "### SentencePiece Approach:\n",
        "- **Initial Representation**: SentencePiece operates directly on Unicode code points. It does not convert the text to bytes but works with the text's original Unicode representation. This allows it to more directly consider the linguistic elements of the text as represented by their code points.\n",
        "- **Tokenization Granularity**: SentencePiece applies BPE or similar algorithms directly to these code points. This means that the merging operations are conducted on a more linguistically meaningful level, potentially offering advantages for certain languages or specific character sets.\n",
        "- **Character Coverage and Byte Fallback**: SentencePiece introduces the concept of character coverage to manage rare code points. If certain code points are deemed too rare (based on the character coverage hyperparameter), they may either be mapped to an unknown token or, if byte fallback is enabled, encoded into UTF-8 bytes and then tokenized. This provides a flexible mechanism for handling text's more infrequent linguistic or symbolic elements.\n",
        "\n",
        "### Key Differences:\n",
        "1. **Level of Operation**: TikToken begins with a byte-level representation of text, while SentencePiece starts with Unicode code points. This fundamental difference affects how each tokenizer views and processes the input text.\n",
        "2. **Handling of Rare Characters**: SentencePiece's character coverage and byte fallback mechanisms offer a nuanced approach to managing rare characters or symbols, translating them into a mix of known tokens and byte-level encodings if necessary. In contrast, TikToken's byte-level approach inherently handles a wide range of characters without distinguishing between common and rare characters at the encoding stage.\n",
        "\n",
        "### Conclusion:\n",
        "- **TikToken** might be considered more straightforward and uniform in its approach, treating all text as sequences of bytes initially, which might simplify processing but potentially abstracts away some linguistic nuances.\n",
        "- **SentencePiece** offers a nuanced, linguistically aware approach by working directly with Unicode code points and providing mechanisms to handle rare characters, which can be particularly beneficial for languages with complex scripts or when fine-grained control over tokenization behavior is needed.\n",
        "\n",
        "Both approaches have their strengths and are chosen based on the specific requirements of the NLP tasks, languages involved, and preferences for handling linguistic diversity and character rarity."
      ],
      "metadata": {
        "id": "PIv6g8Mz5q7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: SentencePiece library\n",
        "\n",
        "The transcript provides a detailed overview of SentencePiece and its comparison to TikToken, including their applications, configurations, and differences in handling tokenization. Here are the most interesting takeaways:\n",
        "\n",
        "1. **SentencePiece's Versatility**: SentencePiece is highlighted for its efficiency in both training and inference, supporting multiple tokenization algorithms, including Byte Pair Encoding (BPE). It's widely used in various language models like LLaMA and MISTral, indicating its robustness and adaptability.\n",
        "\n",
        "2. **Direct Operation on Code Points**: Unlike TikToken, which first encodes text to bytes, SentencePiece operates directly on Unicode code points. This allows for tokenization that is more closely aligned with the linguistic elements represented by those code points, offering potentially more nuanced handling of languages with complex scripts.\n",
        "\n",
        "3. **Handling of Rare Code Points**: SentencePiece has a unique approach to managing rare code points through a character coverage hyperparameter. Rare code points can be mapped to a special unknown token or, with byte fallback enabled, encoded into UTF-8 bytes and then tokenized. This mechanism provides flexibility in dealing with text's less frequent elements.\n",
        "\n",
        "4. **Configuration Complexity and Historical Baggage**: The transcript notes that SentencePiece has accumulated historical baggage over time, resulting in a complex array of configuration options. While this flexibility allows SentencePiece to cater to a wide range of requirements, it can also make the tool daunting for new users due to its configuration complexity and the need for careful documentation review.\n",
        "\n",
        "5. **Pre-processing and Normalization Rules**: SentencePiece's approach to pre-processing and normalization is mentioned as potentially overcomplicated, especially in the context of language models where preserving the raw form of data is often preferred. The tool includes numerous options for text simplification and normalization, which may not always be desirable for training modern NLP models.\n",
        "\n",
        "6. **Concept of Sentences**: SentencePiece treats sentences as individual training examples, with various options for managing sentence length and shuffling. This concept may not align perfectly with the needs of language models, which often benefit from treating text as continuous streams without hard segmentations.\n",
        "\n",
        "7. **Practical Application and Customization**: Despite its complexities, SentencePiece remains a powerful tool for training custom tokenizers, offering detailed control over tokenization behavior. However, achieving desired results can require significant effort to understand and appropriately set the multitude of available options.\n",
        "\n",
        "In summary, SentencePiece stands out for its direct operation on Unicode code points, sophisticated handling of rare characters, and broad configurability. However, its complexity and the extensive array of options highlight a trade-off between versatility and user-friendliness, emphasizing the importance of clear documentation and thoughtful configuration in leveraging SentencePiece effectively for tokenization tasks."
      ],
      "metadata": {
        "id": "tIgz9LtmEpng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentencePiece is widely used for two main reasons\n",
        "\n",
        "The sentence highlights SentencePiece's notable capabilities in the context of Byte Pair Encoding (BPE) tokenizers, distinguishing it from TikToken. Specifically, it states that SentencePiece is widely used for two main reasons:\n",
        "\n",
        "1. **Efficiency in Training**: SentencePiece can train new tokenizers from scratch. Training involves analyzing a large corpus of text to identify the most common pairs of characters (or tokens) and iteratively merging them to form a vocabulary of more complex tokens. This process is crucial for developing a tokenizer that is optimized for the specific linguistic patterns and vocabulary of the corpus it was trained on.\n",
        "\n",
        "2. **Efficiency in Inference**: Beyond just training, SentencePiece can also perform inference efficiently. Inference, in this context, refers to the tokenizer's ability to take unseen text and break it down into the tokens defined during training. Efficient inference is critical for applying the tokenizer in practical NLP tasks, such as input preparation for language models during text generation, translation, or classification tasks.\n",
        "\n",
        "The mention of \"unlike TikToken\" implies that, at the time of the statement, TikToken either does not support both training and inference processes as efficiently as SentencePiece does, or it might specialize in one aspect over the other. This makes SentencePiece a preferred choice for projects where both efficient training of custom BPE tokenizers and their subsequent application for text processing are essential."
      ],
      "metadata": {
        "id": "TEJkOokjF7fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretokenization\n",
        "\n",
        "Pretokenization is the process of splitting text into initial chunks (pretokens) before applying more sophisticated tokenization algorithms, such as Byte Pair Encoding (BPE), WordPiece, or Unigram language models. Pretokenization usually involves breaking down the text into words, spaces, and possibly punctuation, serving as a first step to simplify and standardize the input for further processing.\n",
        "\n",
        "The key difference highlighted in the sentence is that BPE, WordPiece, and Unigram language model tokenizers typically require this initial step of breaking down the text into a more manageable form. In contrast, SentencePiece can operate directly on raw text without needing pretokenization. This is because SentencePiece handles the text at a more granular level (e.g., Unicode code points) and integrates both the splitting and subword tokenization steps into one process.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Let's clarify with an example using the text \"OpenAI's GPT-3 is amazing!\"\n",
        "\n",
        "**With Pretokenization (e.g., for BPE):**\n",
        "1. **Pretokenization:** The text is initially split into words and punctuation:\n",
        "   - \"OpenAI's\", \"GPT-3\", \"is\", \"amazing\", \"!\"\n",
        "2. **BPE Tokenization:** These tokens are then further split or combined into subwords based on the BPE algorithm's learned merges, potentially resulting in something like:\n",
        "   - \"Open\", \"AI\", \"'\", \"s\", \"G\", \"PT-3\", \"is\", \"amaz\", \"ing\", \"!\"\n",
        "\n",
        "**Without Pretokenization (e.g., SentencePiece):**\n",
        "1. **Direct Tokenization:** SentencePiece processes the raw text directly without explicitly separating words first. It might break down the text into subwords and characters based on its own learned patterns, potentially resulting in a different set of tokens that could include pieces of words, punctuation, or characters as tokens:\n",
        "   - \"▁Open\", \"AI\", \"'\", \"s\", \"▁GPT\", \"-\", \"3\", \"▁is\", \"▁amazing\", \"!\"\n",
        "\n",
        "Here, \"▁\" represents a space character that SentencePiece explicitly includes to handle word boundaries, allowing it to reconstruct the original text from tokens.\n",
        "\n",
        "In summary, pretokenization is an initial step to simplify text into a consistent format for certain tokenizers, but SentencePiece's design allows it to bypass this step, working directly on the full text to produce tokens."
      ],
      "metadata": {
        "id": "Af2TnGxi7gMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Protobuf`\n",
        "\n",
        "Protocol Buffers (Protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data, similar to XML or JSON but smaller, faster, and simpler. In Python, as well as in other programming languages, Protobuf is used for a variety of purposes:\n",
        "\n",
        "1. **Cross-Language Services Communication**: Protobuf supports generating code in multiple languages from a single `.proto` schema. This feature is particularly useful in microservices architectures where services are written in different languages but need to exchange data in a robust and efficient manner.\n",
        "\n",
        "2. **Data Storage and Transmission**: Due to its compact size and efficiency in serialization and deserialization, Protobuf is ideal for storing and transmitting data, especially in environments where bandwidth or storage capacity is limited.\n",
        "\n",
        "3. **APIs and RPC Systems**: Protobuf can be used to define APIs and remote procedure call (RPC) interfaces. With tools like gRPC, developers can automatically generate client and server code from Protobuf definitions, facilitating easier and more reliable system integration.\n",
        "\n",
        "4. **Configuration Files**: The structured nature of Protobuf makes it suitable for use in configuration files. Unlike traditional configuration files that might be prone to errors due to their free-form nature, Protobuf ensures that configurations are both structured and type-checked.\n",
        "\n",
        "5. **Efficient Data Logging and Exchange in Distributed Systems**: Protobuf's compact binary format is beneficial for logging and exchanging data in distributed systems where efficiency is crucial for performance and scalability.\n",
        "\n",
        "6. **Machine Learning and Data Analysis**: In machine learning workflows, Protobuf is used to serialize and deserialize datasets, model parameters, and metadata. Its efficiency helps in quickly loading and saving large volumes of data.\n",
        "\n",
        "The main advantages of using Protobuf in these use cases include its strong typing, backward and forward compatibility, efficiency in both size and speed, and the automatic generation of data access classes, which reduce boilerplate code and potential for human error.\n",
        "\n",
        "---\n",
        "\n",
        "To give you a simple example of using Protocol Buffers in Python, let's create a basic `Person` data structure, compile it, and then write some Python code to serialize and deserialize it.\n",
        "\n",
        "### Step 1: Define the Protobuf Schema\n",
        "\n",
        "First, you need to define your data structure in a `.proto` file. Let's create a file named `person.proto`:\n",
        "\n",
        "```proto\n",
        "syntax = \"proto3\";\n",
        "\n",
        "message Person {\n",
        "  string name = 1;\n",
        "  int32 id = 2;\n",
        "  string email = 3;\n",
        "}\n",
        "```\n",
        "\n",
        "This defines a `Person` message with three fields: `name`, `id`, and `email`.\n",
        "\n",
        "### Step 2: Compile the `.proto` File\n",
        "\n",
        "Next, you'll need to compile this `.proto` file to generate the Python classes. You'll need the Protocol Buffers compiler, `protoc`, installed on your machine for this step.\n",
        "\n",
        "Run the following command in the terminal in the directory where your `person.proto` file is located:\n",
        "\n",
        "```sh\n",
        "protoc --python_out=. person.proto\n",
        "```\n",
        "\n",
        "This will generate a `person_pb2.py` file, which contains the generated Python classes.\n",
        "\n",
        "### Step 3: Python Code to Serialize and Deserialize\n",
        "\n",
        "Now, you can write Python code to create a `Person`, serialize it to a string, and then deserialize the string back into a `Person` object. Create a new Python file in the same directory, for example, `main.py`, and add the following code:\n",
        "\n",
        "```python\n",
        "from person_pb2 import Person\n",
        "\n",
        "# Create a new Person\n",
        "person = Person(name=\"John Doe\", id=1234, email=\"johndoe@example.com\")\n",
        "\n",
        "# Serialize the Person to a binary string\n",
        "person_binary = person.SerializeToString()\n",
        "print(f\"Serialized person: {person_binary}\")\n",
        "\n",
        "# Deserialize the binary string back into a Person\n",
        "person_new = Person()\n",
        "person_new.ParseFromString(person_binary)\n",
        "print(f\"Deserialized person: name={person_new.name}, id={person_new.id}, email={person_new.email}\")\n",
        "```\n",
        "\n",
        "Run this Python script:\n",
        "\n",
        "```sh\n",
        "python main.py\n",
        "```\n",
        "\n",
        "You should see output showing the serialized binary string and the deserialized `Person` object's fields.\n",
        "\n",
        "This simple example demonstrates defining a data structure using Protocol Buffers, generating the corresponding Python classes, and using those classes to serialize and deserialize data."
      ],
      "metadata": {
        "id": "8MVsb_3MHlaB"
      }
    }
  ]
}
