{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8d652264d5549c4bae8d3c2b4ae07e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03158b7ce7b9430baeeb75c577bc6893",
              "IPY_MODEL_46809cc75d334e1ab36991774b7e4e20",
              "IPY_MODEL_f1679f69bbdb4029bd3a9968288d80fb"
            ],
            "layout": "IPY_MODEL_67e66cf7738f49b38b0c6fb0d081d4b0"
          }
        },
        "00769d0408784442b9873a1a3f3531e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_085bdefa843f4609841ca81dcbd2bb10",
            "placeholder": "​",
            "style": "IPY_MODEL_ed41e2cf54104a9ea30892a5f74f9853",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a66d6c9db83c4d9ca02bbfaa47cc26f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_adee8b25f4e74bdb996cb7da86872e30",
            "placeholder": "​",
            "style": "IPY_MODEL_87c0df615fcb400b918631d612b38b35",
            "value": ""
          }
        },
        "8da6b6c380b64aeba2288fdf94965e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a39e5a7493f14baf8ad8df4719a6a35b",
            "style": "IPY_MODEL_915f0afdc41c478caf14fb07fe83ff3c",
            "value": false
          }
        },
        "ece50dfa0d2e491c93045ffe2641a639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a814a1150d714839898c43cbfe2c891c",
            "style": "IPY_MODEL_e22b69c8ac68476faa4357d688368c2e",
            "tooltip": ""
          }
        },
        "5a9a621053e04519af341ffbea4da056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_881b54c5363047959961578b3a59fdeb",
            "placeholder": "​",
            "style": "IPY_MODEL_68b0a18fb90b4ed9b0c32740e29b5f77",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "67e66cf7738f49b38b0c6fb0d081d4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "085bdefa843f4609841ca81dcbd2bb10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed41e2cf54104a9ea30892a5f74f9853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adee8b25f4e74bdb996cb7da86872e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c0df615fcb400b918631d612b38b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a39e5a7493f14baf8ad8df4719a6a35b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915f0afdc41c478caf14fb07fe83ff3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a814a1150d714839898c43cbfe2c891c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22b69c8ac68476faa4357d688368c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "881b54c5363047959961578b3a59fdeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b0a18fb90b4ed9b0c32740e29b5f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cc32b2fc3a44bb38f4d0967c68f87c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a21612babce4f04a6cc17f30399cf85",
            "placeholder": "​",
            "style": "IPY_MODEL_4f332d1311e8420fb62c622a3c22bd89",
            "value": "Connecting..."
          }
        },
        "6a21612babce4f04a6cc17f30399cf85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f332d1311e8420fb62c622a3c22bd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03158b7ce7b9430baeeb75c577bc6893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4da463e9c38478c86321e80c0afd55a",
            "placeholder": "​",
            "style": "IPY_MODEL_fc6c61cd1d714d6caee1f1f4689fc62e",
            "value": "Token is valid (permission: write)."
          }
        },
        "46809cc75d334e1ab36991774b7e4e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a9f6fd91d23478d92aba17f3b5a6c6d",
            "placeholder": "​",
            "style": "IPY_MODEL_94480d8a533748a5881cbe0b96719ecb",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "f1679f69bbdb4029bd3a9968288d80fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cfac87361784829b5b7a1e04dc96174",
            "placeholder": "​",
            "style": "IPY_MODEL_5cfb11f9790e47daa0e95f0e9b8bdf8f",
            "value": "Login successful"
          }
        },
        "f4da463e9c38478c86321e80c0afd55a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6c61cd1d714d6caee1f1f4689fc62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9f6fd91d23478d92aba17f3b5a6c6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94480d8a533748a5881cbe0b96719ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cfac87361784829b5b7a1e04dc96174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfb11f9790e47daa0e95f0e9b8bdf8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5818c2bb45c4d389bd359fdd8b34118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6af92f86387d4ac6b6637ab8afd0fb68",
              "IPY_MODEL_7c38ffb8033f47c5946860a8dee45afe",
              "IPY_MODEL_40f8eae6e59a4389b58c48268f84e523"
            ],
            "layout": "IPY_MODEL_f7945267983a487eb354197c2a0a9224"
          }
        },
        "6af92f86387d4ac6b6637ab8afd0fb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e784b9b51c843be82b80561f8246bac",
            "placeholder": "​",
            "style": "IPY_MODEL_32929629656342bea44b0d656e4638a0",
            "value": "model_weights.pth: 100%"
          }
        },
        "7c38ffb8033f47c5946860a8dee45afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38d07110551f46bdad344ffb521cd89e",
            "max": 263493,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44ad3ff3858847229e308dbda9198cba",
            "value": 263493
          }
        },
        "40f8eae6e59a4389b58c48268f84e523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a57dada32ea4cadbf3f734f1e870835",
            "placeholder": "​",
            "style": "IPY_MODEL_c951524d248540ea968c4b530fceb7ab",
            "value": " 263k/263k [00:00&lt;00:00, 84.4kB/s]"
          }
        },
        "f7945267983a487eb354197c2a0a9224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e784b9b51c843be82b80561f8246bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32929629656342bea44b0d656e4638a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38d07110551f46bdad344ffb521cd89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ad3ff3858847229e308dbda9198cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a57dada32ea4cadbf3f734f1e870835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c951524d248540ea968c4b530fceb7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction to Building GPT from Scratch\n",
        "\n",
        "Welcome to this Colab notebook, a compilation of insights and learnings garnered from delving into Andrej Karpathy's YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) for the second time.\n"
      ],
      "metadata": {
        "id": "vPXNOCxwI2vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PXHaA0y-V0tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCf2ZaORVymz",
        "outputId": "0466075f-e749-4cee-b49a-12e186d285c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c2240dc3b10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "80hU0-stI6a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6REHryANI-Hj",
        "outputId": "e0ac4fde-032c-4e9c-8c78-7fd9485a4776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-04 17:25:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  5.19MB/s    in 0.2s    \n",
            "\n",
            "2023-10-04 17:25:23 (5.19 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset processing"
      ],
      "metadata": {
        "id": "K-GjGgaRQ20Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUhRd0f8Qse3",
        "outputId": "04175003-ddf0-4134-c273-a7396031d8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-29 19:51:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-09-29 19:51:29 (98.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt',mode='r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "MPtM11w-QzSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiSKn5-FRisD",
        "outputId": "8c2e83f2-98f1-42d6-fafc-3a1e84baf487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PLmfxn6qRXsf",
        "outputId": "f3ddf287-ed9b-4041-d532-8349967954be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_67qkhsRrkT",
        "outputId": "26fd3b2d-bef5-4c62-dcac-d235a0cb2bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(\"\".join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uch8Vve4Rz5S",
        "outputId": "22a82121-8fde-4ff5-9b4e-20a21495841f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "print(encode(\"hello dude\"))\n",
        "print(decode(encode(\"hello dude\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff2AM_fNSf76",
        "outputId": "ca20c126-4454-4e5b-e5ae-de1993dfd02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 42, 59, 42, 43]\n",
            "hello dude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi"
      ],
      "metadata": {
        "id": "WRnhJCR8uW_Q",
        "outputId": "691630bc-5032-4864-9376-082c6f98df94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype= torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUd-icteV6s-",
        "outputId": "daad3e54-3fce-46c1-f963-d61b2b9359d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo7vXrlsYSDB",
        "outputId": "ec1d031b-4ee9-4a79-db1b-5fac19ad63d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HYiX_QtYxfw",
        "outputId": "4aa116e0-a0ed-4e59-9d78-bfb85b527235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4J9fhlRZFrF",
        "outputId": "32bc0008-098d-4b79-c70c-955d7b34f2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b,:t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T693Gi-cZ0uH",
        "outputId": "855f21c9-b6c7-4b81-f7d7-73b54a02bee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is tensor([24]) the target: 43\n",
            "when input is tensor([24, 43]) the target: 58\n",
            "when input is tensor([24, 43, 58]) the target: 5\n",
            "when input is tensor([24, 43, 58,  5]) the target: 57\n",
            "when input is tensor([24, 43, 58,  5, 57]) the target: 1\n",
            "when input is tensor([24, 43, 58,  5, 57,  1]) the target: 46\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target: 43\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target: 39\n",
            "when input is tensor([44]) the target: 53\n",
            "when input is tensor([44, 53]) the target: 56\n",
            "when input is tensor([44, 53, 56]) the target: 1\n",
            "when input is tensor([44, 53, 56,  1]) the target: 58\n",
            "when input is tensor([44, 53, 56,  1, 58]) the target: 46\n",
            "when input is tensor([44, 53, 56,  1, 58, 46]) the target: 39\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target: 58\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target: 1\n",
            "when input is tensor([52]) the target: 58\n",
            "when input is tensor([52, 58]) the target: 1\n",
            "when input is tensor([52, 58,  1]) the target: 58\n",
            "when input is tensor([52, 58,  1, 58]) the target: 46\n",
            "when input is tensor([52, 58,  1, 58, 46]) the target: 39\n",
            "when input is tensor([52, 58,  1, 58, 46, 39]) the target: 58\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target: 1\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target: 46\n",
            "when input is tensor([25]) the target: 17\n",
            "when input is tensor([25, 17]) the target: 27\n",
            "when input is tensor([25, 17, 27]) the target: 10\n",
            "when input is tensor([25, 17, 27, 10]) the target: 0\n",
            "when input is tensor([25, 17, 27, 10,  0]) the target: 21\n",
            "when input is tensor([25, 17, 27, 10,  0, 21]) the target: 1\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target: 54\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "fUJAN2lw7mj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Embedding table"
      ],
      "metadata": {
        "id": "kI72BievY5HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx, targets are both (B, T) tensors\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # need to reshape as Pytorch Cross Entropy expects 2d logits and 1D targets\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx);\n",
        "            logits = logits[:,-1,:]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "    # def generate(self, idx, max_new_tokens):\n",
        "    #     # idx is (B, T) array of indices in the current context\n",
        "    #     for _ in range(max_new_tokens):\n",
        "    #         logits, _ = self(idx) # inference (B, T, C)\n",
        "    #         # indexing (not slicing) doesn't retain dimension, becomes (B, C)\n",
        "    #         logits = logits[:, -1, :]\n",
        "    #         # apply softmax to get probabilities\n",
        "    #         probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "    #         # sample from the distribution\n",
        "    #         idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "    #         # append sampled index to the running sequence\n",
        "    #         idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    #     return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(-math.log(1/65))\n",
        "print(loss.item())\n",
        "print(logits.shape)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhfyifhA7n8A",
        "outputId": "9772b8b5-6b88-4643-e769-c1539411ecad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.174387269895637\n",
            "4.725051403045654\n",
            "torch.Size([256, 65])\n",
            "\n",
            "emdhPRufnIAsF\n",
            "-J':qOGMClYzOc'KTf,Z\n",
            "cuRoRgYuKMCJhfto,pos.!D&SWFSiHwDba&!3CHzlKthxYq!,.qGD.qID?.tNGzX3'fdr:$-eygqHSYfPZXV\n",
            "j'fvtOyL$b.-icacNVIwksq\n",
            "lbf\n",
            ",kFsvz,&q\n",
            "tsWvtIGemEVsrOrEtSGoPh$hiHA,zZIXbWxYTnNhu&\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KWa4BVoaNfc",
        "outputId": "5090f98e-dd9c-46d2-88b6-d53c61800d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 65)\n",
            ")\n",
            "\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "hWkifsYwY3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in range(10000):\n",
        "\n",
        "    # mini-batch\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update step\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnmDwoSya45Y",
        "outputId": "74c040fe-1428-45a2-cb69-2dc61a60510b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5047218799591064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yJ508TwctBY",
        "outputId": "503785ed-dc6d-4bea-f849-a5e77bf0219c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hananfou t llaceit, br g inde, WAThyoecar, d he curor ance bur bas tot HAD arutrithat ltuanooul prthembu imy vehJTh, ucerir:\n",
            "\n",
            "Whe uieformy t't iofonentareethabearusllin Fie thyomoordomed fesathe.\n",
            "D ou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Add GPU, validation losses and refactor"
      ],
      "metadata": {
        "id": "pcZV9rexeunW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lf2P4Zhe0uW",
        "outputId": "b61272ea-eb03-4a48-90d1-acf9bdacfbec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "od nos CAy go ghanoray t, co haringoudrou clethe k,LARof fr werar,\n",
            "Is fa!\n",
            "\n",
            "\n",
            "Thilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\n",
            "S:\n",
            "STENENEat I athe thou\n",
            "CPU times: user 6.22 s, sys: 22.8 ms, total: 6.24 s\n",
            "Wall time: 6.45 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sMYL2n0f3tg",
        "outputId": "d81ecaeb-b542-4c6b-ee72-e61a3fd47682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 65)\n",
            ")\n",
            "\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: Attention explored"
      ],
      "metadata": {
        "id": "vJTt35AslXoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 1: Averaging"
      ],
      "metadata": {
        "id": "FcbRgLgvlfWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.tensor(range(64), dtype=torch.float32)\n",
        "x = x.view((B, T, C))\n",
        "print(x.shape)\n",
        "print(x[:2].shape)\n",
        "print(x[1].shape)\n",
        "print(x[1,:3].shape)\n",
        "print(x[0])\n",
        "print(x[0,:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gC1tWUzljTn",
        "outputId": "996c5e82-331b-453d-91b7-d9aab516d78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 2])\n",
            "torch.Size([2, 8, 2])\n",
            "torch.Size([8, 2])\n",
            "torch.Size([3, 2])\n",
            "tensor([[ 0.,  1.],\n",
            "        [ 2.,  3.],\n",
            "        [ 4.,  5.],\n",
            "        [ 6.,  7.],\n",
            "        [ 8.,  9.],\n",
            "        [10., 11.],\n",
            "        [12., 13.],\n",
            "        [14., 15.]])\n",
            "tensor([[0., 1.],\n",
            "        [2., 3.],\n",
            "        [4., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev,0) # expects (C,)"
      ],
      "metadata": {
        "id": "BIdGFozImGHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us-lMRc1nsdK",
        "outputId": "5c344744-f5c3-4841-a72f-c140e03e3682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.],\n",
              "        [ 2.,  3.],\n",
              "        [ 4.,  5.],\n",
              "        [ 6.,  7.],\n",
              "        [ 8.,  9.],\n",
              "        [10., 11.],\n",
              "        [12., 13.],\n",
              "        [14., 15.]])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGlJhzn4nkBc",
        "outputId": "e572944f-54d8-4542-c5a0-8463b51bd755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [1., 2.],\n",
              "        [2., 3.],\n",
              "        [3., 4.],\n",
              "        [4., 5.],\n",
              "        [5., 6.],\n",
              "        [6., 7.],\n",
              "        [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN8Y6Ynqgqk2",
        "outputId": "1216378e-4db6-4402-d5d1-c80adb087fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  1.],\n",
              "         [ 2.,  3.],\n",
              "         [ 4.,  5.],\n",
              "         [ 6.,  7.],\n",
              "         [ 8.,  9.],\n",
              "         [10., 11.],\n",
              "         [12., 13.],\n",
              "         [14., 15.]],\n",
              "\n",
              "        [[16., 17.],\n",
              "         [18., 19.],\n",
              "         [20., 21.],\n",
              "         [22., 23.],\n",
              "         [24., 25.],\n",
              "         [26., 27.],\n",
              "         [28., 29.],\n",
              "         [30., 31.]],\n",
              "\n",
              "        [[32., 33.],\n",
              "         [34., 35.],\n",
              "         [36., 37.],\n",
              "         [38., 39.],\n",
              "         [40., 41.],\n",
              "         [42., 43.],\n",
              "         [44., 45.],\n",
              "         [46., 47.]],\n",
              "\n",
              "        [[48., 49.],\n",
              "         [50., 51.],\n",
              "         [52., 53.],\n",
              "         [54., 55.],\n",
              "         [56., 57.],\n",
              "         [58., 59.],\n",
              "         [60., 61.],\n",
              "         [62., 63.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p94pqksugSHR",
        "outputId": "c9afc9e3-5b1e-4805-98ec-e6b159d02f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  1.],\n",
              "         [ 1.,  2.],\n",
              "         [ 2.,  3.],\n",
              "         [ 3.,  4.],\n",
              "         [ 4.,  5.],\n",
              "         [ 5.,  6.],\n",
              "         [ 6.,  7.],\n",
              "         [ 7.,  8.]],\n",
              "\n",
              "        [[16., 17.],\n",
              "         [17., 18.],\n",
              "         [18., 19.],\n",
              "         [19., 20.],\n",
              "         [20., 21.],\n",
              "         [21., 22.],\n",
              "         [22., 23.],\n",
              "         [23., 24.]],\n",
              "\n",
              "        [[32., 33.],\n",
              "         [33., 34.],\n",
              "         [34., 35.],\n",
              "         [35., 36.],\n",
              "         [36., 37.],\n",
              "         [37., 38.],\n",
              "         [38., 39.],\n",
              "         [39., 40.]],\n",
              "\n",
              "        [[48., 49.],\n",
              "         [49., 50.],\n",
              "         [50., 51.],\n",
              "         [51., 52.],\n",
              "         [52., 53.],\n",
              "         [53., 54.],\n",
              "         [54., 55.],\n",
              "         [55., 56.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage2a: Matrix Multiplication\n",
        "\n",
        "The central idea is that by changing the elements of the multiplying matrix you can control the aggregation type and extent.\n",
        "\n",
        "In the Transformer model's self-attention mechanism, tokens are aggregated based on their importance or relevance, and this importance is dynamically learned. Instead of static ones, zeros, or normalized values, the Transformer learns weights (or attention scores) to aggregate tokens in a context-aware manner."
      ],
      "metadata": {
        "id": "GIqPWT904AR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gddCyZrf7I_J",
        "outputId": "d8d62e55-047d-4154-8671-34b09ea50c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDpP_1kQ7oRl",
        "outputId": "2e4b9bfe-20e2-4d6a-d52a-0291d2c61c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a/torch.sum(a, dim=1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Da1ep274Kx",
        "outputId": "f8f826b3-8d9d-40cd-81e7-2b193c7b8eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 2b: wei\n",
        "\n",
        "PyTorch provides a way to perform batched matrix multiplication. Even if the matrices' dimensions aren't perfectly aligned, PyTorch can infer a batch dimension and apply the multiplication across each batch."
      ],
      "metadata": {
        "id": "BE6VRIYiIH5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei/wei.sum(dim=1, keepdims=True)\n",
        "xbow2 = wei @ x # (T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "print(xbow)\n",
        "\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7mHJyF3ILXD",
        "outputId": "be9dd572-5a1f-4653-ff99-3227476d718f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 3: Softmax"
      ],
      "metadata": {
        "id": "QRmAPh8eLfX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones((T,T)))\n",
        "print(tril)\n",
        "wei = torch.zeros((T,T))\n",
        "print(wei)\n",
        "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
        "print(wei)\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "xbow3 = wei @ x\n",
        "print(xbow3)\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovrR9wRjLj40",
        "outputId": "ef34c94c-e8c5-4e3b-a49d-9c86f5fcfa7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 4: Self-Attention"
      ],
      "metadata": {
        "id": "ja7AEKltc1rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "tril = torch.tril(torch.ones((T,T)))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "print(wei.shape)\n",
        "print(x.shape)\n",
        "print(out.shape)\n",
        "print(wei)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-nZLSfae7x3",
        "outputId": "e1c6de27-1f34-439e-8c8d-39eb1bb811b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 8])\n",
            "torch.Size([4, 8, 32])\n",
            "torch.Size([4, 8, 32])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query and key interaction\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B,T,head_size)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,head_size) @ (B,T,head_size) --> (B,T,T)\n",
        "print(wei.shape)\n",
        "print(wei[0])\n",
        "\n",
        "tril = torch.tril(torch.ones((T,T)))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
        "print(wei[0])\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei[0])\n",
        "out = wei @ x\n",
        "\n",
        "print(wei.shape)\n",
        "print(x.shape)\n",
        "print(out.shape)\n",
        "print(wei)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLgqY7M9zlTr",
        "outputId": "d34b14c7-5990-41bf-e491-67da5f3c21be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 8])\n",
            "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
            "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
            "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
            "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
            "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
            "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "torch.Size([4, 8, 8])\n",
            "torch.Size([4, 8, 32])\n",
            "torch.Size([4, 8, 32])\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
            "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
            "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
            "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
            "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
            "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
            "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add value\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B,T,head_size)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,head_size) @ (B,T,head_size) --> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones((T,T)))\n",
        "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "print(wei.shape)\n",
        "print(x.shape)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGnNSQh3s10",
        "outputId": "73cb4651-c46d-4784-90d4-8307bc935b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 8])\n",
            "torch.Size([4, 8, 32])\n",
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones((T,T)))\n",
        "print(tril[:T, :T])\n",
        "print(tril[:2, :2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5HmAUYqaDhF",
        "outputId": "45ccc6d2-2b43-499d-aff6-48577d2b8ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1., 0.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Add Token Embeddings and Position Embeddings"
      ],
      "metadata": {
        "id": "WwPggQt-iXbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5re5AeviZka",
        "outputId": "bc9991ac-d596-4e7d-c680-6e9ad6bbfafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4801, val loss 4.4801\n",
            "step 300: train loss 2.5404, val loss 2.5566\n",
            "step 600: train loss 2.5160, val loss 2.5335\n",
            "step 900: train loss 2.4967, val loss 2.5149\n",
            "step 1200: train loss 2.5106, val loss 2.5254\n",
            "step 1500: train loss 2.4853, val loss 2.5109\n",
            "step 1800: train loss 2.4966, val loss 2.5198\n",
            "step 2100: train loss 2.4949, val loss 2.5100\n",
            "step 2400: train loss 2.4937, val loss 2.5102\n",
            "step 2700: train loss 2.5040, val loss 2.5114\n",
            "\n",
            " ald, arhis'sho risisthanthatarend un'soto vat s kn, use he ute f whongeindd t acoe ts ansur thy ppr h.\n",
            "\n",
            "\n",
            "Y:\n",
            "KIIsqu pcinded chor whave o se bll owhored miner t ooon'stoume wh tomo! fifoveghind hiarnge\n",
            "CPU times: user 7.01 s, sys: 25.6 ms, total: 7.04 s\n",
            "Wall time: 7.08 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQpnvwQlf9kO",
        "outputId": "89e6d879-38fb-4932-985d-103b84ebb7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Total Parameters: 4481\n",
            "Trainable Parameters: 4481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Attention Head"
      ],
      "metadata": {
        "id": "nQPuFdmocrTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnLu1muAcwHP",
        "outputId": "4fac9cd9-4d1a-4a5b-d382-1b3da9b5fa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1990, val loss 4.2031\n",
            "step 300: train loss 2.5062, val loss 2.5121\n",
            "step 600: train loss 2.4679, val loss 2.4931\n",
            "step 900: train loss 2.4549, val loss 2.4599\n",
            "step 1200: train loss 2.4556, val loss 2.4681\n",
            "step 1500: train loss 2.4336, val loss 2.4524\n",
            "step 1800: train loss 2.4198, val loss 2.4474\n",
            "step 2100: train loss 2.4168, val loss 2.4283\n",
            "step 2400: train loss 2.3974, val loss 2.4266\n",
            "step 2700: train loss 2.4021, val loss 2.4301\n",
            "\n",
            "Ad winth odl Cod bardi ma thingro rKesell, thimledl gu berom y whashr wiantorou hd lis sous w lous inn\n",
            "Farils peen ishn shime mip, cize fo thit wy; fo othur out bllow'm tato rtle hflo aty bus baturs s\n",
            "CPU times: user 10.3 s, sys: 20.3 ms, total: 10.4 s\n",
            "Wall time: 10.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wtOcRDJfVwv",
        "outputId": "23eec72e-cd74-4ddf-c603-be90585361b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_head.tril \t torch.Size([8, 8])\n",
            "sa_head.query.weight \t torch.Size([32, 32])\n",
            "sa_head.key.weight \t torch.Size([32, 32])\n",
            "sa_head.value.weight \t torch.Size([32, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_head): Head(\n",
            "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5: Multi-head Attention"
      ],
      "metadata": {
        "id": "klhU_aJvedIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDMCVTikeh8Q",
        "outputId": "672ced21-150d-4902-9025-b067ba2c7e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2229, val loss 4.2220\n",
            "step 300: train loss 2.4194, val loss 2.4281\n",
            "step 600: train loss 2.3258, val loss 2.3593\n",
            "step 900: train loss 2.2979, val loss 2.3138\n",
            "step 1200: train loss 2.2777, val loss 2.3133\n",
            "step 1500: train loss 2.2428, val loss 2.2975\n",
            "step 1800: train loss 2.2339, val loss 2.3005\n",
            "step 2100: train loss 2.2230, val loss 2.2775\n",
            "step 2400: train loss 2.2117, val loss 2.2859\n",
            "step 2700: train loss 2.2099, val loss 2.2828\n",
            "\n",
            "A has thave dus! Tarwingr ass, gocrtysell, to hath ris I! Thas whash wifall foulld lives\n",
            "Whe ways.\n",
            "\n",
            "\n",
            "But when pere ishot 't ther Cacizen: cre to you of thur thing swa'm to of tle hal waryte hablefrs s\n",
            "CPU times: user 18.2 s, sys: 49.6 ms, total: 18.2 s\n",
            "Wall time: 19 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1tFjjcJgodS",
        "outputId": "903a0d62-2d60-49d7-9e32-981cdf24d3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.0.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.1.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.2.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.3.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.value.weight \t torch.Size([8, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_heads): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6: Feed forward"
      ],
      "metadata": {
        "id": "A8c6hCfLGkiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpcSZL3AGpRV",
        "outputId": "40f6b41a-a98c-4bef-da14-a308230856fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1839, val loss 4.1819\n",
            "step 300: train loss 2.3986, val loss 2.4250\n",
            "step 600: train loss 2.3145, val loss 2.3607\n",
            "step 900: train loss 2.2494, val loss 2.2812\n",
            "step 1200: train loss 2.2289, val loss 2.2805\n",
            "step 1500: train loss 2.2077, val loss 2.2451\n",
            "step 1800: train loss 2.2009, val loss 2.2584\n",
            "step 2100: train loss 2.1882, val loss 2.2341\n",
            "step 2400: train loss 2.1832, val loss 2.2417\n",
            "step 2700: train loss 2.1591, val loss 2.2284\n",
            "\n",
            "Casto oll won se hin\n",
            "Will you sucleed\n",
            "S J\n",
            "LUKE OF Ser,\n",
            "thims aly ever ene; deed blis thall grar to the sraws hore, a nothly,-dend't thend Cit the the thot pe;\n",
            "Ame to the sweree soncien to: you fa and \n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.0.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.0.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.1.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.1.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.2.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.2.value.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.tril \t torch.Size([8, 8])\n",
            "sa_heads.heads.3.query.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.key.weight \t torch.Size([8, 32])\n",
            "sa_heads.heads.3.value.weight \t torch.Size([8, 32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "ffwd.net.0.bias \t torch.Size([32])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (sa_heads): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            "  (ffwd): FeedForward(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Total Parameters: 8609\n",
            "Trainable Parameters: 8609\n",
            "CPU times: user 18 s, sys: 33.8 ms, total: 18 s\n",
            "Wall time: 18.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7: Block"
      ],
      "metadata": {
        "id": "fNQ8vnNFJ67p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1detzieJ_C3",
        "outputId": "5616b1f7-9b9e-4ce3-893e-637279ef186a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1635, val loss 4.1642\n",
            "step 300: train loss 2.4873, val loss 2.4880\n",
            "step 600: train loss 2.3878, val loss 2.4132\n",
            "step 900: train loss 2.3357, val loss 2.3376\n",
            "step 1200: train loss 2.2948, val loss 2.3032\n",
            "step 1500: train loss 2.2547, val loss 2.2894\n",
            "step 1800: train loss 2.2380, val loss 2.2550\n",
            "step 2100: train loss 2.2237, val loss 2.2598\n",
            "step 2400: train loss 2.2028, val loss 2.2489\n",
            "step 2700: train loss 2.1903, val loss 2.2382\n",
            "\n",
            "Sample from the model:\n",
            "\n",
            "\n",
            "BERENETING:\n",
            "Asas seeereds gearty lixe there pelop.\n",
            "Th fearr is.\n",
            "I know sash Beast is; hee pleious therbes.s you thee our feinnssour you couthe, To deark lincise ford wam baksy peene regatere,\n",
            "So 'cin\n",
            "-----\n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([32, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Total Parameters: 12737\n",
            "Trainable Parameters: 12737\n",
            "CPU times: user 29.4 s, sys: 53.2 ms, total: 29.5 s\n",
            "Wall time: 29.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8: Residual Connections\n",
        "\n",
        "In Model 7, even though we had a bigger network than Model 6, with more parameters, our loss was higher. The reason is that deeper networks need things like Layer Normalization and Residual Connections for good results.\n",
        "\n",
        "---\n",
        "\n",
        "The idea behind adding an extra Linear layer in the `MultiHeadAttention` and `FeedForward` modules is to allow the model to have more capacity and learn a more complex function. While your model can learn without these extra linear layers, adding them might help in learning more complex relationships in the data, especially when the amount of data is large.\n",
        "\n",
        "### FeedForward\n",
        "In the `FeedForward` module, having an extra linear layer can be thought of as having two layers of transformation. This can help the model to learn more complex relationships in the data. However, it's also common to have different dimensions for the inner layer. For example:\n",
        "```python\n",
        "self.net = nn.Sequential(\n",
        "    nn.Linear(n_embd, 4 * n_embd),  # Increase dimensionality\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4 * n_embd, n_embd),  # Project back to original dimensionality\n",
        ")\n",
        "```\n",
        "This allows the model to have more capacity in the feed-forward part.\n",
        "\n",
        "### MultiHeadAttention\n",
        "For the `MultiHeadAttention`, the additional Linear layer (projection layer) is typically added to allow the concatenated heads to be transformed back to the original embedding dimensionality. It's a standard practice in the original Transformer model. When you concatenate the outputs of multiple attention heads, the dimensionality of the resultant tensor is `num_heads * head_size`. The additional Linear layer projects it back to the original dimensionality `n_embd`.\n",
        "\n",
        "### Conclusion\n",
        "While adding these layers increases the model's capacity to learn complex functions, it also increases the number of parameters, and therefore the risk of overfitting, especially when the amount of training data is limited. If you find that the model is overfitting, you might want to remove some of these layers or use regularization techniques like dropout. Similarly, if the model is underfitting, adding more layers or increasing the size of the existing layers could be beneficial.\n",
        "\n",
        "In practice, the architecture of the model is usually determined based on empirical results obtained through experimenting with different configurations and observing their performance on the validation dataset. If the simpler model without additional Linear layers is performing well in terms of generalization to the validation dataset, it might be the preferable choice."
      ],
      "metadata": {
        "id": "lWOjMgpaP_Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # the the additional Linear layer (projection layer)\n",
        "        # is typically added to allow the concatenated heads\n",
        "        # to be transformed back to the original embedding dimensionality.\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"\\nModel's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI6-fi7yQnSs",
        "outputId": "e464f155-901d-4688-c676-06f7e301e682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6208, val loss 4.6182\n",
            "step 300: train loss 2.4846, val loss 2.4859\n",
            "step 600: train loss 2.3614, val loss 2.3625\n",
            "step 900: train loss 2.2820, val loss 2.2898\n",
            "step 1200: train loss 2.2169, val loss 2.2440\n",
            "step 1500: train loss 2.1824, val loss 2.2152\n",
            "step 1800: train loss 2.1588, val loss 2.1988\n",
            "step 2100: train loss 2.1231, val loss 2.1659\n",
            "step 2400: train loss 2.1084, val loss 2.1568\n",
            "step 2700: train loss 2.1072, val loss 2.1533\n",
            "\n",
            "Sample from the model:\n",
            "\n",
            "Praqows couss, my sitior the vercent ast him, the havince to wise our seame? net upeponjresconsts folsuled ever I\n",
            "What of Is wertmen:\n",
            "Olam; thum I lovoings:\n",
            "Mane cout you the but siden and up thing,\n",
            "W\n",
            "-----\n",
            "\n",
            "Model's state_dict:\n",
            "token_embedding_table.weight \t torch.Size([65, 32])\n",
            "position_embedding_table.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.0.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.0.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.0.sa.proj.bias \t torch.Size([32])\n",
            "blocks.0.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.0.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.0.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.0.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.1.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.1.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.1.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.1.sa.proj.bias \t torch.Size([32])\n",
            "blocks.1.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.1.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.1.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.1.ffwd.net.2.bias \t torch.Size([32])\n",
            "blocks.2.sa.heads.0.tril \t torch.Size([8, 8])\n",
            "blocks.2.sa.heads.0.query.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.0.key.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.0.value.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.1.tril \t torch.Size([8, 8])\n",
            "blocks.2.sa.heads.1.query.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.1.key.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.1.value.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.2.tril \t torch.Size([8, 8])\n",
            "blocks.2.sa.heads.2.query.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.2.key.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.2.value.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.3.tril \t torch.Size([8, 8])\n",
            "blocks.2.sa.heads.3.query.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.3.key.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.heads.3.value.weight \t torch.Size([8, 32])\n",
            "blocks.2.sa.proj.weight \t torch.Size([32, 32])\n",
            "blocks.2.sa.proj.bias \t torch.Size([32])\n",
            "blocks.2.ffwd.net.0.weight \t torch.Size([128, 32])\n",
            "blocks.2.ffwd.net.0.bias \t torch.Size([128])\n",
            "blocks.2.ffwd.net.2.weight \t torch.Size([32, 128])\n",
            "blocks.2.ffwd.net.2.bias \t torch.Size([32])\n",
            "lm_head.weight \t torch.Size([65, 32])\n",
            "lm_head.bias \t torch.Size([65])\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "Total Parameters: 41921\n",
            "Trainable Parameters: 41921\n",
            "CPU times: user 47.4 s, sys: 102 ms, total: 47.5 s\n",
            "Wall time: 47.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 9: Layer Normalization"
      ],
      "metadata": {
        "id": "-R6KsyMkKzaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'\\nTrainable Parameters: {trainable_params}')\n",
        "\n",
        "# Print model's state_dict\n",
        "# print(\"\\nModel's state_dict:\")\n",
        "# for param_tensor in m.state_dict():\n",
        "#     print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIojXItEK2E5",
        "outputId": "a2c2f6ed-633a-40fd-9d58-d842c565ec1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3090, val loss 4.3083\n",
            "step 300: train loss 2.5221, val loss 2.5322\n",
            "step 600: train loss 2.3602, val loss 2.3642\n",
            "step 900: train loss 2.2725, val loss 2.2829\n",
            "step 1200: train loss 2.1968, val loss 2.2269\n",
            "step 1500: train loss 2.1618, val loss 2.1961\n",
            "step 1800: train loss 2.1333, val loss 2.1737\n",
            "step 2100: train loss 2.0997, val loss 2.1378\n",
            "step 2400: train loss 2.0890, val loss 2.1313\n",
            "step 2700: train loss 2.0820, val loss 2.1246\n",
            "\n",
            "Sample from the model:\n",
            "\n",
            "PORINGO: grust my sithou kne, that warst, to sown hy to whan wise our seam fate, upos for of,\n",
            "Whear bell,\n",
            "Sere his dearge-ss?\n",
            "\n",
            "Whmongly de; that is he inestiove;\n",
            "\n",
            "And you the but sided and up the mim \n",
            "-----\n",
            "\n",
            "Total Parameters: 42369\n",
            "\n",
            "Trainable Parameters: 42369\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "CPU times: user 52.6 s, sys: 105 ms, total: 52.7 s\n",
            "Wall time: 52.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 10: Dropout (and code cleanup)"
      ],
      "metadata": {
        "id": "IWwasd0FML5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'\\nTrainable Parameters: {trainable_params}')\n",
        "\n",
        "# Print model's state_dict\n",
        "# print(\"\\nModel's state_dict:\")\n",
        "# for param_tensor in m.state_dict():\n",
        "#     print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SamRQ_ZEMhge",
        "outputId": "0d5d610d-7078-449b-a2d9-da8204e7d310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4066, val loss 4.3990\n",
            "step 300: train loss 2.5158, val loss 2.5268\n",
            "step 600: train loss 2.4060, val loss 2.4122\n",
            "step 900: train loss 2.3254, val loss 2.3335\n",
            "step 1200: train loss 2.2503, val loss 2.2766\n",
            "step 1500: train loss 2.2171, val loss 2.2317\n",
            "step 1800: train loss 2.2017, val loss 2.2102\n",
            "step 2100: train loss 2.1681, val loss 2.1876\n",
            "step 2400: train loss 2.1387, val loss 2.1708\n",
            "step 2700: train loss 2.1250, val loss 2.1755\n",
            "\n",
            "Sample from the model:\n",
            "\n",
            "me orcail, at prand thine her, apnd wite's ath reas coud!\n",
            "\n",
            "QOr not:\n",
            "For.\n",
            "\n",
            "GETUNG:\n",
            "Was not it dond ang wom tros; hin BUTI:\n",
            "Coche mowe ir adorlee; firlow: forks her, a wilTIO:\n",
            "\n",
            "Fors and greand berm? I s\n",
            "-----\n",
            "\n",
            "Total Parameters: 54977\n",
            "\n",
            "Trainable Parameters: 54977\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "CPU times: user 1min 13s, sys: 151 ms, total: 1min 13s\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model to HuggingFace Hub"
      ],
      "metadata": {
        "id": "4M0vXi9uk-4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path where you want to save your model\n",
        "save_directory = os.getcwd()  # This gets the current working directory\n",
        "model_path = os.path.join(save_directory, \"model_weights.pth\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "88xviwTzVPE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save the stoi and itos mappings\n",
        "with open(f\"{save_directory}/stoi.json\", 'w') as f:\n",
        "    json.dump(stoi, f)\n",
        "\n",
        "with open(f\"{save_directory}/itos.json\", 'w') as f:\n",
        "    json.dump(itos, f)\n"
      ],
      "metadata": {
        "id": "NqEP1VqrUtZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49OadFGWBAQ",
        "outputId": "2afb80e7-11ec-4464-b836-d02eea871571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/295.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f8d652264d5549c4bae8d3c2b4ae07e5",
            "00769d0408784442b9873a1a3f3531e9",
            "a66d6c9db83c4d9ca02bbfaa47cc26f8",
            "8da6b6c380b64aeba2288fdf94965e83",
            "ece50dfa0d2e491c93045ffe2641a639",
            "5a9a621053e04519af341ffbea4da056",
            "67e66cf7738f49b38b0c6fb0d081d4b0",
            "085bdefa843f4609841ca81dcbd2bb10",
            "ed41e2cf54104a9ea30892a5f74f9853",
            "adee8b25f4e74bdb996cb7da86872e30",
            "87c0df615fcb400b918631d612b38b35",
            "a39e5a7493f14baf8ad8df4719a6a35b",
            "915f0afdc41c478caf14fb07fe83ff3c",
            "a814a1150d714839898c43cbfe2c891c",
            "e22b69c8ac68476faa4357d688368c2e",
            "881b54c5363047959961578b3a59fdeb",
            "68b0a18fb90b4ed9b0c32740e29b5f77",
            "6cc32b2fc3a44bb38f4d0967c68f87c6",
            "6a21612babce4f04a6cc17f30399cf85",
            "4f332d1311e8420fb62c622a3c22bd89",
            "03158b7ce7b9430baeeb75c577bc6893",
            "46809cc75d334e1ab36991774b7e4e20",
            "f1679f69bbdb4029bd3a9968288d80fb",
            "f4da463e9c38478c86321e80c0afd55a",
            "fc6c61cd1d714d6caee1f1f4689fc62e",
            "0a9f6fd91d23478d92aba17f3b5a6c6d",
            "94480d8a533748a5881cbe0b96719ecb",
            "9cfac87361784829b5b7a1e04dc96174",
            "5cfb11f9790e47daa0e95f0e9b8bdf8f"
          ]
        },
        "id": "CmHYcWPnVhVD",
        "outputId": "af41a662-e1dd-49e3-96b2-fef3c7dd3c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8d652264d5549c4bae8d3c2b4ae07e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "\n",
        "# Define the local file path, repository ID, and path in the repository\n",
        "local_file_path = \"./model_weights.pth\"  # replace with your local file path\n",
        "repo_id = \"RubyDiamond/mini-gpt\"  # replace with your repository ID\n",
        "path_in_repo = \"model_weights.pth\"  # replace with your desired path in the repository\n",
        "\n",
        "# Upload the file\n",
        "url = upload_file(\n",
        "    path_or_fileobj=local_file_path,\n",
        "    path_in_repo=path_in_repo,\n",
        "    repo_id=repo_id\n",
        ")\n",
        "\n",
        "print(f\"File uploaded to {url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e5818c2bb45c4d389bd359fdd8b34118",
            "6af92f86387d4ac6b6637ab8afd0fb68",
            "7c38ffb8033f47c5946860a8dee45afe",
            "40f8eae6e59a4389b58c48268f84e523",
            "f7945267983a487eb354197c2a0a9224",
            "9e784b9b51c843be82b80561f8246bac",
            "32929629656342bea44b0d656e4638a0",
            "38d07110551f46bdad344ffb521cd89e",
            "44ad3ff3858847229e308dbda9198cba",
            "0a57dada32ea4cadbf3f734f1e870835",
            "c951524d248540ea968c4b530fceb7ab"
          ]
        },
        "id": "jTZ25sqSf4an",
        "outputId": "d19093bf-1d51-4f8c-a9d5-ee0bfc46ad81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_weights.pth:   0%|          | 0.00/263k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5818c2bb45c4d389bd359fdd8b34118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded to https://huggingface.co/RubyDiamond/mini-gpt/blob/main/model_weights.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "\n",
        "# Define the local file path, repository ID, and path in the repository\n",
        "local_file_path = \"./stoi.json\"  # replace with your local file path\n",
        "repo_id = \"RubyDiamond/mini-gpt\"  # replace with your repository ID\n",
        "path_in_repo = \"stoi.json\"  # replace with your desired path in the repository\n",
        "\n",
        "# Upload the file\n",
        "url = upload_file(\n",
        "    path_or_fileobj=local_file_path,\n",
        "    path_in_repo=path_in_repo,\n",
        "    repo_id=repo_id\n",
        ")\n",
        "\n",
        "print(f\"File uploaded to {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80FbhUfplMvA",
        "outputId": "fd0779cd-d8ef-44e2-c473-9af194f2d141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded to https://huggingface.co/RubyDiamond/mini-gpt/blob/main/stoi.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model to Google Drive"
      ],
      "metadata": {
        "id": "ElV2XoFUsNwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBePr436sRgs",
        "outputId": "8be49878-bca3-4f47-a2ba-84e692ef29eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the save directory\n",
        "save_directory = \"/content/drive/MyDrive/projects/mini-gpt\"\n",
        "\n",
        "# Check if directory exists, if not create it\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "# Now, save the model\n",
        "torch.save(m.state_dict(), f\"{save_directory}/model_weights.pth\")\n"
      ],
      "metadata": {
        "id": "Bi9HsmM0ws7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the directory in Google Drive to save the files\n",
        "save_directory = '/content/drive/MyDrive/projects/mini-gpt'  # Replace with your directory\n",
        "\n",
        "# Save Model Weights\n",
        "torch.save(model.state_dict(), f\"{save_directory}/model_weights.pth\")\n",
        "\n",
        "# Save itos and stoi as json files\n",
        "with open(f\"{save_directory}/itos.json\", 'w') as f:\n",
        "    json.dump(itos, f)\n",
        "\n",
        "with open(f\"{save_directory}/stoi.json\", 'w') as f:\n",
        "    json.dump(stoi, f)\n",
        "\n",
        "# Save hyperparameters and other components as a json file\n",
        "hyperparameters = {\n",
        "    'block_size': 256,\n",
        "    'n_embd': 384,\n",
        "    'n_head': 6,\n",
        "    'n_layer': 6,\n",
        "    'dropout': 0.2,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "with open(f\"{save_directory}/hyperparameters.json\", 'w') as f:\n",
        "    json.dump(hyperparameters, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "KQvobbTqswcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 11: Refactor (Parameterize)"
      ],
      "metadata": {
        "id": "z8kxn1izD7XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "config = {\n",
        "    'block_size': 8,\n",
        "    'n_embd': 32,\n",
        "    'n_head': 4,\n",
        "    'n_layer': 4,\n",
        "    'dropout': 0.1,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "block_size = config[\"block_size\"]\n",
        "batch_size = 32\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, head_size, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, num_heads, head_size, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(n_embd, head_size, block_size, dropout) for _ in range(num_heads))\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_embd, n_head, head_size, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, config[\"n_embd\"])\n",
        "        self.position_embedding_table = nn.Embedding(config[\"block_size\"], config[\"n_embd\"])\n",
        "        self.blocks = nn.Sequential(*[Block(config[\"n_embd\"], config[\"n_head\"],config[\"block_size\"],config[\"dropout\"]) for _ in range(config[\"n_layer\"])])\n",
        "        self.ln_f = nn.LayerNorm(config[\"n_embd\"])\n",
        "        self.lm_head = nn.Linear(config[\"n_embd\"], vocab_size)\n",
        "        self.device = config[\"device\"]\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, block_size):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size, config)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200, block_size=config[\"block_size\"])[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'\\nTrainable Parameters: {trainable_params}')\n",
        "\n",
        "# Print model's state_dict\n",
        "# print(\"\\nModel's state_dict:\")\n",
        "# for param_tensor in m.state_dict():\n",
        "#     print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLgprAOfEE4_",
        "outputId": "f9459f34-4a68-4008-f669-f7b7639bba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4066, val loss 4.3990\n",
            "step 300: train loss 2.5158, val loss 2.5268\n",
            "step 600: train loss 2.4060, val loss 2.4122\n",
            "step 900: train loss 2.3254, val loss 2.3335\n",
            "step 1200: train loss 2.2503, val loss 2.2766\n",
            "step 1500: train loss 2.2171, val loss 2.2317\n",
            "step 1800: train loss 2.2017, val loss 2.2102\n",
            "step 2100: train loss 2.1681, val loss 2.1876\n",
            "step 2400: train loss 2.1387, val loss 2.1708\n",
            "step 2700: train loss 2.1250, val loss 2.1755\n",
            "\n",
            "Sample from the model:\n",
            "\n",
            "me orcail, at prand thine her, apnd wite's ath reas coud!\n",
            "\n",
            "QOr not:\n",
            "For.\n",
            "\n",
            "GETUNG:\n",
            "Was not it dond ang wom tros; hin BUTI:\n",
            "Coche mowe ir adorlee; firlow: forks her, a wilTIO:\n",
            "\n",
            "Fors and greand berm? I s\n",
            "-----\n",
            "\n",
            "Total Parameters: 54977\n",
            "\n",
            "Trainable Parameters: 54977\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 32)\n",
            "  (position_embedding_table): Embedding(8, 32)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "CPU times: user 1min 22s, sys: 166 ms, total: 1min 23s\n",
            "Wall time: 1min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 11: Scale up (and train on GPU)"
      ],
      "metadata": {
        "id": "fNANy-mzPEH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "config = {\n",
        "    'block_size': 256,\n",
        "    'n_embd': 384,\n",
        "    'n_head': 6,\n",
        "    'n_layer': 6,\n",
        "    'dropout': 0.2,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "block_size = config[\"block_size\"]\n",
        "batch_size = 64\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, head_size, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Transform to Query: (B, T, C) -> (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size,block_size))))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):  # x is of shape (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)  # Query: (B, T, head_size)\n",
        "        k = self.key(x)    # Key: (B, T, head_size)\n",
        "\n",
        "        # Compute attention weights: (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)  # Value: (B, T, head_size)\n",
        "        out = wei @ v  # Output: (B, T, head_size) -> weighted sum of value vectors\n",
        "\n",
        "        return out  # Output tensor of shape (B, T, head_size)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, num_heads, head_size, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(Head(n_embd, head_size, block_size, dropout) for _ in range(num_heads))\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_embd, n_head, head_size, block_size, dropout)\n",
        "        self.ffwd = FeedForward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, config[\"n_embd\"])\n",
        "        self.position_embedding_table = nn.Embedding(config[\"block_size\"], config[\"n_embd\"])\n",
        "        self.blocks = nn.Sequential(*[Block(config[\"n_embd\"], config[\"n_head\"],config[\"block_size\"],config[\"dropout\"]) for _ in range(config[\"n_layer\"])])\n",
        "        self.ln_f = nn.LayerNorm(config[\"n_embd\"])\n",
        "        self.lm_head = nn.Linear(config[\"n_embd\"], vocab_size)\n",
        "        self.device = config[\"device\"]\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, block_size):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size, config)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200, block_size=config[\"block_size\"])[0].tolist()))\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'\\nTrainable Parameters: {trainable_params}')\n",
        "\n",
        "# Print model's state_dict\n",
        "# print(\"\\nModel's state_dict:\")\n",
        "# for param_tensor in m.state_dict():\n",
        "#     print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "esbpraRBPZlA",
        "outputId": "feeb37df-4c85-434c-8c5a-279927d5ec16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4753, val loss 4.4709\n",
            "step 500: train loss 2.0800, val loss 2.1442\n",
            "step 1000: train loss 1.6669, val loss 1.8302\n",
            "step 1500: train loss 1.4938, val loss 1.6808\n",
            "step 2000: train loss 1.3909, val loss 1.6088\n",
            "step 2500: train loss 1.3228, val loss 1.5603\n",
            "step 3000: train loss 1.2662, val loss 1.5272\n",
            "step 3500: train loss 1.2215, val loss 1.5059\n",
            "step 4000: train loss 1.1823, val loss 1.4890\n",
            "step 4500: train loss 1.1460, val loss 1.4831\n",
            "\n",
            "Sample from the model:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens, block_size)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----\")\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'\\nTrainable Parameters: {trainable_params}')\n",
        "\n",
        "# Print model's state_dict\n",
        "# print(\"\\nModel's state_dict:\")\n",
        "# for param_tensor in m.state_dict():\n",
        "#     print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNkvuerIsnEy",
        "outputId": "7b4125b8-2ba5-44e5-a1f6-27156548947c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "\n",
            "Total Parameters: 10788929\n",
            "\n",
            "Trainable Parameters: 10788929\n",
            "\n",
            "Model's Structure: \n",
            "BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 384)\n",
            "  (position_embedding_table): Embedding(256, 384)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(\"\\nSample from the model:\")\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=1000, block_size=config[\"block_size\"])[0].tolist()))\n",
        "\n",
        "\n",
        "print(\"-----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC8v0ZGjsuX8",
        "outputId": "87b59357-cc42-4a38-fbe3-8b54d7262573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample from the model:\n",
            "\n",
            "i' tolded my hand\n",
            "This lack of truth's blind your own green enforce:\n",
            "We'll add, in gross choose to feelous windown down\n",
            "Shephing such a crowned eating, if you our murder,\n",
            "From the only gives, and you disposed, were'd England!\n",
            "But which with dispatcher, likes a rich pave,\n",
            "Hould far breather, to drunk and show you do forges.\n",
            "\n",
            "GLOUCESTER:\n",
            "Clarendent tickly now met did came but with the\n",
            "eading of kinstrady.\n",
            "BIAK:\n",
            "I am lady as our throne condital.\n",
            "\n",
            "CATESBY:\n",
            "Courageous wits much, my displant?\n",
            "\n",
            "EDWARD:\n",
            "I be a progicular.\n",
            "\n",
            "SIXINA:\n",
            "O God!\n",
            "O might!\n",
            "What mighty seem comests a bosom, then about marr's love.\n",
            "\n",
            "LADY ANNE:\n",
            "What, by the dead!\n",
            "\n",
            "LADY ANNE:\n",
            "I met that anced but thou art an ears?\n",
            "\n",
            "HASTINGS:\n",
            "Think it me in stay sight so.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Her, Ely and Aumerly and men!\n",
            "O hope for my sweet means witness bout without\n",
            "Art we are prompt for that tongue remove.\n",
            "Our suin the latter singly of Warwick's reposeth foots libert\n",
            "The flock and Sicinatio it master'd at mark.\n",
            "3 KING HENRY VI\n",
            "\n",
            "KING RICHARD I\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additonal Notes"
      ],
      "metadata": {
        "id": "f9fqJix1Q0AT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.Embedding`\n",
        "\n",
        "`torch.nn.Embedding` is used to create a lookup table where each row represents the embedding of a certain index, often corresponding to a word or a token in NLP. It is initialized with a `num_embeddings` parameter specifying the number of embeddings (or the size of the vocabulary) and an `embedding_dim` parameter specifying the size of each embedding vector.\n",
        "\n",
        "### Tensor Shapes:\n",
        "\n",
        "- If you input a tensor of shape `(B, T)`, where `B` is the batch size and `T` is the sequence length, you will get back a tensor of shape `(B, T, C)`, where `C` is the embedding dimension (`embedding_dim`).\n",
        "- Here, `B` represents the number of sequences in a batch, `T` represents the number of indices in each sequence, and `C` represents the number of elements in the embedding vector for each index.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Let's consider a simple example, detailing the shapes of the tensors:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define Vocabulary Size and Embedding Dimension\n",
        "vocab_size = 5  # e.g., {'<pad>': 0, 'the': 1, 'cat': 2, 'sat': 3, 'on': 4}\n",
        "embedding_dim = 3  # Each word is represented by a 3D vector\n",
        "\n",
        "# Create an Embedding Layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "# Define Input Tensor of shape (B, T), where B is batch size and T is sequence length\n",
        "input_tensor = torch.tensor([[1, 2, 3], [4, 1, 0]])  # 2 sequences, each of length 3\n",
        "# e.g., input_tensor:\n",
        "# tensor([[1, 2, 3],  # <-- Sequence 1: 'the cat sat'\n",
        "#         [4, 1, 0]])  # <-- Sequence 2: 'on the <pad>'\n",
        "\n",
        "# Get the Embeddings\n",
        "embeddings = embedding_layer(input_tensor)\n",
        "# The shape of the embeddings tensor will be (B, T, C) = (2, 3, 3)\n",
        "\n",
        "# Print the Input and Output Tensors\n",
        "print(\"Input Tensor (shape: {})\".format(input_tensor.shape))\n",
        "# Input Tensor (shape: torch.Size([2, 3]))\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 1, 0]])\n",
        "\n",
        "print(\"Embeddings Tensor (shape: {})\".format(embeddings.shape))\n",
        "# Embeddings Tensor (shape: torch.Size([2, 3, 3]))\n",
        "# tensor([[[ 0.3367,  0.1288, -1.4232],  # <-- Embeddings for Sequence 1: 'the cat sat'\n",
        "#          [-0.2694,  0.4839, -1.0219],\n",
        "#          [ 1.3312,  0.3535,  0.8394]],\n",
        "#\n",
        "#         [[ 1.0736, -0.7456,  1.1174],  # <-- Embeddings for Sequence 2: 'on the <pad>'\n",
        "#          [ 0.3367,  0.1288, -1.4232],\n",
        "#          [-0.5273, -0.1164,  0.1453]]], requires_grad=True)\n",
        "```\n",
        "\n",
        "### Summary:\n",
        "\n",
        "In this example, the input tensor of shape `(2, 3)` representing 2 sequences each of length 3 is passed through an embedding layer, and the output is a tensor of shape `(2, 3, 3)`, where each index in the input sequences is replaced by its corresponding 3-dimensional embedding."
      ],
      "metadata": {
        "id": "gcbZoEDP_n_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.functional.cross_entropy`\n",
        "\n",
        "`torch.nn.functional.cross_entropy` is a loss function that combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class, and it is used to measure the performance of a classification model whose output is a probability distribution over classes.\n",
        "\n",
        "### Difference between `torch.nn.functional.cross_entropy` and `torch.nn.CrossEntropyLoss`:\n",
        "\n",
        "1. **Functional API (`torch.nn.functional.cross_entropy`):**\n",
        "   - This is just a functional, stateless approach.\n",
        "   - Does not maintain state, i.e., it does not have internal parameters that need to be stored.\n",
        "   - This is typically used when you do not need to keep track of any state or parameters of the loss function between calls.\n",
        "\n",
        "2. **Module API (`torch.nn.CrossEntropyLoss`):**\n",
        "   - This is an object-oriented approach, where you first create an object of the loss function and then use it to calculate the loss.\n",
        "   - Can maintain state, and has parameters like `weight`, `size_average`, etc., which can be set when the object is created.\n",
        "   - Useful when you need to keep track of parameters or state of the loss function.\n",
        "\n",
        "### Tensor Shapes:\n",
        "\n",
        "- **Input (logits):** The shape is typically `(B, C, T)`, where `B` is the batch size, `C` is the number of classes, and `T` is the sequence length.\n",
        "- **Target:** The shape is `(B, T)`, where each element is the class index in the range `[0, C-1]`.\n",
        "\n",
        "### Use Case and Example:\n",
        "\n",
        "When you have sequences of logits and corresponding sequences of targets, you can use `torch.nn.functional.cross_entropy` to compute the loss. Below is an illustrative example:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a batch of logits (randomly initialized), shape: (B, T, C) -> (2, 3, 4)\n",
        "logits = torch.randn(2, 3, 4)\n",
        "# e.g., logits:\n",
        "# tensor([[[ 0.3367,  0.1288, -1.4232, -0.2694],\n",
        "#          [ 0.4839, -1.0219,  1.3312,  0.3535],\n",
        "#          [ 0.8394,  1.0736, -0.7456,  1.1174]],\n",
        "#\n",
        "#         [[ 0.1453, -0.5273, -0.1164,  0.1453],\n",
        "#          [ 0.6883,  0.7456,  0.8535, -0.6341],\n",
        "#          [-0.8234, -0.6341,  1.2345, -0.1234]]])\n",
        "\n",
        "# Define a batch of corresponding target class indices, shape: (B, T) -> (2, 3)\n",
        "targets = torch.tensor([[0, 2, 1], [3, 1, 0]], dtype=torch.long)\n",
        "# e.g., targets:\n",
        "# tensor([[0, 2, 1],\n",
        "#         [3, 1, 0]])\n",
        "\n",
        "# Compute the cross entropy loss\n",
        "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "# Print the Loss\n",
        "print(\"Cross Entropy Loss:\", loss.item())\n",
        "# Cross Entropy Loss: 2.3043 (Note: This value is illustrative and will vary due to the random initialization of logits)\n",
        "```\n",
        "\n",
        "Here, we reshape both logits and targets to be 1D before passing them to `F.cross_entropy` because it expects the input logits tensor to be 2D `(N, C)` where `N` is the number of samples and `C` is the number of classes, and the targets to be 1D. After the loss calculation, the single scalar loss value representing the mean loss across all samples in the batch is printed."
      ],
      "metadata": {
        "id": "5uRsSSVhFKaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.CrossEntropyLoss`\n",
        "\n",
        "You would typically use `torch.nn.CrossEntropyLoss` when you are defining your model's loss during the model's initialization, and you plan to use this loss multiple times during the training. The advantage is that you can set its parameters like `weight`, `size_average`, etc., during initialization and don't need to pass them every time you compute the loss.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Let's create a simple scenario where we have a classification model and we use `torch.nn.CrossEntropyLoss` as the loss function during the training of this model.\n",
        "\n",
        "#### 1. Define a simple model:\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 4)  # A simple linear layer with 10 input features and 4 output features (4 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleModel()\n",
        "```\n",
        "\n",
        "#### 2. Initialize CrossEntropyLoss and Optimizer:\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()  # Initialize the CrossEntropyLoss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Initialize the optimizer\n",
        "```\n",
        "\n",
        "#### 3. Forward Pass, Loss Computation, and Backward Pass:\n",
        "```python\n",
        "# Example input tensor (Batch size: 2, Features: 10)\n",
        "inputs = torch.randn(2, 10)\n",
        "# Corresponding labels (2 classes for the 2 input samples)\n",
        "labels = torch.tensor([1, 2], dtype=torch.long)\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(inputs)\n",
        "\n",
        "# Compute Loss\n",
        "loss = criterion(outputs, labels)\n",
        "\n",
        "# Zero gradients, backward pass, optimizer step\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# Print the Loss\n",
        "print(\"Loss:\", loss.item())\n",
        "# Loss: 1.5462  (Note: This value is illustrative and will vary due to the random initialization of model parameters and inputs)\n",
        "```\n",
        "\n",
        "### Summary:\n",
        "In this example, `torch.nn.CrossEntropyLoss` is used as it allows defining the loss function at the time of model initialization, and then it can be easily used to compute the loss multiple times during the training loop. The computed loss is then used to perform the backward pass and update the model parameters using the optimizer."
      ],
      "metadata": {
        "id": "1NgAcqDfF1wY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `forward` method, for both training and inference\n",
        "\n",
        "It is indeed a common practice in designing neural networks to use the same method, typically the `forward` method, for both training and inference, but with different execution paths depending on the mode (training or inference).\n",
        "\n",
        "### **Training Path:**\n",
        "During training, the `forward` method is used to compute the network's predictions and the loss between the predictions and the ground truth. The gradients are then backpropagated through the network to update the model's parameters. This often involves additional tensors and computations, like targets and loss, which are not needed during inference.\n",
        "\n",
        "### **Inference Path:**\n",
        "During inference, the primary goal is to compute the network's predictions based on the input, and there is no need to compute the loss or backpropagate gradients. Therefore, aspects related to computing the loss and other training-specific computations are usually bypassed, and only the raw output (like logits or activations) is computed and returned.\n",
        "\n",
        "### **Example:**\n",
        "In the example you've provided, when `targets` is `None`, it is likely that the model is in inference mode, and thus, it doesn't compute the loss and doesn't reshape the logits, returning the raw 3D logits. When `targets` are provided, it is likely in training mode, and it computes the loss using the targets and reshapes the logits as needed for loss computation.\n",
        "\n",
        "### **Benefits:**\n",
        "- **Consistency:** It maintains consistency in the computation graph, whether it is training or inference.\n",
        "- **Code Reusability:** It enables the reuse of the same computation procedures for both training and inference, reducing redundancy.\n",
        "- **Ease of Maintenance:** It makes it easier to maintain and understand the code as training and inference share the same flow, diverging only where necessary.\n",
        "\n",
        "### **Switching Between Modes:**\n",
        "Neural network frameworks, like PyTorch, typically provide mechanisms to switch between training and evaluation modes, such as the `.train()` and `.eval()` methods in PyTorch, which set the mode of the model and its submodules. These mechanisms help in managing aspects like dropout and batch normalization, which have different behaviors during training and inference."
      ],
      "metadata": {
        "id": "aT_TWVnFOdxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing and indexing in Pytorch\n",
        "\n",
        "The reason `c` and `d` have different shapes is due to the way slicing works in PyTorch.\n",
        "\n",
        "### For `c`:\n",
        "```python\n",
        "c = b[:, 2:, :]\n",
        "```\n",
        "Here, you are slicing the middle dimension from index `2` to the end. The slicing `2:` results in a sub-tensor of shape `(1, 4)` for each element in the batch dimension. Thus, `c` retains the middle dimension, resulting in a shape of `(2, 1, 4)`.\n",
        "\n",
        "### For `d`:\n",
        "```python\n",
        "d = b[:, -1, :]\n",
        "```\n",
        "Here, you are selecting the last element `(-1)` of the middle dimension, which results in reducing the middle dimension. Hence, `d` does not retain the middle dimension and results in a shape of `(2, 4)`.\n",
        "\n",
        "### Summary:\n",
        "- When you use slicing `:`, it keeps the dimension even if it's of size `1`.\n",
        "- When you use indexing with a specific value, it reduces that dimension.\n",
        "\n",
        "Here’s a bit more visualization to help:\n",
        "\n",
        "```\n",
        "b:\n",
        "[\n",
        " [[ 0,  1,  2,  3],    --> [0]\n",
        "  [ 4,  5,  6,  7],    --> [1]\n",
        "  [ 8,  9, 10, 11]],   --> [2]\n",
        "                        \n",
        " [[12, 13, 14, 15],    --> [0]\n",
        "  [16, 17, 18, 19],    --> [1]\n",
        "  [20, 21, 22, 23]]    --> [2]\n",
        "]\n",
        "\n",
        "c = b[:, 2:, :]\n",
        "c:\n",
        "[\n",
        " [[ 8,  9, 10, 11]],   --> [2]\n",
        " [[20, 21, 22, 23]]    --> [2]\n",
        "]\n",
        "c.shape: (2, 1, 4)\n",
        "\n",
        "d = b[:, -1, :]\n",
        "d:\n",
        "[\n",
        " [ 8,  9, 10, 11],     --> [2]\n",
        " [20, 21, 22, 23]      --> [2]\n",
        "]\n",
        "d.shape: (2, 4)\n",
        "```"
      ],
      "metadata": {
        "id": "b_nw03HQRj7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= torch.tensor(range(24))\n",
        "b = a.view(2,3,4)\n",
        "print(b)\n",
        "c= b[:,2:,:]\n",
        "print(c)\n",
        "print(c.shape)\n",
        "d= b[:,-1,:]\n",
        "print(d)\n",
        "print(d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUjgT8KLlkK",
        "outputId": "77d546fe-5562-4034-bdd8-9caac636aa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "tensor([[[ 8,  9, 10, 11]],\n",
            "\n",
            "        [[20, 21, 22, 23]]])\n",
            "torch.Size([2, 1, 4])\n",
            "tensor([[ 8,  9, 10, 11],\n",
            "        [20, 21, 22, 23]])\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.functional.softmax` dim\n",
        "\n",
        "The `torch.nn.functional.softmax` function can be applied to tensors of any shape, and it's quite common to use it with batches of data, e.g., `(B, C)`, where `B` is the batch size and `C` is the number of classes.\n",
        "\n",
        "### Using `dim=-1`:\n",
        "```python\n",
        "probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "```\n",
        "Here, `dim=-1` implies applying softmax to the last dimension of the tensor, i.e., across the classes for each instance in the batch independently. This is usually the desired behavior when dealing with batches of logits, as the softmax is typically applied to the scores (logits) corresponding to different classes.\n",
        "\n",
        "### Using `dim=1`:\n",
        "```python\n",
        "probs = F.softmax(logits, dim=1)  # (B, C)\n",
        "```\n",
        "Here, `dim=1` is equivalent to `dim=-1` for a 2D tensor with shape `(B, C)`, as it also applies softmax across the classes (the second dimension) for each instance in the batch independently.\n",
        "\n",
        "### Advantages of Specifying Dimension:\n",
        "- **Correctness:** Specifying the dimension is crucial to ensure that the softmax is applied along the correct dimension, especially when working with tensors with more than two dimensions. It ensures the softmax operation is applied independently to each group of scores corresponding to different classes.\n",
        "- **Flexibility:** It provides flexibility, allowing you to apply softmax along any specific dimension of a tensor depending on the use case.\n",
        "- **Clarity:** Explicitly mentioning the dimension adds to the readability and clarity of the code, making it evident to others (and to \"future you\") along which dimension the softmax is intended to be applied.\n",
        "\n",
        "In summary, whether you use `dim=-1` or `dim=1` for a 2D tensor `(B, C)`, it yields the same result, but it is crucial to specify the correct dimension, and it is often good practice to explicitly state the dimension along which the operation is performed for the sake of clarity."
      ],
      "metadata": {
        "id": "31Q-kvQXTub7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining the model\n",
        "\n",
        "To examine your model, you can print the model itself to get an overview of its structure, and you can also calculate and print the number of parameters. Here's how you can do it:\n",
        "\n",
        "### 1. Print the Model\n",
        "Printing the model gives an overview of all layers and components in your model.\n",
        "\n",
        "```python\n",
        "print(m)\n",
        "```\n",
        "\n",
        "### 2. Calculate and Print the Number of Parameters\n",
        "You can calculate the total number of parameters, as well as the number of trainable (requires_grad=True) parameters.\n",
        "\n",
        "```python\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Total Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n",
        "```\n",
        "\n",
        "### Example:\n",
        "\n",
        "```python\n",
        "m = BigramLanguageModel(100)  # vocab_size=100 for example\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in m.state_dict():\n",
        "    print(param_tensor, \"\\t\", m.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print the model structure\n",
        "print(\"\\nModel's Structure: \")\n",
        "print(m)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```\n",
        "Model's state_dict:\n",
        "token_embedding_table.weight    torch.Size([100, 100])\n",
        "\n",
        "Model's Structure:\n",
        "BigramLanguageModel(\n",
        "  (token_embedding_table): Embedding(100, 100)\n",
        ")\n",
        "\n",
        "Total Parameters: 10000\n",
        "Trainable Parameters: 10000\n",
        "```\n",
        "\n",
        "In this example, you would replace `100` with your actual `vocab_size`, and you should see the structure of your model, the size of the embedding weight tensor, and the total number of parameters in your model."
      ],
      "metadata": {
        "id": "vl_ZCGNwaGEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `model.eval()` and `model.train()`\n",
        "\n",
        "The `model.eval()` and `model.train()` methods in PyTorch are used to set the model in evaluation and training modes, respectively, and they are essential for ensuring that specific layers in the model behave appropriately in each mode.\n",
        "\n",
        "### **1. `model.train()`:**\n",
        "   - **Purpose:** Sets the model to training mode.\n",
        "   - **Effect on Layers:**\n",
        "     - **Dropout Layers:** They are active and will zero out a random subset of units.\n",
        "     - **Batch Normalization Layers:** They compute the mean and variance of the current batch and use these for normalization, and they also update the running mean and variance.\n",
        "   - **When to Use:** During training.\n",
        "\n",
        "### **2. `model.eval()`:**\n",
        "   - **Purpose:** Sets the model to evaluation (or inference) mode.\n",
        "   - **Effect on Layers:**\n",
        "     - **Dropout Layers:** They are deactivated, and no units are zeroed out.\n",
        "     - **Batch Normalization Layers:** They use the running mean and variance accumulated during training for normalization.\n",
        "   - **When to Use:** During evaluation, validation, testing, or any other inference task.\n",
        "\n",
        "### **Layers/Components Affected:**\n",
        "1. **Dropout Layers:**\n",
        "   - **Why Needed:** To prevent overfitting during training and to ensure all units are active during inference for a deterministic output.\n",
        "\n",
        "2. **Batch Normalization Layers:**\n",
        "   - **Why Needed:** To use batch statistics during training and running statistics during inference to correctly normalize the input.\n",
        "\n",
        "3. **Any Custom Layers/Components:**\n",
        "   - **Why Needed:** Any custom layers or components that have different behaviors during training and inference will need to be aware of these modes to behave correctly.\n",
        "\n",
        "### **Summary:**\n",
        "The `model.train()` and `model.eval()` methods are crucial for models containing layers like Dropout and Batch Normalization that have differing behaviors during training and inference, ensuring appropriate and correct functionality in each phase."
      ],
      "metadata": {
        "id": "xmrG3gG5iNPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do the positional embedding get moved to the \"device\"?\n",
        "\n",
        "When you use `model.to(device)`, it moves all the model's parameters and buffers to the specified device. This includes all the learnable parameters of your model, such as the weights in the embedding layers, linear layers, etc. This is why you don’t have to specify the device individually for each parameter like token embeddings.\n",
        "\n",
        "However, the positional embeddings line is not creating a learnable parameter; it is creating a new tensor on-the-fly during the forward pass:\n",
        "\n",
        "```python\n",
        "pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "```\n",
        "\n",
        "Here, `torch.arange(T, device=device)` is creating a new tensor representing the position indices, and this tensor needs to be on the same device as the rest of your model and data to avoid errors during the forward pass. That’s why `device=device` is explicitly specified here.\n",
        "\n",
        "To clarify, the `self.position_embedding_table` does get moved to the correct device when you do `model.to(device)`, but the tensor created by `torch.arange(T, device=device)` needs to have its device specified at the point of creation, because it is not a parameter of the model, but a temporary tensor created during the forward pass.\n",
        "\n",
        "The token embeddings do not need the device to be specified at the point of use because they are not creating any new tensors on-the-fly during the forward pass in the manner that the positional embeddings line is. The input indices for the token embeddings are typically already on the correct device by the time they are used in the forward pass."
      ],
      "metadata": {
        "id": "FuCW-WqYQYKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.mean` dimension\n",
        "\n",
        "The error occurs because you are trying to assign a tensor of incorrect shape to `xbow[b, t]`.\n",
        "\n",
        "### Understanding the Error:\n",
        "When you do:\n",
        "```python\n",
        "xbow[b, t] = torch.mean(xprev, 1)\n",
        "```\n",
        "You are attempting to take the mean along the second dimension (1-based index) of `xprev`, which corresponds to dimension `C`. This would result in a tensor of shape `(t+1,)`, since the mean is computed across the `C` dimension. This is not compatible with the shape `(C,)` expected for `xbow[b, t]`, leading to a `RuntimeError`.\n",
        "\n",
        "### Solution:\n",
        "Since `xbow[b, t]` expects a tensor of shape `(C,)`, you should compute the mean across the `T` dimension (0-based index) of `xprev` to get a tensor of the correct shape `(C,)`. Therefore, you should use:\n",
        "```python\n",
        "xbow[b, t] = torch.mean(xprev, 0)\n",
        "```\n",
        "This will correctly compute the mean of all the `t+1` time steps for each feature in `C`, resulting in a tensor of shape `(C,)` which can be correctly assigned to `xbow[b, t]`.\n",
        "\n",
        "### Summary:\n",
        "- Use `torch.mean(xprev, 0)` to compute the mean across the time steps, resulting in a tensor of shape `(C,)`.\n",
        "- `torch.mean(xprev, 1)` attempts to compute the mean across the feature dimension, resulting in a tensor of shape `(T,)` (or `(t+1,)` in this case), which is incompatible with the expected shape `(C,)`.\n",
        "\n",
        "---\n",
        "\n",
        "Let's consider a simpler example to illustrate the error:\n",
        "\n",
        "Suppose we have a tensor `x` of shape `(3, 2)`, representing 3 time steps and 2 features:\n",
        "\n",
        "```python\n",
        "x = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0],\n",
        "                  [5.0, 6.0]])  # shape (3, 2)\n",
        "```\n",
        "\n",
        "Now, if we try to take the mean along the second dimension (features):\n",
        "\n",
        "```python\n",
        "mean_x = torch.mean(x, 1)  # Attempting to take the mean along the second dimension.\n",
        "print(mean_x)  # This will result in a tensor of shape (3,)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "\n",
        "```python\n",
        "tensor([1.5, 3.5, 5.5])  # shape (3,)\n",
        "```\n",
        "\n",
        "Now, if we have a target tensor `target` of shape `(2,)`:\n",
        "\n",
        "```python\n",
        "target = torch.zeros(2)  # shape (2,)\n",
        "```\n",
        "\n",
        "If we try to assign `mean_x` to `target`, it will throw an error similar to the one you experienced, as the shapes are incompatible:\n",
        "\n",
        "```python\n",
        "target = mean_x  # This will throw an error as the shapes are (2,) and (3,) respectively.\n",
        "```\n",
        "\n",
        "Instead, if you want to compute the mean of `x` and store it in a tensor of shape `(2,)`, you should compute the mean along the first dimension (time steps):\n",
        "\n",
        "```python\n",
        "correct_mean_x = torch.mean(x, 0)  # Taking the mean along the first dimension.\n",
        "print(correct_mean_x)  # This will result in a tensor of shape (2,)\n",
        "```\n",
        "\n",
        "This will output:\n",
        "\n",
        "```python\n",
        "tensor([3., 4.])  # shape (2,)\n",
        "```\n",
        "\n",
        "Now, assigning `correct_mean_x` to `target` will not throw an error, as their shapes are compatible:\n",
        "\n",
        "```python\n",
        "target = correct_mean_x  # No error, as the shapes are both (2,).\n",
        "```"
      ],
      "metadata": {
        "id": "mjruiQgT1j2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `keepdim` parameter in functions like `torch.sum`\n",
        "\n",
        "The `keepdim` parameter in functions like `torch.sum` determines whether to retain the summed dimension in the output tensor's shape.\n",
        "\n",
        "### When `keepdim=True`:\n",
        "The summed dimension is retained as a dimension of size 1 in the resulting tensor.\n",
        "\n",
        "### When `keepdim=False` (Default):\n",
        "The summed dimension is removed from the resulting tensor's shape.\n",
        "\n",
        "### Example:\n",
        "Let’s consider a simple 2x3 tensor:\n",
        "```python\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "```\n",
        "\n",
        "#### 1. **Using `keepdim=True`**:\n",
        "```python\n",
        "sum_x_keepdim = torch.sum(x, dim=1, keepdim=True)\n",
        "```\n",
        "This will keep the summed dimension, resulting in a shape of `(2, 1)`:\n",
        "```python\n",
        "# sum_x_keepdim\n",
        "tensor([[ 6],\n",
        "        [15]])\n",
        "```\n",
        "\n",
        "#### 2. **Using `keepdim=False`** (or omitting it, as `False` is the default):\n",
        "```python\n",
        "sum_x_no_keepdim = torch.sum(x, dim=1)\n",
        "```\n",
        "This will remove the summed dimension, resulting in a shape of `(2,)`:\n",
        "```python\n",
        "# sum_x_no_keepdim\n",
        "tensor([ 6, 15])\n",
        "```\n",
        "\n",
        "### Role of `keepdim`:\n",
        "- **Preserving Dimensions:** When performing subsequent operations that rely on the original dimensionality, `keepdim=True` is useful to avoid shape mismatch errors.\n",
        "- **Broadcasting:** Keeping the dimension is crucial when you want to use broadcasting in subsequent operations, where matching dimensions are essential.\n",
        "- **Readability:** It can make the code more readable by making explicit the intention to retain the original number of dimensions."
      ],
      "metadata": {
        "id": "1T_iaZ8u857m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initializing tensors in PyTorch\n",
        "\n",
        "When initializing tensors in PyTorch, it is generally best practice to be explicit about the shape of the tensor. This aids in readability and reduces the likelihood of bugs due to unintended shapes. Both `torch.ones(T, T)` and `torch.ones((T, T))` will yield tensors with the same shape, but the latter is usually preferred as it is more explicit about the intended shape of the tensor.\n",
        "\n",
        "### Best Practices for Initializing Tensors:\n",
        "1. **Be Explicit with Shape:**\n",
        "   ```python\n",
        "   torch.ones((T, T))  # Preferred way, more readable and explicit about the shape.\n",
        "   ```\n",
        "   This is clearer as it explicitly denotes the shape as a single argument (a tuple), making it evident that the resulting tensor is 2-dimensional.\n",
        "\n",
        "2. **Use `dtype` and `device` Arguments:**\n",
        "   When you need a tensor of a specific data type or on a specific device, specify these using the `dtype` and `device` arguments:\n",
        "   ```python\n",
        "   torch.ones((T, T), dtype=torch.float32, device='cuda')\n",
        "   ```\n",
        "   This ensures the tensor is created with the right type and on the right device, avoiding potential type/device mismatches later in the code.\n",
        "\n",
        "3. **Explicitly Set `requires_grad`:**\n",
        "   If the tensor will be used for gradient computation, explicitly set `requires_grad=True`:\n",
        "   ```python\n",
        "   torch.ones((T, T), requires_grad=True)\n",
        "   ```\n",
        "   This makes it clear that this tensor is part of the computation graph for gradient computation.\n",
        "\n",
        "4. **Use `torch.zeros` for Initializing to Zero:**\n",
        "   When you need a tensor initialized with zeros, use `torch.zeros` with explicit shape:\n",
        "   ```python\n",
        "   torch.zeros((T, T))\n",
        "   ```\n",
        "   This is more intuitive and readable compared to creating a tensor with another method and then zeroing it out.\n",
        "\n",
        "5. **Use `torch.randn` for Random Initialization:**\n",
        "   When you need a tensor initialized with values from a standard normal distribution, use `torch.randn` with explicit shape:\n",
        "   ```python\n",
        "   torch.randn((T, T))\n",
        "   ```\n",
        "   This is more clear and concise than creating an empty tensor and then filling it with random values.\n",
        "\n",
        "### Summary:\n",
        "Being explicit about tensor shapes and properties such as data type, device, and whether it requires gradients, can make the code more readable, understandable, and less prone to bugs and unintended behaviors."
      ],
      "metadata": {
        "id": "XLhk2_TeMsem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Model Hub\n",
        "\n",
        "Given that you've trained a model on Google Colab and you want to upload this model to Hugging Face Model Hub, you would typically follow these steps.\n",
        "\n",
        "### 1. Save Model and Tokenizer\n",
        "\n",
        "Before you can upload your model to Hugging Face, you need to save it to disk along with any other files that are required to use it, such as your tokenizer.\n",
        "\n",
        "#### a. Save the Model:\n",
        "```python\n",
        "# Define the path where you want to save your model\n",
        "save_directory = \"/path/to/your/model\"\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), f\"{save_directory}/model_weights.pth\")\n",
        "```\n",
        "\n",
        "#### b. Save the Tokenizer:\n",
        "Since you have a custom tokenizer, you should also save the necessary files (e.g., vocab files) to be able to load it back. Since your tokenizer is quite simple, you might want to save the `stoi` and `itos` mappings.\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Save the stoi and itos mappings\n",
        "with open(f\"{save_directory}/stoi.json\", 'w') as f:\n",
        "    json.dump(stoi, f)\n",
        "\n",
        "with open(f\"{save_directory}/itos.json\", 'w') as f:\n",
        "    json.dump(itos, f)\n",
        "```\n",
        "\n",
        "### 2. Upload to Hugging Face Model Hub\n",
        "\n",
        "#### a. Authenticate to Hugging Face\n",
        "```python\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "```\n",
        "\n",
        "#### b. Create a Model Repository\n",
        "Create a repository on [Hugging Face](https://huggingface.co/new). The name of this repository will be your `model_name` below.\n",
        "\n",
        "#### c. Upload Model and Tokenizer Files\n",
        "\n",
        "```python\n",
        "from huggingface_hub import Repository\n",
        "\n",
        "# Replace with the name of your Hugging Face repository\n",
        "model_name = \"username/repository_name\"\n",
        "model_repo = Repository(local_dir=save_directory, repo_url=f\"{model_name}\")\n",
        "\n",
        "# Commit and push files to Hugging Face Model Hub\n",
        "model_repo.push_to_hub(commit_message='Initial commit')\n",
        "```\n",
        "\n",
        "### 3. Load Model and Tokenizer for Inference\n",
        "\n",
        "Once your model and tokenizer are available on Hugging Face, you can load them for inference as follows:\n",
        "\n",
        "#### a. Load the Model:\n",
        "```python\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "model.load_state_dict(torch.hub.load('huggingface/' + model_name, 'model_weights.pth'))\n",
        "model.eval()\n",
        "```\n",
        "\n",
        "#### b. Load the Tokenizer:\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Load stoi and itos mappings\n",
        "with open(torch.hub.get_dir() + '/stoi.json', 'r') as f:\n",
        "    stoi = json.load(f)\n",
        "\n",
        "with open(torch.hub.get_dir() + '/itos.json', 'r') as f:\n",
        "    itos = json.load(f)\n",
        "\n",
        "# Now you can use stoi and itos for tokenization and detokenization\n",
        "```\n",
        "\n",
        "### Important Note:\n",
        "- You may need to adjust paths and filenames based on where and how you decide to save your model and tokenizer files.\n",
        "- This example assumes that you are working with a PyTorch model and a simple character-based tokenizer. If your model or tokenizer is different, you may need to adjust the saving and loading code accordingly."
      ],
      "metadata": {
        "id": "9SW9JCM0UOeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compatibility with Hugging Face Transformers Ecosystem\n",
        "\n",
        "Repositories and models on the Hugging Face Model Hub can vary in compatibility. Many models are indeed compatible with the Hugging Face Transformers library, as they are uploaded using the library's tools and adhere to the standard architectures (like BERT, GPT-2, etc.). These can be easily loaded using the library's `from_pretrained` methods.\n",
        "\n",
        "However, the Model Hub also allows the hosting of models that may not strictly adhere to the standard architectures or may not be directly compatible with the Transformers library. These might require custom code to load and use.\n",
        "\n",
        "### Uploading to Model Hub vs Google Drive\n",
        "\n",
        "#### 1. **Public Accessibility:**\n",
        "   - **Model Hub:** Offers a platform where models are publicly available to the community, and others can easily discover, download, and use them.\n",
        "   - **Google Drive:** More suited for personal storage, and sharing models can be cumbersome due to access permissions.\n",
        "\n",
        "#### 2. **Ecosystem Integration:**\n",
        "   - **Model Hub:** When models are compatible with the Transformers library, they can be seamlessly integrated into the ecosystem, allowing easy loading and utilization using Hugging Face methods.\n",
        "   - **Google Drive:** Requires manual download and loading, regardless of compatibility.\n",
        "\n",
        "#### 3. **Versioning and Documentation:**\n",
        "   - **Model Hub:** Supports versioning and provides a platform to document the model, its usage, and any relevant details.\n",
        "   - **Google Drive:** Lacks built-in versioning and documentation features for shared models.\n",
        "\n",
        "#### 4. **Community and Collaboration:**\n",
        "   - **Model Hub:** Facilitates community interactions, feedback, and contributions, enhancing collaborative model development.\n",
        "   - **Google Drive:** Is more isolated and does not inherently support community interactions around shared models.\n",
        "\n",
        "### Utilizing Model Hub Efficiently\n",
        "\n",
        "- **Compatible Models:** To fully leverage the benefits of the Model Hub, it is advisable to make models compatible with the Transformers library so that the community can easily use them.\n",
        "- **Custom Models:** For non-standard or custom models, including clear instructions and possibly custom loading code in the repository can assist users in utilizing the models.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "While uploading a model to the Model Hub without compatibility is somewhat akin to storing it on Google Drive, making it compatible with the Hugging Face ecosystem and providing clear documentation can significantly enhance its usability and accessibility within the community."
      ],
      "metadata": {
        "id": "8fGu41KboXUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment variables\n",
        "\n",
        "To avoid including sensitive information such as your Hugging Face username or any other user-specific information directly in your Colab notebook, you can use environment variables or Google Colab's \"Forms\" feature.\n",
        "\n",
        "### 1. Using Environment Variables:\n",
        "\n",
        "You can set environment variables from the notebook cell as follows:\n",
        "\n",
        "```shell\n",
        "import os\n",
        "os.environ['HF_USERNAME'] = 'your_username'\n",
        "```\n",
        "\n",
        "Then, you can access this environment variable whenever you need it:\n",
        "\n",
        "```python\n",
        "username = os.getenv('HF_USERNAME')\n",
        "```\n",
        "\n",
        "### 2. Using Google Colab's Forms:\n",
        "\n",
        "Google Colab provides a feature called \"Forms\" which allows you to create fields in your notebook where you can input values.\n",
        "\n",
        "You can add a text field in your Colab notebook like this:\n",
        "\n",
        "```python\n",
        "#@param {type:\"string\"}\n",
        "username = \"your_username\"  # you can leave this empty\n",
        "```\n",
        "\n",
        "Now, you can edit this field directly to input your username without modifying the code.\n",
        "\n",
        "### 3. Using Google Drive:\n",
        "\n",
        "You could also store your sensitive information in a file on Google Drive and read the file in your Colab notebook. After mounting your Google Drive in Colab, you can read the file as follows:\n",
        "\n",
        "```python\n",
        "with open('/content/drive/MyDrive/your_file.txt', 'r') as f:\n",
        "    username = f.read().strip()\n",
        "```\n",
        "\n",
        "### 4. Input Prompt:\n",
        "\n",
        "Use Python's `input` function to prompt you to enter sensitive information when running the cell:\n",
        "\n",
        "```python\n",
        "username = input(\"Enter your Hugging Face username: \")\n",
        "```\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "- Do not store sensitive information, such as passwords or secret keys, directly in your notebook, even with the above methods.\n",
        "- For secret keys or tokens, consider using a secure vault or secrets manager.\n",
        "- After using any sensitive information in your notebook, make sure to clear the output cells before sharing the notebook with others.\n",
        "\n",
        "Make sure to choose a method that suits your needs and ensures the security of your sensitive information."
      ],
      "metadata": {
        "id": "2Z_yJcj8kwQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `@dataclass`\n",
        "\n",
        "Using a `dataclass` or a class-based configuration is indeed a good practice, especially as projects grow larger and more complex. It offers type checking, auto-generation of `__init__` methods, and other utilities, making it more maintainable and error-resistant compared to a simple dictionary.\n",
        "\n",
        "Here’s a brief explanation for why and when to use a class-based configuration:\n",
        "\n",
        "### 1. **Type Safety:**\n",
        "   - With a class, you can leverage type hints to ensure that each configuration parameter is of the correct type. This can catch potential bugs early.\n",
        "   - In a dictionary, it’s easy to mistakenly assign a value of the wrong type, leading to potential issues.\n",
        "\n",
        "### 2. **Autocompletion and Documentation:**\n",
        "   - Many IDEs offer autocompletion and inline documentation for class attributes but not for dictionary keys.\n",
        "   - This can make the development process smoother and reduce the risk of typos or using non-existent configuration parameters.\n",
        "\n",
        "### 3. **Default Values:**\n",
        "   - With classes, you can easily set default values for your configuration parameters.\n",
        "   - This can reduce the verbosity of your configuration and make it easier to understand which parameters are essential and which are optional.\n",
        "\n",
        "### 4. **Immutability:**\n",
        "   - Classes (especially dataclasses with frozen parameters) can be made immutable, preventing accidental modifications of the configuration during runtime.\n",
        "   - Dictionaries are mutable by default.\n",
        "\n",
        "### Example using `dataclass`:\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int\n",
        "    n_embd: int\n",
        "    n_head: int\n",
        "    n_layer: int\n",
        "    dropout: float\n",
        "    device: str\n",
        "\n",
        "config = Config(block_size=8, n_embd=32, n_head=4, n_layer=4, dropout=0.1, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "```\n",
        "\n",
        "### Why it might not have been used:\n",
        "- **Simplicity and Quick Prototyping:** For smaller, simpler projects or during initial prototyping stages, using a dictionary might be simpler and quicker.\n",
        "- **Familiarity:** The author might be more familiar or comfortable with using dictionaries for configuration.\n",
        "\n",
        "### Conclusion:\n",
        "While using dictionaries is fine for simpler scenarios or quick prototyping, adopting a class-based approach for configurations is generally a good practice for larger and more complex projects, as it provides additional safety, clarity, and development convenience.\n",
        "\n",
        "It would indeed be a beneficial refactor to use a `dataclass` or a similar approach for the configuration in the provided code, especially as it grows and evolves."
      ],
      "metadata": {
        "id": "dTvu6riOeRwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits of External Configuration Files\n",
        "\n",
        "Using `@dataclass` does make it convenient to organize configuration or hyperparameter settings within your code, and it can help in keeping the code cleaner and more structured. However, it doesn't inherently eliminate the need or usefulness of an external `hyperparameters.json` file or another configuration file.\n",
        "\n",
        "### Benefits of External Configuration Files:\n",
        "1. **Easy Updates:** External configuration files allow you to change hyperparameters without modifying the code. This is particularly useful when you want to experiment with different hyperparameters or when different users need different settings.\n",
        "2. **Version Control:** Keeping hyperparameters in an external file can be beneficial for version control. The code remains unchanged, and only the configuration file is modified.\n",
        "3. **Automation and Scaling:** For automated experimentation and hyperparameter tuning, external configuration files are essential as they can be easily generated and modified by scripts.\n",
        "\n",
        "### Example Workflow with `@dataclass` and External Config File:\n",
        "1. **Define a `@dataclass` for Configuration:**\n",
        "   ```python\n",
        "   @dataclass\n",
        "   class ModelConfig:\n",
        "       learning_rate: float\n",
        "       batch_size: int\n",
        "       num_epochs: int\n",
        "   ```\n",
        "   \n",
        "2. **Load Hyperparameters from an External JSON File:**\n",
        "   ```python\n",
        "   import json\n",
        "   from dataclasses import asdict\n",
        "\n",
        "   with open('config.json', 'r') as f:\n",
        "       config_dict = json.load(f)\n",
        "\n",
        "   config = ModelConfig(**config_dict)\n",
        "   ```\n",
        "\n",
        "3. **Use the Config in your Model:**\n",
        "   ```python\n",
        "   model = MyModel(config)\n",
        "   ```\n",
        "\n",
        "This way, you can leverage the benefits of both `@dataclass` for structured and clean code and external configuration files for flexibility and convenience."
      ],
      "metadata": {
        "id": "rMBVCFL2fm28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roadmap to go from a document completer/babbler to a question-answerer\n",
        "\n",
        "Below is a simplified roadmap to go from a document completer/babbler (like GPT) to a question-answerer, based on the transcript you provided:\n",
        "\n",
        "### 1. **Pre-training Stage**\n",
        "   - **Goal:** Train a language model to understand language structure and generate coherent text.\n",
        "   - **Model:** Transformer-based model (e.g., GPT-3).\n",
        "   - **Training Data:** Large chunk of the internet.\n",
        "   - **Task:** Predict the next word/token in a sequence.\n",
        "   - **Outcome:** A model capable of completing documents, babbling coherent and diverse text, but not necessarily useful or context-aware responses.\n",
        "\n",
        "### 2. **Fine-tuning Stage**\n",
        "   - **Goal:** Refine the pre-trained model to respond to questions with coherent, contextually appropriate, and useful answers.\n",
        "   - **Model:** The pre-trained model from stage 1.\n",
        "   - **Training Data:** Curated datasets with a format of a question followed by an answer.\n",
        "   - **Task:** Predict the appropriate answer given a question.\n",
        "   - **Outcome:** A model that is more aligned to being an assistant, expecting to complete questions with coherent answers.\n",
        "\n",
        "### 3. **Reward Modeling and Reinforcement Learning**\n",
        "   - **Goal:** Further refine the model’s responses using human feedback.\n",
        "   - **Model:** The fine-tuned model from stage 2.\n",
        "   - **Training Data:** Model-generated responses ranked by human raters.\n",
        "   - **Task:** Optimize the model’s policy to generate responses that are expected to score high rewards according to human preference.\n",
        "   - **Technique:** Proximal Policy Optimization (PPO).\n",
        "   - **Outcome:** A model that generates responses that are more likely to be preferred by humans, serving as an efficient question-answerer.\n",
        "\n",
        "### How it Works:\n",
        "   - **Fine-tuning Aligns the Model:** The model is fine-tuned on specific Q&A formatted data, aligning its responses to be more like answers to the questions, rather than just completing documents.\n",
        "   - **Reward Model Predicts Desirability:** A separate network (reward model) is trained to predict the desirability of the model's responses based on human rankings.\n",
        "   - **PPO Optimizes Policy:** The PPO algorithm uses the reward model to optimize the model's policy, making it generate responses that are expected to receive higher rewards.\n",
        "\n",
        "### Conclusion:\n",
        "Yes, the reward model is used to fine-tune the initial model. The reward model helps in guiding the reinforcement learning algorithm (PPO) to make the fine-tuned model generate more human-preferred responses, effectively transforming it from a document completer to a question-answerer.\n",
        "\n",
        "### Additional Note:\n",
        "The fine-tuning stage and the subsequent reinforcement learning stage are crucial and involve proprietary data and techniques, making them harder to replicate without access to such resources and knowledge."
      ],
      "metadata": {
        "id": "KFECjESMkgEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF1pp8GL7qZb",
        "outputId": "863212a0-7cac-4655-e99d-6cc8b11f44b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 5, 7, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "list(range(10))[1::2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5SyX9gf7_Vg",
        "outputId": "e3886f6c-6049-42d4-a448-842f1b4fe084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = torch.randn(2,3,4)\n",
        "e[:,:,:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUbEq-wQ8MnZ",
        "outputId": "30fc18a3-a623-4e59-d159-0a8730ed1afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.9672],\n",
              "         [-0.3810],\n",
              "         [ 0.2085]],\n",
              "\n",
              "        [[-1.9406],\n",
              "         [ 2.4469],\n",
              "         [-0.2574]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}