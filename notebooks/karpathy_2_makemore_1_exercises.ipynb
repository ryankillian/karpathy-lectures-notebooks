{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "-wx6xsyeqhVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8BwW38xq7Oz",
        "outputId": "5c0c7e2c-e6d5-4f99-a356-180dd84997a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-19 08:06:25--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-19 08:06:26 (13.3 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "UVesFzYWq_Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "5wGsV6S7rBct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fsku2pEyrJQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an overview of the process for creating a trigram language model based on the bigram single-layer language model we previously discussed.\n",
        "\n",
        "1. **Data Preparation:**\n",
        "    - Instead of bigrams, we will be creating trigrams (i.e., sequences of three characters) from the training data. We will have two input sequences `xs1` and `xs2` representing the first and second characters of each trigram, respectively. The output sequence `ys` will represent the third character of each trigram.\n",
        "\n",
        "2. **Network Architecture:**\n",
        "    - We will have two one-hot encoded input vectors for each trigram. Each vector will have a length of 27 (corresponding to the 26 characters in the alphabet plus the period character).\n",
        "    - We will need a way to combine these two input vectors into a single input to the network. One approach is to concatenate the two vectors, resulting in a single input vector of length 54 (27+27) for each trigram.\n",
        "    - The neural network will still be a single linear layer, but now it will have 54 input nodes (one for each element of the concatenated input vector) and 27 output nodes (one for each character).\n",
        "\n",
        "3. **Forward Pass:**\n",
        "    - We will compute the dot product between the concatenated input vector and the weight matrix `W`, which will now have dimensions 54x27.\n",
        "    - We will compute the probabilities for the next character using the softmax function, as we did in the bigram model.\n",
        "\n",
        "4. **Loss Function:**\n",
        "    - The loss function will still be the negative log likelihood, which measures how well the predicted probabilities match the actual next character in each trigram.\n",
        "\n",
        "5. **Backward Pass and Optimization:**\n",
        "    - We will use gradient descent to optimize the weight matrix `W` based on the loss function.\n",
        "    - We will compute the gradients of the loss function with respect to the weight matrix and update the weights accordingly.\n",
        "\n",
        "6. **Regularization:**\n",
        "    - As in the bigram model, we may use regularization to prevent overfitting. This is done by adding a term to the loss function that encourages the weights to stay close to zero.\n",
        "\n",
        "The main difference between the trigram and bigram models lies in the data preparation and network architecture. In the trigram model, we need to handle two input characters (and hence two input vectors) for each example, while in the bigram model, we only had one input character (and one input vector) per example.\n",
        "\n",
        "In terms of the shape of the tensors at each step:\n",
        "- `xs1` and `xs2` will both have shape `(num_samples,)`.\n",
        "- The one-hot encoded and concatenated input vectors will have shape `(num_samples, 54)`.\n",
        "- The weight matrix `W` will have shape `(54, 27)`.\n",
        "- The output logits and probabilities will have shape `(num_samples, 27)`.\n",
        "- The output labels `ys` will have shape `(num_samples,)`.\n",
        "\n",
        "The process for training and evaluating the trigram model will be similar to the bigram model, but we will be working with trigrams and a slightly different network architecture."
      ],
      "metadata": {
        "id": "SZunUSW0vBLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvmfIpHqOgi",
        "outputId": "03c457f9-edf4-411f-cbea-b34458d2a68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". m\n",
            "e m\n",
            "m a\n",
            "m .\n",
            "number of examples:  4\n"
          ]
        }
      ],
      "source": [
        "# bigrams\n",
        "xs, ys = [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    print(ch1, ch2)\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVCheiUptKep",
        "outputId": "bf10c527-2f2b-4820-d5f8-d96ce05a31d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". e m\n",
            "e m m\n",
            "m m a\n",
            "m a .\n",
            "number of examples:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdAor2rhrQBl",
        "outputId": "31c203aa-fe26-4fd8-a3e0-321e591b1fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)"
      ],
      "metadata": {
        "id": "BnToT1j1q0cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "wY37rbaZvloq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2])\n",
        "b = torch.tensor([3,4])\n",
        "c = torch.cat((a,b))\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I6oZGJAw6le",
        "outputId": "5fcda92f-b14b-427c-aa72-5df5b0895e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # input to the network: one-hot encoding\n"
      ],
      "metadata": {
        "id": "4kRxXMG_zAnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj9pR1OKzD43",
        "outputId": "a22c0075-17a2-4e62-b2eb-4af6946eb89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc2 = F.one_hot(xs2, num_classes=27).float()"
      ],
      "metadata": {
        "id": "mkvDCOpIzI6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzC181u8zNK9",
        "outputId": "2124766b-4efe-4645-90c8-847d61764b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = torch.cat((xenc1, xenc2),1)"
      ],
      "metadata": {
        "id": "rx8_M1skzR_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYM5lEWSzVW6",
        "outputId": "41016fd7-bf80-49c8-9eb7-20e648f9778e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJccpy6R1Tah",
        "outputId": "581fb2ee-edf3-463e-bfbf-6f3d35bdf9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  196113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# gradient descent\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(xs2, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "    print(loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBdWBDBDvOMn",
        "outputId": "75282d02-4976-428e-ebf2-781540c9f177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5801351070404053\n",
            "2.5415830612182617\n",
            "2.5098514556884766\n",
            "2.4840075969696045\n",
            "2.463128089904785\n",
            "2.4460954666137695\n",
            "2.431868076324463\n",
            "2.4196877479553223\n",
            "2.40906023979187\n",
            "2.399660110473633\n",
            "2.3912625312805176\n",
            "2.3837015628814697\n",
            "2.3768529891967773\n",
            "2.3706166744232178\n",
            "2.364915370941162\n",
            "2.3596839904785156\n",
            "2.354867935180664\n",
            "2.350421667098999\n",
            "2.3463058471679688\n",
            "2.342486619949341\n",
            "2.3389341831207275\n",
            "2.335622549057007\n",
            "2.3325283527374268\n",
            "2.329632520675659\n",
            "2.326916217803955\n",
            "2.324364423751831\n",
            "2.321962833404541\n",
            "2.3196990489959717\n",
            "2.317561626434326\n",
            "2.3155412673950195\n",
            "2.3136277198791504\n",
            "2.311814308166504\n",
            "2.3100931644439697\n",
            "2.3084568977355957\n",
            "2.3069007396698\n",
            "2.3054189682006836\n",
            "2.3040056228637695\n",
            "2.30265736579895\n",
            "2.3013689517974854\n",
            "2.300137758255005\n",
            "2.298959493637085\n",
            "2.297830820083618\n",
            "2.2967493534088135\n",
            "2.2957115173339844\n",
            "2.294715404510498\n",
            "2.2937583923339844\n",
            "2.2928383350372314\n",
            "2.2919528484344482\n",
            "2.2911007404327393\n",
            "2.2902798652648926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Yes, it is expected that the trigram model would have a lower loss than the bigram model. This is because the trigram model takes into account more context (two previous characters) when predicting the next character, whereas the bigram model only considers one previous character. This additional context allows the trigram model to make more informed predictions, leading to a lower loss.\n",
        "\n",
        "A seasoned ML practitioner would likely say that the improvement in loss is due to the increased context and information provided to the trigram model. They would also note that this pattern of improved performance with increased context is a general trend in language modeling. As models consider more context (n-grams, sentences, paragraphs, etc.), they typically become better at predicting the next word or character. However, there is a trade-off, as models with more context usually require more computational resources and may suffer from the curse of dimensionality.\n",
        "\n",
        "In this specific case, the difference in loss is not huge, but it does indicate that the trigram model is better at predicting the next character than the bigram model. The seasoned practitioner might also consider other factors, such as the size of the dataset, the quality of the data, and the complexity of the model, when interpreting the difference in loss."
      ],
      "metadata": {
        "id": "ZhrMihtW2_CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "Pg-7sQPa2rIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A dev set is another name for a validation set.\n",
        "\n",
        "A dev set is another name for a validation set. In machine learning, it's common to split the dataset into three parts: a training set, a validation (or dev) set, and a test set. The training set is used to train the model, the validation set is used to tune the model's hyperparameters and prevent overfitting, and the test set is used to evaluate the model's performance on unseen data.\n"
      ],
      "metadata": {
        "id": "I0oX4h8S5x7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Validation (or Development), and Test sets\n",
        "\n",
        "When training a machine learning model, it's common to divide the data into three separate sets: training, validation (or development), and test sets. Each of these sets plays a distinct role in the model development process:\n",
        "\n",
        "1. **Training set:** This set is used to train the model. The model learns to make predictions by adjusting its parameters based on the examples in this set.\n",
        "2. **Validation (dev) set:** This set is used to tune the model's hyperparameters and make decisions about the model architecture. By evaluating the model on the validation set, you can see how well the model is generalizing to new, unseen data. However, you may make changes to the model or its training process based on the performance on this set, which can lead to overfitting to the validation set.\n",
        "3. **Test set:** This set is used to assess the final performance of the model. It provides an unbiased estimate of how well the model is likely to perform on new, unseen data. You should only evaluate the model on the test set once, after all model development and tuning have been completed.\n",
        "\n",
        "The distinction between the validation and test sets is crucial to avoid overfitting and ensure that you have an accurate measure of the model's performance on new data. If you use the test set to make decisions about the model during the development process, you risk overfitting to the test set, and your performance estimates will be overly optimistic.\n",
        "\n",
        "In summary, you use the validation set to make decisions about the model during development, while you use the test set to assess the final performance of the model."
      ],
      "metadata": {
        "id": "mKZ0sX9vAipa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation set\n",
        "The need for a validation set arises from the desire to create a machine learning model that generalizes well to new, unseen data. When we train a model, we want to make sure it's not just memorizing the training data but learning patterns that can be applied to data it hasn't seen before. This is where the validation set comes in.\n",
        "\n",
        "The validation set serves as a proxy for the test set, allowing us to simulate how the model would perform on new data without using the test set. The test set should be reserved for the final evaluation of the model and not used during the model development process.\n",
        "\n",
        "Here's what a seasoned ML practitioner might say about the need for a validation set:\n",
        "\n",
        "1. **Overfitting Detection**: The main role of the validation set is to detect overfitting. Overfitting occurs when a model becomes too complex and starts fitting the training data very closely, including the noise. As a result, it may perform poorly on new data. By comparing the training loss and validation loss, we can detect overfitting. If the training loss continues to decrease while the validation loss starts increasing, the model is likely overfitting.\n",
        "\n",
        "2. **Hyperparameter Tuning**: The validation set is used to tune hyperparameters, which are parameters that are not learned during training but set beforehand. Examples include learning rate, regularization strength, and model architecture. By evaluating the model on the validation set, we can choose the best hyperparameters that lead to the best performance.\n",
        "\n",
        "3. **Model Selection**: When comparing different models or model architectures, the validation set can be used to select the best model. The model that performs best on the validation set is likely to perform well on the test set.\n",
        "\n",
        "4. **Early Stopping**: The validation set can be used to implement early stopping, a regularization technique where training is stopped once the validation loss stops improving or starts increasing. This helps prevent overfitting and can save computational resources.\n",
        "\n",
        "5. **Adaptation to New Data**: In some cases, the validation set can be used to adapt the model to new data distributions. If the model's performance on the validation set degrades over time, it may indicate a shift in the data distribution, and the model can be retrained or adapted to the new data.\n",
        "\n",
        "In summary, the validation set plays a crucial role in building robust machine learning models that generalize well to new data. It helps detect overfitting, tune hyperparameters, select models, implement early stopping, and adapt to new data distributions."
      ],
      "metadata": {
        "id": "-XSZ4BMSDhqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram - create the dataset\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        ix3 = stoi[ch3]\n",
        "        xs1.append(ix1)\n",
        "        xs2.append(ix2)\n",
        "        ys.append(ix3)\n",
        "\n"
      ],
      "metadata": {
        "id": "vF-sIkG03EYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah0Mmfmm8A0X",
        "outputId": "be9e6c27-0765-456b-f885-31b14d04d47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196113"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.arange(10).reshape((5, 2))\n",
        "y = range(5)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5fYAYYk-Ms7",
        "outputId": "4100a293-ea7e-4bed-ee92-c574456f67a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4, 5],\n",
              "       [0, 1],\n",
              "       [6, 7]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.arange(10).reshape((5, 2))\n",
        "X2 = np.arange(10).reshape((5, 2))\n",
        "y = range(5)\n",
        "\n",
        "X_train, X_test, X2_train, X2_test, y_train, y_test = train_test_split(X,X2, y, test_size=0.33, random_state=42)\n",
        "X2_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYCbrabQ-1bq",
        "outputId": "ebc5aac0-5698-402b-b5d4-ade53f6996fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4, 5],\n",
              "       [0, 1],\n",
              "       [6, 7]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)"
      ],
      "metadata": {
        "id": "4CHdGGyFGOL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to numpy arrays\n",
        "xs1 = np.array(xs1)\n",
        "xs2 = np.array(xs2)\n",
        "ys = np.array(ys)\n",
        "\n",
        "# Split the dataset into 80% train, 10% validation, and 10% test\n",
        "X1_train, X1_temp, X2_train, X2_temp, y_train, y_temp = train_test_split(xs1, xs2, ys, test_size=0.2, random_state=42)\n",
        "X1_val, X1_test, X2_val, X2_test, y_val, y_test = train_test_split(X1_temp, X2_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X1_train = torch.tensor(X1_train)\n",
        "X2_train = torch.tensor(X2_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "X1_val = torch.tensor(X1_val)\n",
        "X2_val = torch.tensor(X2_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "X1_test = torch.tensor(X1_test)\n",
        "X2_test = torch.tensor(X2_test)\n",
        "y_test = torch.tensor(y_test)\n"
      ],
      "metadata": {
        "id": "oWnGSuCi6VEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits_val = xenc_val @ W # predict log-counts\n",
        "    counts_val = logits_val.exp() # counts, equivalent to N\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ve6_3BA_k7R",
        "outputId": "3b8cf4c5-2ce1-40f7-a1fc-be95e92036b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 4.195179462432861\n",
            "Epoch 1, Validation Loss: 4.2056193351745605\n",
            "Epoch 2, Training Loss: 3.3654816150665283\n",
            "Epoch 2, Validation Loss: 3.377584218978882\n",
            "Epoch 3, Training Loss: 3.0503995418548584\n",
            "Epoch 3, Validation Loss: 3.060450792312622\n",
            "Epoch 4, Training Loss: 2.8794777393341064\n",
            "Epoch 4, Validation Loss: 2.8844852447509766\n",
            "Epoch 5, Training Loss: 2.7748968601226807\n",
            "Epoch 5, Validation Loss: 2.778290271759033\n",
            "Epoch 6, Training Loss: 2.702197313308716\n",
            "Epoch 6, Validation Loss: 2.7031898498535156\n",
            "Epoch 7, Training Loss: 2.6465039253234863\n",
            "Epoch 7, Validation Loss: 2.646820306777954\n",
            "Epoch 8, Training Loss: 2.6023285388946533\n",
            "Epoch 8, Validation Loss: 2.6012966632843018\n",
            "Epoch 9, Training Loss: 2.566324234008789\n",
            "Epoch 9, Validation Loss: 2.564929485321045\n",
            "Epoch 10, Training Loss: 2.5365397930145264\n",
            "Epoch 10, Validation Loss: 2.5342533588409424\n",
            "Epoch 11, Training Loss: 2.511561870574951\n",
            "Epoch 11, Validation Loss: 2.509014368057251\n",
            "Epoch 12, Training Loss: 2.4904139041900635\n",
            "Epoch 12, Validation Loss: 2.4872119426727295\n",
            "Epoch 13, Training Loss: 2.4723143577575684\n",
            "Epoch 13, Validation Loss: 2.4689009189605713\n",
            "Epoch 14, Training Loss: 2.456676483154297\n",
            "Epoch 14, Validation Loss: 2.4527626037597656\n",
            "Epoch 15, Training Loss: 2.443023920059204\n",
            "Epoch 15, Validation Loss: 2.4389331340789795\n",
            "Epoch 16, Training Loss: 2.4309959411621094\n",
            "Epoch 16, Validation Loss: 2.426513433456421\n",
            "Epoch 17, Training Loss: 2.4203057289123535\n",
            "Epoch 17, Validation Loss: 2.4156734943389893\n",
            "Epoch 18, Training Loss: 2.410736083984375\n",
            "Epoch 18, Validation Loss: 2.4057939052581787\n",
            "Epoch 19, Training Loss: 2.402113437652588\n",
            "Epoch 19, Validation Loss: 2.3970441818237305\n",
            "Epoch 20, Training Loss: 2.394301652908325\n",
            "Epoch 20, Validation Loss: 2.388986349105835\n",
            "Epoch 21, Training Loss: 2.3871910572052\n",
            "Epoch 21, Validation Loss: 2.381770372390747\n",
            "Epoch 22, Training Loss: 2.38069224357605\n",
            "Epoch 22, Validation Loss: 2.375074863433838\n",
            "Epoch 23, Training Loss: 2.3747291564941406\n",
            "Epoch 23, Validation Loss: 2.3690242767333984\n",
            "Epoch 24, Training Loss: 2.3692402839660645\n",
            "Epoch 24, Validation Loss: 2.3633792400360107\n",
            "Epoch 25, Training Loss: 2.3641719818115234\n",
            "Epoch 25, Validation Loss: 2.358238458633423\n",
            "Epoch 26, Training Loss: 2.359478235244751\n",
            "Epoch 26, Validation Loss: 2.353419542312622\n",
            "Epoch 27, Training Loss: 2.3551197052001953\n",
            "Epoch 27, Validation Loss: 2.3490023612976074\n",
            "Epoch 28, Training Loss: 2.3510634899139404\n",
            "Epoch 28, Validation Loss: 2.344846248626709\n",
            "Epoch 29, Training Loss: 2.3472795486450195\n",
            "Epoch 29, Validation Loss: 2.3410141468048096\n",
            "Epoch 30, Training Loss: 2.3437421321868896\n",
            "Epoch 30, Validation Loss: 2.3373963832855225\n",
            "Epoch 31, Training Loss: 2.3404290676116943\n",
            "Epoch 31, Validation Loss: 2.3340439796447754\n",
            "Epoch 32, Training Loss: 2.33732008934021\n",
            "Epoch 32, Validation Loss: 2.330871105194092\n",
            "Epoch 33, Training Loss: 2.3343982696533203\n",
            "Epoch 33, Validation Loss: 2.3279173374176025\n",
            "Epoch 34, Training Loss: 2.3316473960876465\n",
            "Epoch 34, Validation Loss: 2.325115203857422\n",
            "Epoch 35, Training Loss: 2.3290534019470215\n",
            "Epoch 35, Validation Loss: 2.322495698928833\n",
            "Epoch 36, Training Loss: 2.3266043663024902\n",
            "Epoch 36, Validation Loss: 2.3200061321258545\n",
            "Epoch 37, Training Loss: 2.3242886066436768\n",
            "Epoch 37, Validation Loss: 2.3176703453063965\n",
            "Epoch 38, Training Loss: 2.322096824645996\n",
            "Epoch 38, Validation Loss: 2.315446376800537\n",
            "Epoch 39, Training Loss: 2.3200185298919678\n",
            "Epoch 39, Validation Loss: 2.3133533000946045\n",
            "Epoch 40, Training Loss: 2.318047285079956\n",
            "Epoch 40, Validation Loss: 2.311357021331787\n",
            "Epoch 41, Training Loss: 2.316174030303955\n",
            "Epoch 41, Validation Loss: 2.3094725608825684\n",
            "Epoch 42, Training Loss: 2.314392566680908\n",
            "Epoch 42, Validation Loss: 2.3076725006103516\n",
            "Epoch 43, Training Loss: 2.3126964569091797\n",
            "Epoch 43, Validation Loss: 2.3059682846069336\n",
            "Epoch 44, Training Loss: 2.311079740524292\n",
            "Epoch 44, Validation Loss: 2.3043384552001953\n",
            "Epoch 45, Training Loss: 2.309537649154663\n",
            "Epoch 45, Validation Loss: 2.3027920722961426\n",
            "Epoch 46, Training Loss: 2.308065176010132\n",
            "Epoch 46, Validation Loss: 2.3013107776641846\n",
            "Epoch 47, Training Loss: 2.3066580295562744\n",
            "Epoch 47, Validation Loss: 2.2999022006988525\n",
            "Epoch 48, Training Loss: 2.305312156677246\n",
            "Epoch 48, Validation Loss: 2.298550844192505\n",
            "Epoch 49, Training Loss: 2.304023265838623\n",
            "Epoch 49, Validation Loss: 2.2972629070281982\n",
            "Epoch 50, Training Loss: 2.302788496017456\n",
            "Epoch 50, Validation Loss: 2.2960259914398193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting these results\n",
        "\n",
        "The results show the training and validation loss over 50 epochs of training. A seasoned ML practitioner would interpret these results as follows:\n",
        "\n",
        "1. **Loss Decreasing**: Both the training and validation losses are decreasing over time, indicating that the model is learning from the data and improving its performance. This is a good sign.\n",
        "\n",
        "2. **No Overfitting**: The validation loss follows the training loss closely, which means the model is generalizing well to the validation set. If the validation loss started to increase while the training loss continued to decrease, it would be a sign of overfitting. In this case, there is no sign of overfitting.\n",
        "\n",
        "3. **Convergence**: The loss values seem to be converging, meaning that they are reaching a stable value and not changing significantly with more training. This suggests that the model has learned most of what it can from the data and further training might not bring significant improvements.\n",
        "\n",
        "4. **Hyperparameter Tuning**: Since there's no sign of overfitting, there might not be an immediate need for hyperparameter tuning. However, it could still be beneficial to experiment with different hyperparameters (e.g., learning rate, regularization strength, etc.) to see if it's possible to achieve even better performance.\n",
        "\n",
        "5. **Next Steps**: Since the model seems to be performing well, a natural next step would be to evaluate it on the test set to get an estimate of its performance on unseen data. Additionally, more advanced models or features could be explored to see if they can further improve performance.\n",
        "\n",
        "Overall, these results indicate that the model has been successfully trained, is generalizing well to the validation data, and is likely ready for evaluation on the test set. The absence of overfitting and the convergence of the loss values are positive signs.\n",
        "\n",
        "Regarding the difference in loss between the training and validation sets, it's normal to observe slight differences because the two sets are made up of different data samples. The important thing is that the validation loss follows the training loss closely, which indicates good generalization. If the model were overfitting to the training data, you would observe a significant divergence between the two loss curves."
      ],
      "metadata": {
        "id": "sB367EIYG89s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Test set\n",
        "\n",
        "Unlike the training phase, where we perform multiple iterations (epochs) to update the model's parameters, during evaluation we only need to pass the test data through the model once to compute the loss or any other performance metrics. There is no need for multiple iterations or parameter updates during evaluation.\n",
        "\n",
        "The idea is to assess how well the model, which was trained on the training data, generalizes to unseen data. We use the test set, which the model has never seen before, to get an unbiased estimate of the model's performance. This allows us to make informed decisions about the model's quality and suitability for deployment or further refinement."
      ],
      "metadata": {
        "id": "gtGegGmiJDWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model on the Test set\n",
        "To evaluate the model on the test set, follow these steps:\n",
        "\n",
        "1. **Forward Pass**: Pass the test data through the model (perform a forward pass) to get the predicted outputs. Make sure not to include the gradients computation or the backward pass, as you are not training the model at this stage. You can use the `torch.no_grad()` context manager to ensure that no gradients are computed.\n",
        "\n",
        "2. **Compute the Loss**: Compute the loss on the test set using the predicted outputs and the actual targets. This will give you an idea of how well the model performs on unseen data.\n",
        "\n",
        "3. **Compute Performance Metrics**: Depending on the task, compute relevant performance metrics on the test set. For example, in a classification task, you may compute accuracy, precision, recall, F1-score, etc. For a regression task, you may compute the mean squared error, mean absolute error, R-squared, etc.\n",
        "\n",
        "4. **Compare with Validation Performance**: Compare the performance on the test set with that on the validation set. This will help you assess the model's generalization ability.\n",
        "\n",
        "5. **Interpret the Results**: Analyze the results, including any discrepancies between training, validation, and test performances. Consider whether the model is overfitting, underfitting, or generalizing well.\n",
        "\n",
        "6. **Report the Results**: Document the test results, including the loss and performance metrics, and any insights you have gained from the evaluation.\n",
        "\n",
        "7. **Model Analysis**: Optionally, you can further analyze the model's predictions. For example, you can visualize the model's predictions vs. actual values, analyze errors made by the model, or identify patterns in the data where the model performs well or poorly.\n",
        "\n",
        "Remember that once you evaluate the model on the test set, you should not tune the model further based on the test results. The test set is meant to provide an unbiased estimate of the model's performance on unseen data. If you tune the model based on test results, you risk overfitting to the test set and losing the ability to generalize to new, unseen data."
      ],
      "metadata": {
        "id": "eDNtaecfHiZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have already split the data into train, dev, and test sets\n",
        "# The trained model is stored in the variable W\n",
        "\n",
        "test_num = X1_test.nelement()\n",
        "\n",
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc1_test), dim=1)\n",
        "    logits_test = xenc_test @ W # predict log-counts on test data\n",
        "    counts_test = logits_test.exp() # counts, equivalent to N\n",
        "    probs_test = counts_test / counts_test.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_test = -probs_test[torch.arange(test_num), y_test].log().mean() + 0.01*(W**2).mean() # compute the loss on test data\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Kjsa5jJSns",
        "outputId": "f760db9d-6050-4c7c-cefd-47346c1693a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 3.149600028991699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting these results\n",
        "\n",
        "A seasoned ML practitioner would interpret these results as follows:\n",
        "\n",
        "1. **Training Loss**: The training loss has reduced over the course of 50 epochs, indicating that the model has been learning and improving its performance on the training dataset. However, the training loss could be further reduced by increasing the number of epochs, changing the optimization algorithm, or tuning the hyperparameters.\n",
        "\n",
        "2. **Validation Loss**: The validation loss, which is quite close to the training loss, indicates that the model is generalizing well to new data from the same distribution as the training set. This is a good sign, as it suggests that the model is not overfitting.\n",
        "\n",
        "3. **Test Loss**: The test loss is higher than both the training and validation losses, indicating that the model may not be performing as well on the test set as it did on the training and validation sets. This could be due to a variety of factors, such as differences in the distribution of the test set compared to the training and validation sets, or due to the model not being complex enough to capture the underlying patterns in the test set.\n",
        "\n",
        "Based on these observations, the ML practitioner could consider the following steps:\n",
        "\n",
        "1. **Evaluate Other Metrics**: In addition to the loss, it is important to evaluate the model using other metrics that are relevant to the task at hand, such as accuracy, precision, recall, or F1 score. This will provide a more comprehensive view of the model's performance.\n",
        "\n",
        "2. **Investigate the Test Set**: It is important to analyze the test set to understand why the model's performance may be lower on it compared to the training and validation sets. This could involve checking for class imbalance, outliers, or differences in the distribution of features.\n",
        "\n",
        "3. **Model Complexity**: The practitioner could experiment with a more complex model architecture, such as adding more layers or neurons, to see if it helps improve performance on the test set.\n",
        "\n",
        "4. **Hyperparameter Tuning**: The practitioner could conduct a hyperparameter search to find the optimal set of hyperparameters that yield the best performance on the validation set. This could involve tuning the learning rate, regularization strength, batch size, and other hyperparameters.\n",
        "\n",
        "5. **Regularization and Augmentation**: If overfitting is suspected, the practitioner could apply regularization techniques (e.g., dropout or L1/L2 regularization) or use data augmentation to increase the diversity of the training data.\n",
        "\n",
        "6. **Early Stopping**: If the validation loss starts to increase while the training loss continues to decrease, the practitioner could implement early stopping to prevent overfitting.\n",
        "\n",
        "It is essential to iteratively experiment with different approaches and carefully evaluate the results to improve the model's performance on the test set."
      ],
      "metadata": {
        "id": "RzX38hZmLfnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network' with different seed\n",
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)"
      ],
      "metadata": {
        "id": "TJWpCU_ELp3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to numpy arrays\n",
        "xs1 = np.array(xs1)\n",
        "xs2 = np.array(xs2)\n",
        "ys = np.array(ys)\n",
        "\n",
        "# Split the dataset into 80% train, 10% validation, and 10% test\n",
        "X1_train, X1_temp, X2_train, X2_temp, y_train, y_temp = train_test_split(xs1, xs2, ys, test_size=0.2, random_state=42)\n",
        "X1_val, X1_test, X2_val, X2_test, y_val, y_test = train_test_split(X1_temp, X2_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X1_train = torch.tensor(X1_train)\n",
        "X2_train = torch.tensor(X2_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "X1_val = torch.tensor(X1_val)\n",
        "X2_val = torch.tensor(X2_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "X1_test = torch.tensor(X1_test)\n",
        "X2_test = torch.tensor(X2_test)\n",
        "y_test = torch.tensor(y_test)"
      ],
      "metadata": {
        "id": "x8VDRMhELvHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits_val = xenc_val @ W # predict log-counts\n",
        "    counts_val = logits_val.exp() # counts, equivalent to N\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + 0.01*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I05p8o80L0R1",
        "outputId": "559b0823-902a-405a-8218-698fe528ce62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 4.031497001647949\n",
            "Epoch 1, Validation Loss: 4.0244526863098145\n",
            "Epoch 2, Training Loss: 3.349173069000244\n",
            "Epoch 2, Validation Loss: 3.3432114124298096\n",
            "Epoch 3, Training Loss: 3.0437161922454834\n",
            "Epoch 3, Validation Loss: 3.0348005294799805\n",
            "Epoch 4, Training Loss: 2.8713440895080566\n",
            "Epoch 4, Validation Loss: 2.860811948776245\n",
            "Epoch 5, Training Loss: 2.7608463764190674\n",
            "Epoch 5, Validation Loss: 2.7489452362060547\n",
            "Epoch 6, Training Loss: 2.6846230030059814\n",
            "Epoch 6, Validation Loss: 2.6728975772857666\n",
            "Epoch 7, Training Loss: 2.6293413639068604\n",
            "Epoch 7, Validation Loss: 2.617408037185669\n",
            "Epoch 8, Training Loss: 2.587205171585083\n",
            "Epoch 8, Validation Loss: 2.575857639312744\n",
            "Epoch 9, Training Loss: 2.553819179534912\n",
            "Epoch 9, Validation Loss: 2.542658567428589\n",
            "Epoch 10, Training Loss: 2.5265302658081055\n",
            "Epoch 10, Validation Loss: 2.5159969329833984\n",
            "Epoch 11, Training Loss: 2.5037059783935547\n",
            "Epoch 11, Validation Loss: 2.49346923828125\n",
            "Epoch 12, Training Loss: 2.4842801094055176\n",
            "Epoch 12, Validation Loss: 2.4746131896972656\n",
            "Epoch 13, Training Loss: 2.467534065246582\n",
            "Epoch 13, Validation Loss: 2.4581823348999023\n",
            "Epoch 14, Training Loss: 2.4529480934143066\n",
            "Epoch 14, Validation Loss: 2.4440901279449463\n",
            "Epoch 15, Training Loss: 2.4401352405548096\n",
            "Epoch 15, Validation Loss: 2.4315719604492188\n",
            "Epoch 16, Training Loss: 2.4287936687469482\n",
            "Epoch 16, Validation Loss: 2.420644760131836\n",
            "Epoch 17, Training Loss: 2.4186863899230957\n",
            "Epoch 17, Validation Loss: 2.4107933044433594\n",
            "Epoch 18, Training Loss: 2.409621238708496\n",
            "Epoch 18, Validation Loss: 2.4020652770996094\n",
            "Epoch 19, Training Loss: 2.401442289352417\n",
            "Epoch 19, Validation Loss: 2.3940982818603516\n",
            "Epoch 20, Training Loss: 2.3940212726593018\n",
            "Epoch 20, Validation Loss: 2.3869471549987793\n",
            "Epoch 21, Training Loss: 2.3872547149658203\n",
            "Epoch 21, Validation Loss: 2.3803515434265137\n",
            "Epoch 22, Training Loss: 2.381054639816284\n",
            "Epoch 22, Validation Loss: 2.3743655681610107\n",
            "Epoch 23, Training Loss: 2.3753504753112793\n",
            "Epoch 23, Validation Loss: 2.36879825592041\n",
            "Epoch 24, Training Loss: 2.370082378387451\n",
            "Epoch 24, Validation Loss: 2.3636996746063232\n",
            "Epoch 25, Training Loss: 2.3652002811431885\n",
            "Epoch 25, Validation Loss: 2.3589274883270264\n",
            "Epoch 26, Training Loss: 2.3606619834899902\n",
            "Epoch 26, Validation Loss: 2.3545243740081787\n",
            "Epoch 27, Training Loss: 2.3564326763153076\n",
            "Epoch 27, Validation Loss: 2.350383996963501\n",
            "Epoch 28, Training Loss: 2.352480888366699\n",
            "Epoch 28, Validation Loss: 2.346540927886963\n",
            "Epoch 29, Training Loss: 2.348780870437622\n",
            "Epoch 29, Validation Loss: 2.3429129123687744\n",
            "Epoch 30, Training Loss: 2.345308780670166\n",
            "Epoch 30, Validation Loss: 2.339529037475586\n",
            "Epoch 31, Training Loss: 2.342045783996582\n",
            "Epoch 31, Validation Loss: 2.3363254070281982\n",
            "Epoch 32, Training Loss: 2.338973045349121\n",
            "Epoch 32, Validation Loss: 2.3333241939544678\n",
            "Epoch 33, Training Loss: 2.3360753059387207\n",
            "Epoch 33, Validation Loss: 2.3304755687713623\n",
            "Epoch 34, Training Loss: 2.3333380222320557\n",
            "Epoch 34, Validation Loss: 2.3277976512908936\n",
            "Epoch 35, Training Loss: 2.330749750137329\n",
            "Epoch 35, Validation Loss: 2.3252501487731934\n",
            "Epoch 36, Training Loss: 2.328298330307007\n",
            "Epoch 36, Validation Loss: 2.3228471279144287\n",
            "Epoch 37, Training Loss: 2.325974225997925\n",
            "Epoch 37, Validation Loss: 2.3205575942993164\n",
            "Epoch 38, Training Loss: 2.3237674236297607\n",
            "Epoch 38, Validation Loss: 2.3183913230895996\n",
            "Epoch 39, Training Loss: 2.3216705322265625\n",
            "Epoch 39, Validation Loss: 2.3163230419158936\n",
            "Epoch 40, Training Loss: 2.3196752071380615\n",
            "Epoch 40, Validation Loss: 2.314361810684204\n",
            "Epoch 41, Training Loss: 2.317775011062622\n",
            "Epoch 41, Validation Loss: 2.312485933303833\n",
            "Epoch 42, Training Loss: 2.31596302986145\n",
            "Epoch 42, Validation Loss: 2.3107025623321533\n",
            "Epoch 43, Training Loss: 2.3142340183258057\n",
            "Epoch 43, Validation Loss: 2.3089942932128906\n",
            "Epoch 44, Training Loss: 2.31258225440979\n",
            "Epoch 44, Validation Loss: 2.3073668479919434\n",
            "Epoch 45, Training Loss: 2.3110032081604004\n",
            "Epoch 45, Validation Loss: 2.3058059215545654\n",
            "Epoch 46, Training Loss: 2.3094918727874756\n",
            "Epoch 46, Validation Loss: 2.3043153285980225\n",
            "Epoch 47, Training Loss: 2.308044672012329\n",
            "Epoch 47, Validation Loss: 2.302884101867676\n",
            "Epoch 48, Training Loss: 2.306657075881958\n",
            "Epoch 48, Validation Loss: 2.3015143871307373\n",
            "Epoch 49, Training Loss: 2.305326461791992\n",
            "Epoch 49, Validation Loss: 2.3001978397369385\n",
            "Epoch 50, Training Loss: 2.304048538208008\n",
            "Epoch 50, Validation Loss: 2.2989351749420166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have already split the data into train, dev, and test sets\n",
        "# The trained model is stored in the variable W\n",
        "\n",
        "test_num = X1_test.nelement()\n",
        "\n",
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc2_test), dim=1) # typo was here!!!!!\n",
        "    logits_test = xenc_test @ W # predict log-counts on test data\n",
        "    counts_test = logits_test.exp() # counts, equivalent to N\n",
        "    probs_test = counts_test / counts_test.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_test = -probs_test[torch.arange(test_num), y_test].log().mean() + 0.01*(W**2).mean() # compute the loss on test data\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAB5k2HyL3nx",
        "outputId": "07fb340e-36a8-4a47-c872-19ce2f9cb8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.3037922382354736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Exercise 3:\n",
        "\n",
        "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
      ],
      "metadata": {
        "id": "XgfsYSpjqcVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tune the Strength of Smoothing or Regularization\n",
        "\n",
        "Follow these steps:\n",
        "\n",
        "1. **Tune the Strength of Smoothing or Regularization**:\n",
        "    - Define a range of values for the regularization strength (or smoothing strength). You can use a logarithmic scale, for example, \\([0.0001, 0.001, 0.01, 0.1, 1, 10]\\).\n",
        "    - For each value in the range, train the trigram model on the training set, applying the chosen regularization strength.\n",
        "    - Evaluate the model on the dev set (validation set) and record the loss for each value of the regularization strength.\n",
        "    - Plot the training and dev set losses as a function of the regularization strength to visualize the patterns.\n",
        "\n",
        "2. **Observe Patterns**:\n",
        "    - Look for patterns in the plot. Typically, as the regularization strength increases, the training loss may increase (due to the model being constrained), while the dev set loss may first decrease (as it prevents overfitting) and then increase (due to underfitting).\n",
        "    - Identify the value of the regularization strength that gives the lowest dev set loss. This is the optimal value that balances the bias-variance trade-off.\n",
        "\n",
        "3. **Evaluate on the Test Set**:\n",
        "    - Once you have identified the best regularization strength, retrain the trigram model on the entire training set using this value.\n",
        "    - Evaluate the model on the test set and report the loss. This will give you an unbiased estimate of the model's performance on unseen data.\n",
        "\n",
        "4. **Report the Loss**:\n",
        "    - Report the test set loss achieved with the best setting of the regularization strength. Compare it with the losses achieved without tuning the regularization strength.\n",
        "\n",
        "Remember that the goal of tuning the regularization strength is to find a balance between fitting the training data well and generalizing to new, unseen data."
      ],
      "metadata": {
        "id": "8YuJdhJ2QCHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)"
      ],
      "metadata": {
        "id": "Z8e8MYrPRXrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "regularization_strengths = [0.0000001, 0.000001, 0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "\n",
        "for reg_strength in regularization_strengths:\n",
        "    # Re-initialize the model with the new regularization strength\n",
        "    W = torch.randn((54, 27), generator=g, requires_grad=True) # Change: The weight matrix now has shape (54, 27)\n",
        "\n",
        "    # Train the model on the training set\n",
        "    for k in range(50):\n",
        "      # forward pass\n",
        "      xenc1 = F.one_hot(X1_train, num_classes=27).float()\n",
        "      xenc2 = F.one_hot(X2_train, num_classes=27).float()\n",
        "      xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "      logits = xenc @ W\n",
        "      counts = logits.exp()\n",
        "      probs = counts / counts.sum(1, keepdims=True)\n",
        "      loss = -probs[torch.arange(num), y_train].log().mean() + reg_strength * (W**2).mean()\n",
        "      # print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "      # backward pass\n",
        "      W.grad = None\n",
        "      loss.backward()\n",
        "\n",
        "      # update\n",
        "      W.data += -50.0 * W.grad\n",
        "\n",
        "\n",
        "    # Evaluate the model on the dev set\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float()\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float()\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1)\n",
        "    logits_val = xenc_val @ W\n",
        "    counts_val = logits_val.exp()\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True)\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + reg_strength * (W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # Record the losses\n",
        "    train_losses.append(loss.item())\n",
        "    dev_losses.append(loss_val.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkCAZGJeQvbg",
        "outputId": "53a7ff98-3ef3-4a83-e2eb-f6dc02194cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Validation Loss: 2.287224292755127\n",
            "Epoch 50, Validation Loss: 2.289905071258545\n",
            "Epoch 50, Validation Loss: 2.2870969772338867\n",
            "Epoch 50, Validation Loss: 2.290210008621216\n",
            "Epoch 50, Validation Loss: 2.285079002380371\n",
            "Epoch 50, Validation Loss: 2.2852158546447754\n",
            "Epoch 50, Validation Loss: 2.293046712875366\n",
            "Epoch 50, Validation Loss: 2.3439362049102783\n",
            "Epoch 50, Validation Loss: 2.542637825012207\n",
            "Epoch 50, Validation Loss: 2.988537073135376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hznu1jhPV5rk",
        "outputId": "e331fcb2-ba2d-4642-aa37-9b6c789c282f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.2952938079833984,\n",
              " 2.296586751937866,\n",
              " 2.2926440238952637,\n",
              " 2.296921730041504,\n",
              " 2.2944579124450684,\n",
              " 2.294264316558838,\n",
              " 2.300032615661621,\n",
              " 2.3512258529663086,\n",
              " 2.5461947917938232,\n",
              " 2.9872498512268066]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wab6_zDaV8ab",
        "outputId": "e1f9b529-2892-4189-c69f-13137800f6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.287224292755127,\n",
              " 2.289905071258545,\n",
              " 2.2870969772338867,\n",
              " 2.290210008621216,\n",
              " 2.285079002380371,\n",
              " 2.2852158546447754,\n",
              " 2.293046712875366,\n",
              " 2.3439362049102783,\n",
              " 2.542637825012207,\n",
              " 2.988537073135376]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the losses as a function of the regularization strength\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(regularization_strengths, train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(regularization_strengths, dev_losses, label='Dev Loss', marker='x')\n",
        "plt.xlabel('Regularization Strength')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "AbDsU-cZS9Mg",
        "outputId": "d42073c3-e8cc-4e2c-9c77-441329c211a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Identify the best regularization strength\n",
        "best_reg_strength = regularization_strengths[np.argmin(dev_losses)]\n",
        "best_reg_strength"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKBpF4v3S95K",
        "outputId": "26f8abf7-c2eb-4926-870e-5a5125075d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain with optimum regularization strength\n",
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "o-dv-ItAW99h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num), y_train].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float() # one-hot encoding for the first character\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float() # one-hot encoding for the second character\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "    logits_val = xenc_val @ W # predict log-counts\n",
        "    counts_val = logits_val.exp() # counts, equivalent to N\n",
        "    probs_val = counts_val / counts_val.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRIBG5PPWtuI",
        "outputId": "9bc3bebc-99fe-48ef-81d3-2fa8fe90b34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 4.0216522216796875\n",
            "Epoch 1, Validation Loss: 4.014607906341553\n",
            "Epoch 2, Training Loss: 3.3409671783447266\n",
            "Epoch 2, Validation Loss: 3.3350014686584473\n",
            "Epoch 3, Training Loss: 3.036214590072632\n",
            "Epoch 3, Validation Loss: 3.0272939205169678\n",
            "Epoch 4, Training Loss: 2.8642473220825195\n",
            "Epoch 4, Validation Loss: 2.8536946773529053\n",
            "Epoch 5, Training Loss: 2.7539925575256348\n",
            "Epoch 5, Validation Loss: 2.7420620918273926\n",
            "Epoch 6, Training Loss: 2.6779322624206543\n",
            "Epoch 6, Validation Loss: 2.666165351867676\n",
            "Epoch 7, Training Loss: 2.622770071029663\n",
            "Epoch 7, Validation Loss: 2.6107897758483887\n",
            "Epoch 8, Training Loss: 2.5807249546051025\n",
            "Epoch 8, Validation Loss: 2.5693233013153076\n",
            "Epoch 9, Training Loss: 2.5474092960357666\n",
            "Epoch 9, Validation Loss: 2.5361921787261963\n",
            "Epoch 10, Training Loss: 2.5201735496520996\n",
            "Epoch 10, Validation Loss: 2.509579658508301\n",
            "Epoch 11, Training Loss: 2.4973881244659424\n",
            "Epoch 11, Validation Loss: 2.4870896339416504\n",
            "Epoch 12, Training Loss: 2.4779882431030273\n",
            "Epoch 12, Validation Loss: 2.468257427215576\n",
            "Epoch 13, Training Loss: 2.46125864982605\n",
            "Epoch 13, Validation Loss: 2.451843738555908\n",
            "Epoch 14, Training Loss: 2.4466805458068848\n",
            "Epoch 14, Validation Loss: 2.437758684158325\n",
            "Epoch 15, Training Loss: 2.4338693618774414\n",
            "Epoch 15, Validation Loss: 2.425244092941284\n",
            "Epoch 16, Training Loss: 2.42252516746521\n",
            "Epoch 16, Validation Loss: 2.414313793182373\n",
            "Epoch 17, Training Loss: 2.412410259246826\n",
            "Epoch 17, Validation Loss: 2.4044570922851562\n",
            "Epoch 18, Training Loss: 2.403334617614746\n",
            "Epoch 18, Validation Loss: 2.395719528198242\n",
            "Epoch 19, Training Loss: 2.3951427936553955\n",
            "Epoch 19, Validation Loss: 2.3877415657043457\n",
            "Epoch 20, Training Loss: 2.387706995010376\n",
            "Epoch 20, Validation Loss: 2.3805766105651855\n",
            "Epoch 21, Training Loss: 2.380922794342041\n",
            "Epoch 21, Validation Loss: 2.3739657402038574\n",
            "Epoch 22, Training Loss: 2.374703884124756\n",
            "Epoch 22, Validation Loss: 2.3679616451263428\n",
            "Epoch 23, Training Loss: 2.36897873878479\n",
            "Epoch 23, Validation Loss: 2.3623762130737305\n",
            "Epoch 24, Training Loss: 2.3636882305145264\n",
            "Epoch 24, Validation Loss: 2.3572561740875244\n",
            "Epoch 25, Training Loss: 2.3587825298309326\n",
            "Epoch 25, Validation Loss: 2.352461814880371\n",
            "Epoch 26, Training Loss: 2.3542189598083496\n",
            "Epoch 26, Validation Loss: 2.348034620285034\n",
            "Epoch 27, Training Loss: 2.349963426589966\n",
            "Epoch 27, Validation Loss: 2.34386944770813\n",
            "Epoch 28, Training Loss: 2.3459839820861816\n",
            "Epoch 28, Validation Loss: 2.3399999141693115\n",
            "Epoch 29, Training Loss: 2.3422555923461914\n",
            "Epoch 29, Validation Loss: 2.3363454341888428\n",
            "Epoch 30, Training Loss: 2.338754653930664\n",
            "Epoch 30, Validation Loss: 2.332933187484741\n",
            "Epoch 31, Training Loss: 2.335461378097534\n",
            "Epoch 31, Validation Loss: 2.329700469970703\n",
            "Epoch 32, Training Loss: 2.332357883453369\n",
            "Epoch 32, Validation Loss: 2.326669931411743\n",
            "Epoch 33, Training Loss: 2.3294286727905273\n",
            "Epoch 33, Validation Loss: 2.323791265487671\n",
            "Epoch 34, Training Loss: 2.32666015625\n",
            "Epoch 34, Validation Loss: 2.321082592010498\n",
            "Epoch 35, Training Loss: 2.3240394592285156\n",
            "Epoch 35, Validation Loss: 2.3185043334960938\n",
            "Epoch 36, Training Loss: 2.3215560913085938\n",
            "Epoch 36, Validation Loss: 2.316070079803467\n",
            "Epoch 37, Training Loss: 2.3191990852355957\n",
            "Epoch 37, Validation Loss: 2.3137481212615967\n",
            "Epoch 38, Training Loss: 2.3169593811035156\n",
            "Epoch 38, Validation Loss: 2.3115501403808594\n",
            "Epoch 39, Training Loss: 2.3148293495178223\n",
            "Epoch 39, Validation Loss: 2.3094499111175537\n",
            "Epoch 40, Training Loss: 2.312800884246826\n",
            "Epoch 40, Validation Loss: 2.3074560165405273\n",
            "Epoch 41, Training Loss: 2.3108675479888916\n",
            "Epoch 41, Validation Loss: 2.3055477142333984\n",
            "Epoch 42, Training Loss: 2.3090221881866455\n",
            "Epoch 42, Validation Loss: 2.303731918334961\n",
            "Epoch 43, Training Loss: 2.3072597980499268\n",
            "Epoch 43, Validation Loss: 2.3019912242889404\n",
            "Epoch 44, Training Loss: 2.305574893951416\n",
            "Epoch 44, Validation Loss: 2.3003311157226562\n",
            "Epoch 45, Training Loss: 2.3039629459381104\n",
            "Epoch 45, Validation Loss: 2.2987377643585205\n",
            "Epoch 46, Training Loss: 2.3024184703826904\n",
            "Epoch 46, Validation Loss: 2.2972145080566406\n",
            "Epoch 47, Training Loss: 2.3009378910064697\n",
            "Epoch 47, Validation Loss: 2.295750379562378\n",
            "Epoch 48, Training Loss: 2.2995173931121826\n",
            "Epoch 48, Validation Loss: 2.2943482398986816\n",
            "Epoch 49, Training Loss: 2.2981534004211426\n",
            "Epoch 49, Validation Loss: 2.292998790740967\n",
            "Epoch 50, Training Loss: 2.296842336654663\n",
            "Epoch 50, Validation Loss: 2.2917041778564453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the Test Set:\n",
        "\n",
        "test_num = X1_test.nelement()\n",
        "\n",
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc2_test), dim=1) # typo was here!!!!!\n",
        "    logits_test = xenc_test @ W # predict log-counts on test data\n",
        "    counts_test = logits_test.exp() # counts, equivalent to N\n",
        "    probs_test = counts_test / counts_test.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss_test = -probs_test[torch.arange(test_num), y_test].log().mean() + best_reg_strength*(W**2).mean() # compute the loss on test data\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx6fJMeGXlSm",
        "outputId": "5d94c49a-8452-4a06-ca50-547aa2345834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.2965903282165527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "A seasoned machine learning practitioner would likely conclude the following:\n",
        "\n",
        "1. **Regularization Helped**: The improved test loss after tuning the regularization strength indicates that regularization helped prevent overfitting on the training data and improved generalization to new, unseen data.\n",
        "\n",
        "2. **Good Model Generalization**: The similar performance on the training, dev, and test sets suggests that the model has learned patterns that are generalizable across different data splits. This is a good sign of a well-trained model.\n",
        "\n",
        "3. **Further Improvements**: While the model has improved with regularization, it is always worth considering other ways to further enhance the model. This could include experimenting with different model architectures, adding more data, or exploring additional features.\n",
        "\n",
        "4. **Interpret Results**: The practitioner might also want to interpret the results in the context of the problem domain. Is the improvement in test loss significant enough to make a difference in the practical application of the model? The answer to this question can help determine the next steps in the model development process.\n",
        "\n",
        "5. **Model Deployment**: If the model has reached a satisfactory level of performance, it may be time to consider deploying it in a real-world setting. This involves integrating the model into a production environment and monitoring its performance over time.\n",
        "\n",
        "In summary, the decrease in test loss after optimizing the regularization strength indicates a successful regularization process, which helped the model generalize better to new data. Further model improvements can be explored, and the results should be interpreted in the context of the specific problem domain."
      ],
      "metadata": {
        "id": "tpqID7xZZTGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4:\n",
        "\n",
        "(this was tricky to get working)\n",
        "\n",
        "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n"
      ],
      "metadata": {
        "id": "e3fiICtvQT-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing the one-hot encoding\n",
        "\n",
        "You can replace the one-hot encoding with direct indexing into the weight matrix \\( W \\).\n",
        "\n",
        "1. Instead of using one-hot encoding for \\( X1 \\) and \\( X2 \\), use the indices directly to index into the weight matrix \\( W \\).\n",
        "2. Concatenate the rows you've selected from \\( W \\) along the second dimension.\n",
        "\n",
        "\n",
        "This approach is more efficient because it avoids the unnecessary step of creating one-hot encoded vectors, which can be memory-intensive and computationally expensive. Instead, we directly use the indices to select the relevant rows from the weight matrix \\( W \\)."
      ],
      "metadata": {
        "id": "ZpT4EgiVdqqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's assume we have a vocabulary of only 3 characters: \"a\", \"b\", and \"c\". We want to create a bigram model, so our weight matrix \\(W\\) will have a shape of (3, 3). Let's initialize it with the following values:\n",
        "\n",
        "$$\n",
        "W = \\begin{pmatrix}\n",
        "0.1 & 0.2 & 0.3 \\\\\n",
        "0.4 & 0.5 & 0.6 \\\\\n",
        "0.7 & 0.8 & 0.9\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Now, let's consider a simple input sequence \"abc\". We want to predict the next character for each character in the sequence. We'll use the one-hot encoding approach and the direct indexing approach to calculate the logits for each character.\n",
        "\n",
        "1. One-hot encoding approach:\n",
        "\n",
        "$$\n",
        "\\text{For character \"a\"}: \\text{one-hot} = [1, 0, 0], \\text{logits} = [1, 0, 0] \\times W = [0.1, 0.2, 0.3]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{For character \"b\"}: \\text{one-hot} = [0, 1, 0], \\text{logits} = [0, 1, 0] \\times W = [0.4, 0.5, 0.6]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{For character \"c\"}: \\text{one-hot} = [0, 0, 1], \\text{logits} = [0, 0, 1] \\times W = [0.7, 0.8, 0.9]\n",
        "$$\n",
        "\n",
        "2. Direct indexing approach:\n",
        "\n",
        "$$\n",
        "\\text{For character \"a\"}: \\text{index} = 0, \\text{logits} = W[0] = [0.1, 0.2, 0.3]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{For character \"b\"}: \\text{index} = 1, \\text{logits} = W[1] = [0.4, 0.5, 0.6]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{For character \"c\"}: \\text{index} = 2, \\text{logits} = W[2] = [0.7, 0.8, 0.9]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "DdH1V7AgChbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "logits_one_hot = xenc1 @ W\n",
        "loss_one_hot = -torch.log(logits_one_hot.exp() / logits_one_hot.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index = W[X1_train]\n",
        "loss_index = -torch.log(logits_index.exp() / logits_index.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using direct indexing:\", loss_index.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLKzJo6R9Tcn",
        "outputId": "644d79cf-75b5-458a-b48b-eae954f9778c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss using one-hot encoding: 1.101942777633667\n",
            "Loss using direct indexing: 1.101942777633667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "logits_one_hot1 = xenc1 @ W\n",
        "logits_one_hot2 = xenc2 @ W\n",
        "loss_one_hot = (-torch.log(logits_one_hot1.exp() / logits_one_hot1.exp().sum(1, keepdims=True)).mean()\n",
        "                -torch.log(logits_one_hot2.exp() / logits_one_hot2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index1 = W[X1_train]\n",
        "logits_index2 = W[X2_train]\n",
        "loss_index = (-torch.log(logits_index1.exp() / logits_index1.exp().sum(1, keepdims=True)).mean()\n",
        "              -torch.log(logits_index2.exp() / logits_index2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QOXxEaMDDSr",
        "outputId": "2265a072-18b4-4482-8c49-09bcd501a9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss using one-hot encoding: 1.101942777633667\n",
            "Loss using direct indexing: 1.101942777633667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.equal(logits_one_hot1, logits_index1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESlcgjK-HjCf",
        "outputId": "ff76390c-29ae-4c92-d9a5-ac851e923b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "A6pP58u3FTBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77VKhBd3aLt1",
        "outputId": "c47be582-7bc3-4d21-fb39-39347963ebd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". e m\n",
            "e m m\n",
            "m m a\n",
            "m a .\n",
            "number of examples:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "KpgdAM46bdlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character"
      ],
      "metadata": {
        "id": "JhaVWLJdbW4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryVe1k1ubwGk",
        "outputId": "e5a1b06e-43d8-42db-e534-15f4dcd1ebcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSmj8VDlbqUl",
        "outputId": "effd86f3-6d99-4663-8fa6-e62ed326409c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMozhnGm5oPc",
        "outputId": "fc9cfebd-856c-4e45-a0d5-868007963dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iuhY5ZW6C9b",
        "outputId": "59d31745-68b9-48be-aa8b-5ab42998d860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# much experimentation follows.\n",
        "\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWF94g18bl9g",
        "outputId": "87b48b7c-fa19-4e2d-c204-04f78901a8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.173408508300781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shJycz8UepCo",
        "outputId": "14f4244f-b43a-48ae-e0ff-c9d315365383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.482119560241699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc  # Use the indexed values directly as logits\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6Pfr-t99Dd3",
        "outputId": "e203c37d-4682-4a27-a2d0-cd49325f262a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.173408508300781\n",
            "4.482119560241699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits = xenc @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits_d = wenc # Use the indexed values directly as logits\n",
        "counts_d = logits_d.exp()\n",
        "probs_d = counts_d / counts_d.sum(1, keepdims=True)\n",
        "loss_d = -probs_d[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss_d.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y48IH32dBeZm",
        "outputId": "aecd3496-bf20-4697-e252-e5e0471bca35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.173408508300781\n",
            "4.1515631675720215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.equal(xenc1, wenc1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oDZrpkGF938",
        "outputId": "57701fc6-e572-49b0-804f-a8aa1fb461ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS1vfyINUJXf",
        "outputId": "a02ef09b-e5fd-4d9e-b932-4704bc54830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wenc1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC1OCdR7UNAM",
        "outputId": "8d6806d6-955e-4a54-e41f-2942f8cd60bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wenc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcafDea0Uqce",
        "outputId": "9958f8d1-e76c-425b-9127-64566aa8941a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float()\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float()\n",
        "logits1 = xenc1 @ W[:27]\n",
        "logits2 = xenc2 @ W[:27]\n",
        "logits = torch.cat((logits1, logits2), dim=1)\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[xs1]\n",
        "wenc2 = W[xs2]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits = wenc  # Use the indexed values directly as logits\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY0opMT5F1ng",
        "outputId": "2e9041dd-562c-461e-93d8-25056435d83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.482119560241699\n",
            "4.482119560241699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "logits_one_hot = xenc @ torch.cat((W, W), dim=0)\n",
        "loss_one_hot = -torch.log(logits_one_hot.exp() / logits_one_hot.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "wenc1 = W[X1_train]\n",
        "wenc2 = W[X2_train]\n",
        "wenc = torch.cat((wenc1, wenc2), dim=1)\n",
        "logits_index = wenc  # Use the indexed values directly as logits\n",
        "loss_index = -torch.log(logits_index.exp() / logits_index.exp().sum(1, keepdims=True)).mean()\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNxBmjrT-azL",
        "outputId": "a70e9782-00ba-4ad4-d725-451d2fcc3103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss using one-hot encoding: 1.1119014024734497\n",
            "Loss using direct indexing: 1.8173422813415527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the small W matrix\n",
        "W = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                  [0.4, 0.5, 0.6],\n",
        "                  [0.7, 0.8, 0.9]], requires_grad=True)\n",
        "\n",
        "# Define the input sequence\n",
        "input_seq = \"abc\"\n",
        "stoi = {'a': 0, 'b': 1, 'c': 2}\n",
        "X1_train = torch.tensor([stoi[c] for c in input_seq])\n",
        "X2_train = torch.roll(X1_train, -1)\n",
        "\n",
        "# One-hot encoding approach\n",
        "xenc1 = F.one_hot(X1_train, num_classes=3).float()\n",
        "xenc2 = F.one_hot(X2_train, num_classes=3).float()\n",
        "logits_one_hot1 = xenc1 @ W\n",
        "logits_one_hot2 = xenc2 @ W\n",
        "loss_one_hot = (-torch.log(logits_one_hot1.exp() / logits_one_hot1.exp().sum(1, keepdims=True)).mean()\n",
        "                -torch.log(logits_one_hot2.exp() / logits_one_hot2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using one-hot encoding:\", loss_one_hot.item())\n",
        "\n",
        "# Direct indexing approach\n",
        "logits_index1 = W[X1_train]\n",
        "logits_index2 = W[X2_train]\n",
        "loss_index = (-torch.log(logits_index1.exp() / logits_index1.exp().sum(1, keepdims=True)).mean()\n",
        "              -torch.log(logits_index2.exp() / logits_index2.exp().sum(1, keepdims=True)).mean()) / 2\n",
        "print(\"Loss using direct indexing:\", loss_index.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaAMwjKd_EoK",
        "outputId": "b37fe7ca-e1dc-4e45-9c2a-0f09722c21c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss using one-hot encoding: 1.101942777633667\n",
            "Loss using direct indexing: 1.101942777633667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wenc1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqmONi7phUCv",
        "outputId": "dd4a06f6-1a71-49b2-8a55-778b5b308d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([156890, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion of problems\n",
        "\n",
        "The discrepancy between the two loss values arises from the way the input features are generated in each case.\n",
        "\n",
        "In the first code block, you are using one-hot encoding. Each character is represented by a 27-dimensional vector with a single 1 at the index corresponding to the character, and all other elements are 0. The input feature vectors are then created by concatenating these one-hot encoded vectors.\n",
        "\n",
        "In the second code block, you are using the rows of the weight matrix \\( W \\) as the input feature vectors. Each character is represented by a 27-dimensional vector, which is a row of \\( W \\) corresponding to the character's index. These vectors are then concatenated to create the input feature vectors.\n",
        "\n",
        "The difference in the input feature vectors leads to different loss values. The input features in the second code block are not one-hot encoded and have different values, which result in different logits, probabilities, and loss values.\n",
        "\n",
        "To summarize, the two code blocks produce different input features due to the different methods of generating the feature vectors. This leads to different loss values in each case."
      ],
      "metadata": {
        "id": "uNtqlMnuf8UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have implemented the indexing into \\( W \\) correctly, but there's a slight misconception here. Let me clarify:\n",
        "\n",
        "When you use one-hot encoding to multiply with the weight matrix \\( W \\), it's equivalent to selecting a row of \\( W \\). So, the operation \\( \\text{{one-hot encoding}} \\times W \\) is equivalent to selecting the corresponding row of \\( W \\).\n",
        "\n",
        "Here's the key point: when you use one-hot encoding, the resulting vectors should be the same as when you index directly into \\( W \\). If they are different, it means there's an issue with the implementation.\n",
        "\n",
        "Let's break down the two operations:\n",
        "\n",
        "1. One-hot encoding:\n",
        "   - You create a one-hot vector (e.g., [0, 0, 1, 0, ..., 0]) representing the character.\n",
        "   - You multiply the one-hot vector with \\( W \\), which is equivalent to selecting the corresponding row of \\( W \\).\n",
        "\n",
        "2. Indexing into \\( W \\):\n",
        "   - You directly select the row of \\( W \\) corresponding to the character's index.\n",
        "\n",
        "Both methods should give you the same vectors. If they don't, there might be an issue with the implementation.\n",
        "\n",
        "Regarding the loss calculation, it should be the same for both methods, as the input features should be the same. If the loss values are different, there might be a mistake in the code."
      ],
      "metadata": {
        "id": "MmK9vrz4g9Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    print(ch1, ch2, ch3)\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "xTeEx9PFdyzv",
        "outputId": "ff46c513-8574-469f-b54e-d6397d368307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-91da53711190>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mix2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mix3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfezl_-N1XjF",
        "outputId": "2d1f2ef1-4255-4c54-ce7c-94a974912016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCwwigH01a4K",
        "outputId": "39c5d953-4944-4cf7-938f-390c560b0cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD0OFX_i1dMo",
        "outputId": "c2e1a4df-2a96-4c10-8cef-6d05dd9745dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647 + 1)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "SPq02aXd1GlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "WoprQjIb18z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character"
      ],
      "metadata": {
        "id": "9suJFqm51AWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfKqWBR31_ZN",
        "outputId": "de6a27f4-cb05-4e50-83a7-8fb546ed664b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc2 = F.one_hot(xs2, num_classes=27).float() # one-hot encoding for the second character"
      ],
      "metadata": {
        "id": "cWf7r9oW2GEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnxx37O92I8T",
        "outputId": "334f3d96-3315-4ce1-c473-730c7b2e8cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = torch.cat((xenc1, xenc2), dim=1)"
      ],
      "metadata": {
        "id": "bjvwlPup2MhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgHHgCmt2NlZ",
        "outputId": "7ac2c7c8-6f17-4b99-a1e8-884a3c64451a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbwjen3x2TOE",
        "outputId": "d3ac9bcd-f884-4f73-a03e-a34cefee9e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj5UOU4G2XZ6",
        "outputId": "d9bd7260-889e-4a72-df2a-10505452de65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = xenc @ W"
      ],
      "metadata": {
        "id": "aCd22yFu2iAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD_GjlJc2jwL",
        "outputId": "bf00e507-3045-40e7-e558-46437e5107dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kamB95D120AT",
        "outputId": "40e430af-626a-45df-fba9-d0fbece93505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.8604, -0.0889,  0.3562,  0.6478,  0.0947,  1.3138, -0.4473, -0.3683,\n",
              "         -0.9185, -0.5939,  0.2033, -0.1885,  0.3468, -1.6096, -3.5663,  0.8754,\n",
              "         -0.5076, -1.2948, -1.2174,  0.7571, -0.2811,  1.5891, -0.7729, -3.6464,\n",
              "         -0.5814, -0.6022,  0.0141],\n",
              "        [-1.2016, -1.3183, -1.7950, -1.0987,  4.4293,  2.4405,  0.2236, -1.7889,\n",
              "         -1.3992, -0.1719, -2.7437,  2.0933,  2.9323, -2.2785, -2.3792,  0.8934,\n",
              "          0.4049,  2.7457, -1.8066, -0.9694, -0.3684, -0.8973, -0.5126, -0.1915,\n",
              "          0.5802, -0.0993,  0.5476],\n",
              "        [-0.2141, -0.4224,  0.3918, -1.6844,  3.0541,  2.0704,  0.8099, -1.8645,\n",
              "         -0.8688,  0.5127, -0.6052, -1.6720,  1.4171, -1.3302, -2.5247, -1.0668,\n",
              "         -0.0103,  1.0862, -1.5937, -0.1254,  0.1067,  1.3126, -0.4615, -0.0940,\n",
              "          1.9840, -0.6918,  0.9048],\n",
              "        [ 0.9132, -0.9307,  3.6993,  1.0844, -0.5137,  0.5867,  0.8831, -0.6592,\n",
              "          0.4703,  1.7854,  0.7108, -2.4447, -0.6505,  1.9832, -0.3776, -0.4645,\n",
              "          0.1235, -1.6647,  2.7821, -0.1840, -1.0757,  1.4499,  1.7054, -0.0877,\n",
              "          2.2636, -1.9208, -1.5833]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "xenc1 = F.one_hot(xs1, num_classes=27).float() # one-hot encoding for the first character\n",
        "xenc2 = F.one_hot(xs2, num_classes=27).float() # one-hot encoding for the second character\n",
        "xenc = torch.cat((xenc1, xenc2), dim=1) # Change: Concatenate the two one-hot encoded vectors\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "UQQr4U8w1hG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5:\n",
        "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
      ],
      "metadata": {
        "id": "KdRss5ebQb2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-entropy loss\n",
        "\n",
        "Cross-entropy loss is a measure used to quantify the difference between two probability distributions. In the context of machine learning, it is often used to measure the difference between the predicted probability distribution and the true distribution of the labels.\n",
        "\n",
        "Here's a simple explanation:\n",
        "\n",
        "1. **Probability Distributions**: In classification tasks, we often want our model to predict a probability distribution over the possible classes. For example, in a dog vs. cat classification task, the model might predict a probability of 0.8 for the \"dog\" class and 0.2 for the \"cat\" class for a given image.\n",
        "\n",
        "2. **Predicted vs. True Distributions**: The true distribution of the labels is often represented as a one-hot encoded vector. For the above example, if the image is indeed of a dog, the true distribution would be [1, 0], representing 100% probability for the \"dog\" class and 0% for the \"cat\" class.\n",
        "\n",
        "3. **Calculating the Loss**: Cross-entropy loss measures how well the predicted probability distribution matches the true distribution. It does so by taking the negative log of the predicted probability for the true class. For the above example, the loss would be \\($-\\log(0.8)$\\) since the true class is \"dog\".\n",
        "\n",
        "4. **Interpretation**: A lower cross-entropy loss indicates that the predicted distribution is closer to the true distribution. If the predicted probabilities perfectly match the true distribution (e.g., [0.8, 0.2] vs. [0.8, 0.2]), the cross-entropy loss would be zero.\n",
        "\n",
        "5. **Optimization**: During training, the goal is to minimize the cross-entropy loss, which effectively means making the predicted distribution as close as possible to the true distribution. This is achieved by adjusting the model's parameters in the direction that reduces the loss.\n",
        "\n",
        "In summary, cross-entropy loss is a metric that tells us how well our model's predicted probability distribution matches the true distribution of the labels. By minimizing this loss, we aim to improve the accuracy of our model's predictions."
      ],
      "metadata": {
        "id": "DuX8hNsZbneY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Log Likelihood (NLL) and Cross Entropy (CE)\n",
        "\n",
        "Negative Log Likelihood (NLL) and Cross Entropy (CE) are closely related concepts in machine learning, and in fact, CE is an extension of NLL. Both are loss functions used to measure how well a model's predictions match the actual data.\n",
        "\n",
        "1. **Negative Log Likelihood (NLL)**:\n",
        "   - NLL is a measure of how probable the observed data is given the model's predictions.\n",
        "   - In the context of classification, NLL only considers the predicted probability of the correct class (the class that actually occurred), and not the probabilities assigned to the incorrect classes.\n",
        "   - It is calculated by taking the negative logarithm of the predicted probability for the true class. A lower NLL indicates a better model.\n",
        "   - NLL does not consider the entire distribution of predictions, only the prediction for the actual class.\n",
        "\n",
        "2. **Cross Entropy (CE)**:\n",
        "   - CE is a measure of the difference between two probability distributions - the predicted distribution and the actual distribution (often one-hot encoded).\n",
        "   - It considers the entire distribution of predicted probabilities, not just the probability assigned to the correct class.\n",
        "   - CE is the sum of the NLL and the entropy of the true distribution. Since the true distribution is often a one-hot vector (with entropy of zero), in practice, CE and NLL are often identical for classification problems.\n",
        "   - Like NLL, a lower CE indicates a better model, and CE is minimized during training.\n",
        "\n",
        "**When to use one over the other?**\n",
        "   - In practice, for classification problems, CE and NLL are often used interchangeably because the true distribution is typically one-hot encoded, making the entropy term in CE zero.\n",
        "   - However, if you have a problem where the true distribution is not one-hot encoded (e.g., soft labels or multi-label classification), you might prefer to use CE as it considers the entire distribution of predictions.\n",
        "   - In general, CE is more commonly used in the field of machine learning, as it is applicable to a wider range of problems.\n",
        "\n",
        "In summary, both NLL and CE are measures of how well a model's predictions align with the actual data. While they are closely related and often used interchangeably for classification problems, CE considers the entire distribution of predictions, making it more versatile for different types of problems."
      ],
      "metadata": {
        "id": "CrYoOEBbcldZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.functional.cross_entropy`\n",
        "\n",
        "`torch.nn.functional.cross_entropy` is a PyTorch function that calculates the cross-entropy loss between the predicted values and the actual values. Cross-entropy loss measures the difference between two probability distributions, and it is commonly used in classification tasks. Lower cross-entropy indicates that the predicted probability distribution is closer to the actual distribution.\n",
        "\n",
        "In the context of a classification task, the actual values are usually represented as one-hot encoded vectors, where one element is 1 (indicating the true class) and the rest are 0. The predicted values are probability distributions produced by the model for each class.\n",
        "\n",
        "Here's what the `cross_entropy` function does step-by-step:\n",
        "\n",
        "1. **Softmax Activation**: It applies the softmax function to the model's output (logits) to convert them into probability distributions. Softmax ensures that the sum of the probabilities for all classes is 1 and that each probability is in the range [0, 1].\n",
        "2. **Negative Log Likelihood**: It calculates the negative log likelihood (NLL) for each instance in the batch. NLL is the negative logarithm of the predicted probability of the true class. Lower NLL indicates that the model assigns a higher probability to the correct class.\n",
        "3. **Averaging**: It averages the NLL values across the batch to get the final cross-entropy loss.\n",
        "\n",
        "When training a model, the goal is to minimize the cross-entropy loss, which effectively means maximizing the predicted probability of the correct class for each instance.\n",
        "\n",
        "In summary, `torch.nn.functional.cross_entropy` is a convenient function in PyTorch that calculates the cross-entropy loss for classification tasks. It combines the softmax activation and the NLL computation into one step and is commonly used as the loss function for training classification models."
      ],
      "metadata": {
        "id": "mPdQ9w2ZdaCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-entropy loss is often preferred\n",
        "\n",
        "The cross-entropy loss is often preferred over the negative log likelihood (NLL) for classification problems because it combines the softmax activation and the NLL computation into one step. This can be more numerically stable and efficient, as the softmax function ensures that the predicted probabilities are properly normalized (they sum to 1) before computing the NLL.\n",
        "\n",
        "In the code you provided, the loss is computed manually by first applying the exponential function to the logits to get the counts, then normalizing them to get the probabilities, and finally computing the NLL. Using the cross-entropy loss would simplify this process.\n",
        "\n",
        "Note that we no longer need to calculate the counts and probabilities manually, as the `cross_entropy` function handles this for us. Also, keep in mind that the `cross_entropy` function already applies the softmax activation to the logits internally, so there is no need to do it manually in this case."
      ],
      "metadata": {
        "id": "5MfXn280rFdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DDZz4EXo8ZG",
        "outputId": "04ced912-8b04-4c8a-acca-928630e6386f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-19 11:57:20--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.1’\n",
            "\n",
            "names.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-19 11:57:21 (12.9 MB/s) - ‘names.txt.1’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# create the dataset for trigrams\n",
        "xs1, xs2, ys = [], [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs1.append(ix1)\n",
        "    xs2.append(ix2)\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs1 = torch.tensor(xs1)\n",
        "xs2 = torch.tensor(xs2)\n",
        "ys = torch.tensor(ys)\n",
        "num = ys.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCCH8SIVpC97",
        "outputId": "dd06fac5-cd20-428f-bce6-894b12fd6efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  196113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to numpy arrays\n",
        "xs1 = np.array(xs1)\n",
        "xs2 = np.array(xs2)\n",
        "ys = np.array(ys)\n",
        "\n",
        "# Split the dataset into 80% train, 10% validation, and 10% test\n",
        "X1_train, X1_temp, X2_train, X2_temp, y_train, y_temp = train_test_split(xs1, xs2, ys, test_size=0.2, random_state=42)\n",
        "X1_val, X1_test, X2_val, X2_test, y_val, y_test = train_test_split(X1_temp, X2_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X1_train = torch.tensor(X1_train)\n",
        "X2_train = torch.tensor(X2_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "X1_val = torch.tensor(X1_val)\n",
        "X2_val = torch.tensor(X2_val)\n",
        "y_val = torch.tensor(y_val)\n",
        "X1_test = torch.tensor(X1_test)\n",
        "X2_test = torch.tensor(X2_test)\n",
        "y_test = torch.tensor(y_test)"
      ],
      "metadata": {
        "id": "mrKIk1uPpGpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "\n",
        "best_reg_strength = 0.0001\n",
        "\n",
        "num = y_train.nelement()\n",
        "num_val = y_val.nelement()\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # forward pass\n",
        "    xenc1 = F.one_hot(X1_train, num_classes=27).float()\n",
        "    xenc2 = F.one_hot(X2_train, num_classes=27).float()\n",
        "    xenc = torch.cat((xenc1, xenc2), dim=1)\n",
        "    logits = xenc @ W\n",
        "    # counts = logits.exp()\n",
        "    # probs = counts / counts.sum(1, keepdims=True)\n",
        "    # loss = -probs[torch.arange(num), y_train].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    loss = F.cross_entropy(logits, y_train)\n",
        "    print(f\"Epoch {k+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Forward pass (validation)\n",
        "    xenc_val1 = F.one_hot(X1_val, num_classes=27).float()\n",
        "    xenc_val2 = F.one_hot(X2_val, num_classes=27).float()\n",
        "    xenc_val = torch.cat((xenc_val1, xenc_val2), dim=1)\n",
        "    logits_val = xenc_val @ W\n",
        "    # counts_val = logits_val.exp()\n",
        "    # probs_val = counts_val / counts_val.sum(1, keepdims=True)\n",
        "    # loss_val = -probs_val[torch.arange(num_val), y_val].log().mean() + best_reg_strength*(W**2).mean()\n",
        "    loss_val = F.cross_entropy(logits_val, y_val) + best_reg_strength*(W**2).mean()\n",
        "    print(f\"Epoch {k+1}, Validation Loss: {loss_val.item()}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    W.data += -50.0 * W.grad\n",
        "\n",
        "\n",
        "    test_num = X1_test.nelement()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4R3ZP__oB-K",
        "outputId": "9b361924-b199-4e61-f5b7-a1815ad08601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 4.185479164123535\n",
            "Epoch 1, Validation Loss: 4.19601583480835\n",
            "Epoch 2, Training Loss: 3.3574705123901367\n",
            "Epoch 2, Validation Loss: 3.3696699142456055\n",
            "Epoch 3, Training Loss: 3.0430190563201904\n",
            "Epoch 3, Validation Loss: 3.0531718730926514\n",
            "Epoch 4, Training Loss: 2.872457265853882\n",
            "Epoch 4, Validation Loss: 2.8775646686553955\n",
            "Epoch 5, Training Loss: 2.7681379318237305\n",
            "Epoch 5, Validation Loss: 2.771624803543091\n",
            "Epoch 6, Training Loss: 2.6956446170806885\n",
            "Epoch 6, Validation Loss: 2.6967272758483887\n",
            "Epoch 7, Training Loss: 2.6401021480560303\n",
            "Epoch 7, Validation Loss: 2.640502452850342\n",
            "Epoch 8, Training Loss: 2.5960330963134766\n",
            "Epoch 8, Validation Loss: 2.5950827598571777\n",
            "Epoch 9, Training Loss: 2.560100555419922\n",
            "Epoch 9, Validation Loss: 2.558781623840332\n",
            "Epoch 10, Training Loss: 2.530360698699951\n",
            "Epoch 10, Validation Loss: 2.528148651123047\n",
            "Epoch 11, Training Loss: 2.5054073333740234\n",
            "Epoch 11, Validation Loss: 2.502929449081421\n",
            "Epoch 12, Training Loss: 2.4842700958251953\n",
            "Epoch 12, Validation Loss: 2.4811363220214844\n",
            "Epoch 13, Training Loss: 2.4661717414855957\n",
            "Epoch 13, Validation Loss: 2.4628219604492188\n",
            "Epoch 14, Training Loss: 2.4505274295806885\n",
            "Epoch 14, Validation Loss: 2.446676015853882\n",
            "Epoch 15, Training Loss: 2.436863899230957\n",
            "Epoch 15, Validation Loss: 2.432830333709717\n",
            "Epoch 16, Training Loss: 2.424819231033325\n",
            "Epoch 16, Validation Loss: 2.420393228530884\n",
            "Epoch 17, Training Loss: 2.4141085147857666\n",
            "Epoch 17, Validation Loss: 2.409528970718384\n",
            "Epoch 18, Training Loss: 2.4045143127441406\n",
            "Epoch 18, Validation Loss: 2.399622917175293\n",
            "Epoch 19, Training Loss: 2.3958640098571777\n",
            "Epoch 19, Validation Loss: 2.3908421993255615\n",
            "Epoch 20, Training Loss: 2.388021945953369\n",
            "Epoch 20, Validation Loss: 2.3827521800994873\n",
            "Epoch 21, Training Loss: 2.380878210067749\n",
            "Epoch 21, Validation Loss: 2.3754994869232178\n",
            "Epoch 22, Training Loss: 2.3743441104888916\n",
            "Epoch 22, Validation Loss: 2.368767499923706\n",
            "Epoch 23, Training Loss: 2.36834454536438\n",
            "Epoch 23, Validation Loss: 2.362677574157715\n",
            "Epoch 24, Training Loss: 2.3628177642822266\n",
            "Epoch 24, Validation Loss: 2.356992244720459\n",
            "Epoch 25, Training Loss: 2.3577096462249756\n",
            "Epoch 25, Validation Loss: 2.3518097400665283\n",
            "Epoch 26, Training Loss: 2.352975606918335\n",
            "Epoch 26, Validation Loss: 2.3469491004943848\n",
            "Epoch 27, Training Loss: 2.348576545715332\n",
            "Epoch 27, Validation Loss: 2.3424882888793945\n",
            "Epoch 28, Training Loss: 2.3444786071777344\n",
            "Epoch 28, Validation Loss: 2.3382890224456787\n",
            "Epoch 29, Training Loss: 2.3406524658203125\n",
            "Epoch 29, Validation Loss: 2.3344123363494873\n",
            "Epoch 30, Training Loss: 2.3370728492736816\n",
            "Epoch 30, Validation Loss: 2.3307511806488037\n",
            "Epoch 31, Training Loss: 2.333717107772827\n",
            "Epoch 31, Validation Loss: 2.3273534774780273\n",
            "Epoch 32, Training Loss: 2.3305652141571045\n",
            "Epoch 32, Validation Loss: 2.324136257171631\n",
            "Epoch 33, Training Loss: 2.3276002407073975\n",
            "Epoch 33, Validation Loss: 2.3211376667022705\n",
            "Epoch 34, Training Loss: 2.3248062133789062\n",
            "Epoch 34, Validation Loss: 2.318291187286377\n",
            "Epoch 35, Training Loss: 2.322169542312622\n",
            "Epoch 35, Validation Loss: 2.315627098083496\n",
            "Epoch 36, Training Loss: 2.3196780681610107\n",
            "Epoch 36, Validation Loss: 2.3130931854248047\n",
            "Epoch 37, Training Loss: 2.317319631576538\n",
            "Epoch 37, Validation Loss: 2.310713291168213\n",
            "Epoch 38, Training Loss: 2.3150851726531982\n",
            "Epoch 38, Validation Loss: 2.308445692062378\n",
            "Epoch 39, Training Loss: 2.3129653930664062\n",
            "Epoch 39, Validation Loss: 2.3063082695007324\n",
            "Epoch 40, Training Loss: 2.3109512329101562\n",
            "Epoch 40, Validation Loss: 2.3042688369750977\n",
            "Epoch 41, Training Loss: 2.30903697013855\n",
            "Epoch 41, Validation Loss: 2.3023407459259033\n",
            "Epoch 42, Training Loss: 2.30721378326416\n",
            "Epoch 42, Validation Loss: 2.3004980087280273\n",
            "Epoch 43, Training Loss: 2.3054769039154053\n",
            "Epoch 43, Validation Loss: 2.2987518310546875\n",
            "Epoch 44, Training Loss: 2.3038198947906494\n",
            "Epoch 44, Validation Loss: 2.2970802783966064\n",
            "Epoch 45, Training Loss: 2.3022377490997314\n",
            "Epoch 45, Validation Loss: 2.29549241065979\n",
            "Epoch 46, Training Loss: 2.3007256984710693\n",
            "Epoch 46, Validation Loss: 2.2939701080322266\n",
            "Epoch 47, Training Loss: 2.29927921295166\n",
            "Epoch 47, Validation Loss: 2.292520523071289\n",
            "Epoch 48, Training Loss: 2.29789400100708\n",
            "Epoch 48, Validation Loss: 2.2911291122436523\n",
            "Epoch 49, Training Loss: 2.296566963195801\n",
            "Epoch 49, Validation Loss: 2.2898013591766357\n",
            "Epoch 50, Training Loss: 2.2952938079833984\n",
            "Epoch 50, Validation Loss: 2.288525342941284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass (without gradients computation)\n",
        "with torch.no_grad():\n",
        "    xenc1_test = F.one_hot(X1_test, num_classes=27).float()\n",
        "    xenc2_test = F.one_hot(X2_test, num_classes=27).float()\n",
        "    xenc_test = torch.cat((xenc1_test, xenc2_test), dim=1)\n",
        "    logits_test = xenc_test @ W\n",
        "    loss_test = F.cross_entropy(logits_test, y_test) + best_reg_strength*(W**2).mean()\n",
        "\n",
        "print(\"Test Loss:\", loss_test.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x7-jI9kqBII",
        "outputId": "0f1b8848-b07c-4194-c44e-fd74823bc061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.2949140071868896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "The difference in the loss values observed between the NLL and cross-entropy implementations is likely due to numerical stability improvements in the cross-entropy computation. In machine learning, numerical stability refers to the ability of an algorithm to produce accurate and consistent results even when dealing with very small or very large numbers. This is important because computers have finite precision, and small numerical errors can accumulate over time, leading to inaccurate results.\n",
        "\n",
        "In the NLL implementation, you manually computed the exponential of the logits and then normalized them to get probabilities. This approach can be numerically unstable when dealing with very large or very small logits, as the exponential function can quickly overflow or underflow.\n",
        "\n",
        "On the other hand, the cross-entropy loss in PyTorch combines the softmax activation and the NLL computation into one step. This function is implemented in a way that is more numerically stable, as it avoids the explicit computation of the exponential and normalization steps. Instead, it directly computes the log probabilities in a way that is less prone to numerical issues.\n",
        "\n",
        "The difference in loss values you observed is likely due to this improved numerical stability in the cross-entropy computation. It is expected to see small differences in the loss values when switching between the two methods. However, the difference you observed is not significant, and it is unlikely to have a meaningful impact on the model's performance.\n",
        "\n",
        "In conclusion, using the cross-entropy loss in PyTorch is a good choice, as it provides a more numerically stable and efficient way to compute the loss compared to manually computing the NLL."
      ],
      "metadata": {
        "id": "ksYX5Gqari3P"
      }
    }
  ]
}