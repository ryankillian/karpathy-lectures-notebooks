{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VqvEmxLh3Hab",
        "Kkecr9IUGp00",
        "VAly5we33YnS",
        "P-d2wMz13dYk",
        "pqbGMqdw2UFk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n"
      ],
      "metadata": {
        "id": "RNSRXbWC5oaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Notebook Introduction\n",
        "\n",
        "Welcome to this Colab Notebook. In this session, we closely follow Andrej Karpathy's Neural Networks: Hero to Zero Lecture 2 as presented in the video \"The spelled-out intro to language modeling: building makemore\". The video provides a comprehensive understanding of building a bigram character level language model. Here, we will implement and dissect the main components discussed in the lecture.\n",
        "\n",
        "**Main Tasks Covered:**\n",
        "1. **Understanding Bigram Language Models:** We'll delve into the concept of bigrams and how they can be used to predict sequences of characters in a language model.\n",
        "2. **Sampling from a Bigram Character Level Language Model:** Learn how to sample from the model using an array of counts and the importance of tensor broadcasting in PyTorch.\n",
        "3. **Training a Bi-gram Language Model:** We'll go through the process of training our model, evaluating its performance using the negative log likelihood, and implementing model smoothing to handle unseen bigrams.\n",
        "4. **Neural Network Approach:** We'll introduce a gradient-based neural network approach to the bigram character level language modeling, explaining the forward and backward passes, loss calculation, and weight updates.\n",
        "5. **Feeding Integers into Neural Networks:** Understand how to prepare data for neural networks, specifically one-hot encoding for integer data, and the softmax operation.\n",
        "6. **Implementing a Basic Neural Network:** We will construct a basic neural network, understanding its architecture and how it's trained to make predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LLE-pQtj7wD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Character-Level Bigram Neural Network\n",
        "\n",
        "### Introduction\n",
        "\n",
        "We're building a simple character-level bigram neural network. This model aims to predict the next character in a sequence given a current character.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - The network receives a single character as input. We one-hot encode this character, converting it into a binary vector where only the index corresponding to the character is set to 1.\n",
        "   - Our vocabulary consists of 27 characters (26 letters plus a special character for the start/end of a word).\n",
        "\n",
        "2. **Weights or Parameters:**\n",
        "   - The network has a weight matrix `W`, representing the parameters of the network. This matrix is initially randomly initialized.\n",
        "   - Each neuron in the network has a set of weights corresponding to each input character. Since we have 27 characters in our vocabulary, we have a 27x27 weight matrix.\n",
        "\n",
        "3. **Output Layer:**\n",
        "   - The network outputs a probability distribution over the next possible character in the sequence.\n",
        "   - It does this by performing a linear transformation of the input using the weight matrix `W`, followed by a softmax transformation to convert the raw output (logits) into probabilities.\n",
        "\n",
        "### Training the Model\n",
        "\n",
        "1. **Loss Function:**\n",
        "   - We evaluate the network's predictions using a loss function. In this case, we're using the negative log-likelihood (NLL) loss.\n",
        "   - This loss function measures how well the network's predicted probabilities align with the actual next characters in the sequence.\n",
        "   - We want the network to assign high probability to the correct next character and low probability to the incorrect characters. Lower loss indicates better performance.\n",
        "\n",
        "2. **Gradient-Based Optimization:**\n",
        "   - We use gradient-based optimization to tune the parameters of the network.\n",
        "   - The optimization process involves calculating the gradient of the loss with respect to the parameters, and then adjusting the parameters in the direction that reduces the loss.\n",
        "   - This iterative process continues until the loss converges to a minimum value.\n",
        "\n",
        "3. **Training Goal:**\n",
        "   - Our goal is to find the parameters that minimize the loss function, resulting in a model that accurately predicts the probability distribution over the next character in a sequence.\n",
        "   - As the network trains, it learns the patterns of character sequences in the training data and becomes better at predicting the next character.\n",
        "\n",
        "By following this step-by-step process, we can train a character-level bigram neural network that can predict the next character in a sequence based on the input character. This simple model serves as a building block for more complex language models."
      ],
      "metadata": {
        "id": "ytnp1nzrtJzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of transcript\n",
        "\n",
        "---\n",
        "\n",
        "# Understanding Bigram Language Models\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- We are building a character-level language model based on a dataset of names (`names.txt`).\n",
        "- The dataset contains approximately 32,000 names.\n",
        "- The goal is to predict sequences of characters, specifically predicting the next character in a sequence.\n",
        "\n",
        "## What is a Bigram Language Model?\n",
        "\n",
        "- A bigram language model predicts the next character in a sequence based on only the preceding character.\n",
        "- It works with pairs of characters (bigrams) at a time, looking at a given character and predicting the next.\n",
        "- The model is simple and does not capture long-term dependencies between characters.\n",
        "\n",
        "## Data Processing\n",
        "\n",
        "### Extracting Bigrams\n",
        "\n",
        "1. Iterate over each word and extract pairs of characters.\n",
        "2. Use special start (`s`) and end (`e`) tokens to indicate the beginning and end of a word.\n",
        "3. With this method, even a single word like \"emma\" can provide multiple examples:\n",
        "   - `s` followed by `e`\n",
        "   - `e` followed by `m`\n",
        "   - `m` followed by `m`\n",
        "   - `m` followed by `a`\n",
        "   - `a` followed by `e`\n",
        "\n",
        "### Counting Bigrams\n",
        "\n",
        "1. To understand the structure of the language, count how often each bigram occurs in the dataset.\n",
        "2. Store this information in a Python dictionary or a 2D array (tensor) for efficient access and manipulation.\n",
        "\n",
        "## Visualizing Bigram Counts with PyTorch\n",
        "\n",
        "1. Import PyTorch, a deep learning framework that provides efficient multi-dimensional arrays called tensors.\n",
        "2. Store the bigram counts in a 2D tensor (`n`).\n",
        "3. Visualize the tensor using the Matplotlib library to better understand the distribution of bigrams.\n",
        "\n",
        "## Improving the Model Representation\n",
        "\n",
        "1. Simplify the representation by using a single special token (`.`) instead of separate start and end tokens.\n",
        "2. Adjust the model to accommodate this change, making the implementation more efficient and easier to interpret.\n",
        "\n",
        "This lecture provides foundational knowledge on how to construct a basic character-level language model using bigrams and how to visualize the underlying structure of the data. As we progress, we will delve into more complex models, such as recurrent neural networks and transformers, to capture deeper relationships within the data.\n",
        "\n",
        "---\n",
        "\n",
        "# Sampling from a Bigram Character Level Language Model\n",
        "\n",
        "## Introduction\n",
        "\n",
        "A bigram character level language model provides us the probability of occurrence of a character given the previous character. The lecturer explains how to sample from such a model using an array of counts and demonstrates the importance of understanding tensor broadcasting in PyTorch.\n",
        "\n",
        "## Creating the Model Array\n",
        "\n",
        "- The array contains counts of occurrences of characters following each other.\n",
        "- The first row corresponds to the start token (denoted as `.`) and gives the counts of each character that could be the start of a word.\n",
        "- To sample from the model, one would start with the start token, observe the counts in the row corresponding to this token, and determine the next character based on these counts.\n",
        "\n",
        "## Normalizing Counts to Probabilities\n",
        "\n",
        "- For sampling, raw counts are not enough; we need to convert them into probabilities.\n",
        "- To obtain probabilities, one has to normalize the counts such that the sum of probabilities in each row equals 1.\n",
        "  \n",
        "  $$\n",
        "  p = \\frac{n}{\\text{sum}(n)}\n",
        "  $$\n",
        "\n",
        "## Sampling Using PyTorch's Multinomial Function\n",
        "\n",
        "- To sample according to a given probability distribution, PyTorch's `torch.multinomial` function is employed.\n",
        "- This function returns samples from a multinomial probability distribution.\n",
        "- A generator object in PyTorch can be used to make the random sampling deterministic.\n",
        "\n",
        "## Issues with a Bigram Model\n",
        "\n",
        "- Sampling from a bigram model can produce nonsensical results because the model only considers the immediate previous character.\n",
        "- Despite its simplicity and speed, bigram models might not generate realistic sequences.\n",
        "\n",
        "## Broadcasting in PyTorch\n",
        "\n",
        "### Importance of Broadcasting\n",
        "\n",
        "- Broadcasting allows PyTorch to perform operations on tensors of different shapes by automatically expanding the smaller tensor to match the shape of the larger tensor.\n",
        "- It's crucial to understand broadcasting rules to avoid potential pitfalls.\n",
        "\n",
        "### Broadcasting Rules\n",
        "\n",
        "1. Each tensor must have at least one dimension.\n",
        "2. Starting from the trailing dimension, the dimension sizes must either:\n",
        "   - Be equal.\n",
        "   - One of them is 1.\n",
        "   - One of them doesn't exist.\n",
        "\n",
        "### Common Pitfall\n",
        "\n",
        "- When performing operations like division between two tensors, broadcasting might replicate rows or columns in unexpected ways, leading to incorrect results.\n",
        "- The `keepdim` argument in functions like `torch.sum` can be critical to ensure the desired broadcasting behavior.\n",
        "\n",
        "## Efficiency Tips\n",
        "\n",
        "- In-place operations (`/=`) can be more efficient than creating new tensors.\n",
        "- Precompute and store normalized probabilities to avoid redundant calculations during sampling.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Sampling from a bigram character level language model provides a hands-on understanding of how character sequences can be generated based on learned probabilities. While the bigram approach is simple, its outputs might not always be realistic. Proper understanding of tensor operations, especially broadcasting in PyTorch, is essential for correct and efficient model implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## Bi-gram Language Model\n",
        "\n",
        "### Overview\n",
        "A bi-gram language model predicts the next character in a sequence based on the current character. It's trained by counting the frequency of character pairings and normalizing to get a probability distribution. The elements of the probability distribution are the model parameters, summarizing the statistics of the bigrams.\n",
        "\n",
        "### Training the Model\n",
        "To train the model:\n",
        "1. Parse through the dataset, counting the occurrence of every character pairing.\n",
        "2. Normalize these counts to obtain a probability distribution.\n",
        "\n",
        "### Evaluating the Model: Negative Log Likelihood\n",
        "To evaluate the quality of the trained model:\n",
        "1. Compute the **likelihood** which is the product of the probabilities assigned by the model.\n",
        "2. Maximize the likelihood to get a measure of model quality.\n",
        "3. To make calculations easier, compute the **log likelihood** by taking the logarithm of the likelihood.\n",
        "4. Since we want a loss function where lower values are better, use the **negative log likelihood** (NLL). It's the negative of the log likelihood.\n",
        "5. For practical purposes, it's common to use the average NLL.\n",
        "\n",
        "### Model Smoothing\n",
        "A common problem in bi-gram models is encountering bigrams in the test data that were not present in the training data, leading to a probability of zero. To address this:\n",
        "1. Implement **model smoothing**.\n",
        "2. Add fake counts (like adding 1) to each bi-gram count.\n",
        "3. This ensures no zero probabilities and avoids the issue of infinite negative log likelihood.\n",
        "\n",
        "## Neural Network Approach\n",
        "\n",
        "### Introduction\n",
        "The neural network approach casts the bi-gram character-level language modeling into the neural network framework. The network receives a character as input and outputs a probability distribution over the next character.\n",
        "\n",
        "### Creating the Dataset for Neural Network\n",
        "1. Parse the dataset, generating bigrams.\n",
        "2. Each bigram consists of two parts: an input (the current character) and a target (the next character).\n",
        "3. Convert these characters into integers and construct tensors using `torch.tensor` (prefer lowercase for integer type).\n",
        "\n",
        "### Note on Tensor Construction in PyTorch\n",
        "- Be cautious when using PyTorch APIs. Both `torch.tensor` (lowercase t) and `torch.Tensor` (uppercase T) can be used to construct tensors.\n",
        "- The main difference is in data type determination. The lowercase version is recommended as it automatically infers the data type.\n",
        "\n",
        "Remember, when working with neural networks or any other machine learning models, always read documentation carefully and be mindful of the tools and functions you're using.\n",
        "\n",
        "---\n",
        "\n",
        "### Feeding Integers into Neural Networks: One-Hot Encoding\n",
        "\n",
        "When working with neural networks, we can't directly input integers. This is because the inputs undergo multiplicative operations with weights, making it nonsensical for them to take integer values. A common solution is to use **one-hot encoding**:\n",
        "- One-hot encoding converts an integer into a vector that is all zeros, except for the integer's index which is set to one.\n",
        "- For example, an integer 13 in a 27-class system would be converted into a vector that's all zeros except for the 13th position, which is a one.\n",
        "- Libraries like PyTorch have built-in functions (`one_hot`) to perform this encoding.\n",
        "\n",
        "### Constructing a Basic Neural Network\n",
        "\n",
        "Neurons in a neural network perform a simple function, \\( wx + b \\), where:\n",
        "- \\( w \\) are the weights.\n",
        "- \\( x \\) is the input.\n",
        "- \\( b \\) is the bias.\n",
        "\n",
        "However, in our case:\n",
        "- We focus on a single linear layer without biases and non-linearities.\n",
        "- The aim is to convert 27-dimensional inputs (from one-hot encoding) to 27 outputs (probabilities for each class).\n",
        "\n",
        "### From Neural Net Outputs to Probabilities: The Softmax\n",
        "\n",
        "After feeding inputs into the network, we get outputs which may be positive or negative values. However, we want these outputs to represent probabilities. Probabilities are:\n",
        "- Positive.\n",
        "- Sum up to 1.\n",
        "\n",
        "To achieve this, we use the **softmax** operation:\n",
        "1. Interpret the neural net outputs as \"log counts\" or logits.\n",
        "2. Exponentiate these logits to get something resembling counts.\n",
        "3. Normalize these counts to produce probabilities.\n",
        "\n",
        "The softmax operation ensures that the neural network's output is always between 0 and 1, and the total sums up to 1, making them interpretable as probabilities.\n",
        "\n",
        "### Overview and Connection to Previous Knowledge\n",
        "\n",
        "The process described above is analogous to neural network operations in other contexts, such as in the micrograd framework:\n",
        "- There's a forward pass where inputs are fed through the network.\n",
        "- A loss function computes how far off the network's predictions are from the true values.\n",
        "- Gradients are back-propagated to adjust the network's weights.\n",
        "- This iterative process continues until the network's predictions are satisfactory or the loss reaches a desired level.\n",
        "\n",
        "**Key Takeaway**: This lecture covers the fundamentals of preparing integer data for neural networks using one-hot encoding, designing a basic network, and converting the outputs into meaningful probabilities using the softmax operation. The approach aligns with general neural network practices, emphasizing the importance of data representation, network design, and the iterative process of training.\n",
        "\n",
        "---\n",
        "\n",
        "# Bigram Character Level Language Model\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- A bigram character level language model predicts the next character given the previous one. It is foundational to understanding more complex models like transformers.\n",
        "\n",
        "## Two Approaches to Training:\n",
        "\n",
        "1. **Frequency Count Approach**:\n",
        "    - Calculate the frequency of each character following another character.\n",
        "    - Normalize the counts to get the probability distribution.\n",
        "    - This is straightforward and uses explicit counting.\n",
        "\n",
        "2. **Gradient-based Neural Network Approach**:\n",
        "    - This involves training a simple neural network to predict the next character.\n",
        "    - While more complex than direct counting, it offers flexibility and scalability.\n",
        "\n",
        "## Neural Network Framework:\n",
        "\n",
        "### Forward Pass:\n",
        "\n",
        "- **Input**:\n",
        "    - The input is a one-hot encoded vector representing the previous character.\n",
        "- **Processing**:\n",
        "    - The input vector is passed through a linear layer (a weight matrix) to get the logits.\n",
        "    - The logits are then passed through a softmax function to get the probability distribution of the next character.\n",
        "- **Output**:\n",
        "    - The probability distribution of the next character.\n",
        "\n",
        "### Loss Calculation:\n",
        "\n",
        "- The loss used here is the \"Negative Log Likelihood\" since this is a classification task.\n",
        "- The aim is to maximize the likelihood of the correct next character.\n",
        "\n",
        "### Backward Pass:\n",
        "\n",
        "- PyTorch keeps track of all operations in the forward pass and builds a computational graph.\n",
        "- The `.backward()` function calculates the gradients of all the parameters with respect to the loss.\n",
        "\n",
        "### Weight Update:\n",
        "\n",
        "- The weights (or parameters) of the neural network are updated using the calculated gradients.\n",
        "- This is achieved using Gradient Descent.\n",
        "\n",
        "### Regularization:\n",
        "\n",
        "- Regularization can be introduced by adding a penalty to the weights.\n",
        "- This is equivalent to \"smoothing\" in the frequency count approach and helps prevent overfitting.\n",
        "- Regularizing towards zero makes predictions more uniform.\n",
        "\n",
        "## Sampling from the Model:\n",
        "\n",
        "- To generate sequences, we can sample from the model's output probability distribution.\n",
        "- This allows us to generate likely sequences of characters based on the trained model.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "- While the frequency count and gradient-based approaches yield similar results for bigram models, the gradient-based approach is more scalable and flexible.\n",
        "- This foundational knowledge will be expanded upon in future lectures, where the simple neural network will evolve into more complex architectures like transformers.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hI5gw9Ti5qj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing and First Layer"
      ],
      "metadata": {
        "id": "VqvEmxLh3Hab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsFBeB_I6_6V",
        "outputId": "b41e9654-0a6d-4b52-e630-adc4d58dab91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-18 07:54:11--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-08-18 07:54:11 (7.07 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "csV1rJDF7SCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "opV_0uAu7eNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvTxz8kUt_Yc",
        "outputId": "4692f901-e181-4d40-9add-53c9b51f582c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb3DhRwduDKH",
        "outputId": "a6334ac8-6bb6-46b7-eeed-7d634a7af190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26,\n",
              " '.': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq4nejt7uGU2",
        "outputId": "f38d9426-cd88-42c6-8764-db3068d7fe6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'a',\n",
              " 2: 'b',\n",
              " 3: 'c',\n",
              " 4: 'd',\n",
              " 5: 'e',\n",
              " 6: 'f',\n",
              " 7: 'g',\n",
              " 8: 'h',\n",
              " 9: 'i',\n",
              " 10: 'j',\n",
              " 11: 'k',\n",
              " 12: 'l',\n",
              " 13: 'm',\n",
              " 14: 'n',\n",
              " 15: 'o',\n",
              " 16: 'p',\n",
              " 17: 'q',\n",
              " 18: 'r',\n",
              " 19: 's',\n",
              " 20: 't',\n",
              " 21: 'u',\n",
              " 22: 'v',\n",
              " 23: 'w',\n",
              " 24: 'x',\n",
              " 25: 'y',\n",
              " 26: 'z',\n",
              " 0: '.'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "3dAsZyBU79kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experimenting towards a solution\n",
        "\n",
        "# iterate over the bigrams to create the training set of bigrams (x,y)\n",
        "# we don't do counts here\n",
        "# the training set will be 2 lists, the inputs and the targets.\n",
        "# both are integers.\n",
        "# we want tensors as the output\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    print(ch1, ch2)\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v_bN-RG7ZF4",
        "outputId": "46afee6a-2162-4096-a122-63dceea921e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". e\n",
            "e m\n",
            "m m\n",
            "m a\n",
            "a .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGIbtS2Z8HHU",
        "outputId": "4568eca0-f61a-4dc2-fb4b-540dd10e0361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w97XitH1xoxd",
        "outputId": "e92aa4d7-4002-4ca2-b1e9-a3f631ec3e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJODGztLxq20",
        "outputId": "ad044dad-7261-45e7-90f6-0787295f052d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e63yOulX8It0",
        "outputId": "25b328ed-c60d-426c-9f4c-0fccbeeaa364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s62rEk01wUWo",
        "outputId": "2532d319-5213-4596-911f-f8e2f6d5b39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation\n",
        "\n",
        "We are preparing the input data (`xs`) and target labels (`ys`) for training a neural network on a character-level language model. The neural network will learn the relationships between characters in a sequence and predict the next character given the current character.\n",
        "\n",
        "Here's how the data preparation works, step by step:\n",
        "\n",
        "1. We start with a single word from the training data, which is represented as a list of characters (e.g., `['e', 'm', 'm', 'a']`).\n",
        "2. We prepend and append a special character `'.'` to the list, representing the start and end of the word.\n",
        "3. We iterate through the characters in the modified list, taking pairs of adjacent characters (`ch1`, `ch2`). For each pair, `ch1` is the current character and `ch2` is the next character.\n",
        "4. We convert the characters `ch1` and `ch2` into integer indices (`ix1`, `ix2`) using the `stoi` dictionary, which maps characters to unique integer indices.\n",
        "5. We append the index `ix1` to the input data `xs` and the index `ix2` to the target labels `ys`.\n",
        "6. Finally, we convert the lists `xs` and `ys` into PyTorch tensors.\n",
        "\n",
        "Now let's take a closer look at the example word `'emma'`:\n",
        "\n",
        "- When the input is 0 (representing `'.'`), the desired output is 5 (representing `'e'`).\n",
        "- When the input is 5 (`'e'`), we want the model to predict 13 (representing `'m'`).\n",
        "- When the input is 13 (`'m'`), we want the model to predict 13 (`'m'`) again.\n",
        "- When the input is 13 (`'m'`), we want the model to predict 1 (representing `'a'`).\n",
        "- When the input is 1 (`'a'`), we want the model to predict 0 (representing `'.'`).\n",
        "\n",
        "In summary, we are preparing the input data and target labels so that the neural network can learn the relationships between characters in the training data. Given an input character, the model will learn to predict the next character in the sequence.\n",
        "\n",
        "## Question:\n",
        "\n",
        "How are we going to feed in these examples to a neural network?\n",
        "\n",
        "## Answer\n",
        "\n",
        "One-hot encoding\n"
      ],
      "metadata": {
        "id": "RZOn8ajl93oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we cast to a float here. nn's want floats not ints.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=27).float()\n",
        "xenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcXSmMsA_c7k",
        "outputId": "b7d2e689-be07-41f6-d911-43a869a2e4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvNuIhBayA8t",
        "outputId": "be8c48db-1553-43e7-e3ee-5da940643fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYAoE4ZR_fk_",
        "outputId": "784b7b0b-ae92-4d83-f393-8c64d6df5e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first array, `xs`, is a one-dimensional tensor with 5 entries. In PyTorch, one-dimensional tensors are often referred to as \"vectors.\" In this case, `xs` is a vector with shape `(5,)`, which means it has 5 elements along its only axis. When you print it, it's displayed horizontally, but it's still a one-dimensional array.\n",
        "\n",
        "The second array, `xenc`, is a two-dimensional tensor with shape `(5, 27)`. In PyTorch, two-dimensional tensors are often referred to as \"matrices.\" In this case, `xenc` is a matrix with 5 rows and 27 columns. When you print it, it's displayed vertically with each row on a new line. This is the standard way to display matrices.\n",
        "\n",
        "To summarize:\n",
        "\n",
        "- `xs` is a one-dimensional tensor (or vector) with shape `(5,)`.\n",
        "- `xenc` is a two-dimensional tensor (or matrix) with shape `(5, 27)`.\n",
        "\n",
        "The terms \"one-dimensional tensor\" and \"vector\" can be used interchangeably, as can \"two-dimensional tensor\" and \"matrix.\""
      ],
      "metadata": {
        "id": "b4OEdD0E1tYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the `torch.Size` object is a subclass of the Python `tuple` class, but it's customized to display more compactly when printed. Instead of displaying as `(5,)`, it displays as `[5]`. This is purely a matter of display formatting.\n",
        "\n",
        "Under the hood, the shape of `xs` is still a tuple with one element, representing a one-dimensional tensor with 5 entries. When you see `torch.Size([5])`, you can interpret it as a shape of `(5,)` in standard tuple notation."
      ],
      "metadata": {
        "id": "0Knt69yF2ZJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSTlzpj02z4r",
        "outputId": "6efd0ca6-c21b-44f9-8cf1-c1aa62fd65d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(xenc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "NYYjoilr_qF-",
        "outputId": "55bce4a8-6a3c-4123-94ba-b287faaee15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bc82374f970>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the initial weights for the neuron\n",
        "W = torch.randn((27,1))"
      ],
      "metadata": {
        "id": "nxbIQkIf3LTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkGoxV1S3p80",
        "outputId": "175d6114-6b84-47c2-8f53-dbe4ac65e11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5392],\n",
              "        [-0.1753],\n",
              "        [ 0.8747],\n",
              "        [-0.6604],\n",
              "        [ 2.8611],\n",
              "        [-0.3775],\n",
              "        [ 0.2980],\n",
              "        [ 1.6915],\n",
              "        [ 0.1661],\n",
              "        [-0.4401],\n",
              "        [ 0.5906],\n",
              "        [-1.3598],\n",
              "        [-1.3202],\n",
              "        [-0.1002],\n",
              "        [ 1.0313],\n",
              "        [-1.0032],\n",
              "        [ 0.2084],\n",
              "        [-0.7773],\n",
              "        [-2.2099],\n",
              "        [ 0.1356],\n",
              "        [ 0.4989],\n",
              "        [ 0.1801],\n",
              "        [ 0.0301],\n",
              "        [ 1.2996],\n",
              "        [ 0.8184],\n",
              "        [-1.0010],\n",
              "        [ 0.3535]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,1))\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiPKsByU30Fr",
        "outputId": "ab973b11-4aea-43a6-cd31-13cb1500265c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8766],\n",
              "        [-1.0752],\n",
              "        [-0.8432],\n",
              "        [-0.8432],\n",
              "        [-0.1038]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5, 27) @ (27, 1) dot product (5, 1)"
      ],
      "metadata": {
        "id": "QYY30CkZ4Bp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 activations of this neuron on these 5 inputs\n",
        "# evaluated all in parallel\n",
        "# we fed in simultaneously all the 5 inputs"
      ],
      "metadata": {
        "id": "ccOFNxfd4z9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single input to single neuron, single activation\n",
        "xenc[0] @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkS0reRaCR_j",
        "outputId": "46c12914-aac6-46e8-b0f6-257a4d351182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.8766])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 inputs to single neuron, 5 activations, 1 for each input\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gEVpBHjCuPz",
        "outputId": "1d3f6972-ff30-4ff8-cde8-4187fad7ce5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8766],\n",
              "        [-1.0752],\n",
              "        [-0.8432],\n",
              "        [-0.8432],\n",
              "        [-0.1038]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:  Single neuron with weights (W)\n",
        "\n",
        "1. **Definition of a Neuron**: A neuron is a basic building block of neural networks. It takes one or more inputs, applies a weight to each input, sums the weighted inputs, and then passes the sum through an activation function to produce an output.\n",
        "\n",
        "2. **Weight Matrix \\(W\\)**: In the code, you have a weight matrix \\(W\\) of shape \\(27 \\times 1\\). Each row of this matrix corresponds to a weight associated with a particular character in the input vocabulary.\n",
        "\n",
        "3. **Input Matrix \\(xenc\\)**: The input matrix \\(xenc\\) has been one-hot encoded, which means each row of this matrix represents an input character in a format where only one element is 1 (indicating the presence of the character) and the rest are 0 (indicating the absence of the character). The matrix has shape \\(5 \\times 27\\), corresponding to 5 input characters and 27 possible characters in the vocabulary.\n",
        "\n",
        "4. **Matrix Multiplication**: The operation \\(xenc @ W\\) is a matrix multiplication. It multiplies the input matrix \\(xenc\\) with the weight matrix \\(W\\) to produce an output matrix of shape \\(5 \\times 1\\). This operation calculates the weighted sum of inputs for each input character.\n",
        "\n",
        "5. **Output**: The resulting matrix represents the output (also known as activations) of the neuron for each input character. Each element in this matrix corresponds to the output of the neuron for a specific input character. This is the result of evaluating the neuron on the 5 input characters simultaneously.\n",
        "\n",
        "6. **Parallel Evaluation**: Since the inputs are fed into the neuron simultaneously, the matrix multiplication allows us to compute the outputs in parallel, which can be more efficient than computing them one by one.\n",
        "\n",
        "To summarize, this code defines a single neuron with weights \\(W\\). It takes 5 input characters, represented in a one-hot encoded format, and computes the output of the neuron for each input character using matrix multiplication. The output represents the activations of the neuron for each input character. This is done in parallel for all 5 inputs."
      ],
      "metadata": {
        "id": "mjOBrG0k7MOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,27))\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHUYpIo_9Pb8",
        "outputId": "401f586c-8274-461d-9fc1-912e8825ae7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5436,  0.4000, -0.8983,  0.5787, -1.3954, -1.5012,  0.9266, -0.1598,\n",
              "          0.3028,  1.6622, -0.7582,  0.6804, -1.3113,  0.7485, -0.9420, -1.1925,\n",
              "          0.7775, -1.1850,  0.1783, -0.0528,  0.9542,  1.3119,  0.1010,  1.1518,\n",
              "          0.3347,  0.9267, -0.0030],\n",
              "        [ 0.0452,  0.8436,  0.2099, -0.6964, -0.2445,  0.1827, -0.9503, -1.5473,\n",
              "         -1.2381, -0.0067, -0.1173, -0.4540,  0.5134, -0.1926, -0.4810,  0.5868,\n",
              "         -1.0153,  1.2164,  0.8127, -1.6983, -0.1393, -1.5653,  0.6500, -0.7030,\n",
              "         -0.0024,  1.9544, -1.1859],\n",
              "        [-0.6580,  1.0054,  0.6433,  2.0007,  0.2147, -0.1760,  0.9988,  0.7172,\n",
              "         -0.3547,  0.1579,  0.7393, -0.1603, -0.2503, -0.4495, -0.3824,  0.3799,\n",
              "         -0.2560, -0.9885, -0.6910,  0.4727, -0.2704, -1.4679,  0.9980,  0.3444,\n",
              "          2.0468, -1.0425, -2.1539],\n",
              "        [-0.6580,  1.0054,  0.6433,  2.0007,  0.2147, -0.1760,  0.9988,  0.7172,\n",
              "         -0.3547,  0.1579,  0.7393, -0.1603, -0.2503, -0.4495, -0.3824,  0.3799,\n",
              "         -0.2560, -0.9885, -0.6910,  0.4727, -0.2704, -1.4679,  0.9980,  0.3444,\n",
              "          2.0468, -1.0425, -2.1539],\n",
              "        [ 0.3937,  1.0987,  0.3520,  0.8874, -0.7043, -0.0204, -1.3189, -1.2199,\n",
              "          1.0506,  1.4618,  2.4095,  2.3610,  0.3994, -0.4895,  1.3026,  1.6222,\n",
              "         -0.9765,  0.1961, -0.0878,  0.8536, -0.3050, -0.3900, -0.7056,  1.0118,\n",
              "         -0.5171, -0.2348, -0.8543]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (5, 27) @ (27, 27) dot product (5, 27)"
      ],
      "metadata": {
        "id": "uncSkUOX9c1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(xenc @ W).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFK7EpR2-bKV",
        "outputId": "4654bd41-cc2b-4169-f54d-0829f63199cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# individual calculation of dot product\n",
        "(xenc @ W)[3,13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2yywwHjDnE6",
        "outputId": "554eecb0-a101-4368-9312-f1efc057ee29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.4495)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xenc[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhzAhTS8DImh",
        "outputId": "cfad6d34-1bcd-47d1-c2ea-f8b0cbfabb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[:,13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKpxrpesDStx",
        "outputId": "cebf754b-2454-41c3-b5ff-95c44485f78c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.7485, -0.4895, -0.7696, -1.4412,  1.3454, -0.1926, -0.6399, -1.1842,\n",
              "        -1.7606, -0.8529,  0.3229,  0.8511, -3.2818, -0.4495,  1.5899, -1.6332,\n",
              "        -0.4364, -2.7047, -1.1353, -3.0854,  1.3822,  0.8943, -0.3202, -1.7449,\n",
              "        -0.4577,  0.5247,  0.1901])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# elementwise multiplication and sum it up\n",
        "(xenc[3] * W[:,13]).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt6PsNb6EMK6",
        "outputId": "fd3d5bc9-2dc4-4ac6-a836-ea9cd93a5e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.4495)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation: Multi-Neuron System\n",
        "\n",
        "1. **Multi-Neuron System**: In the new code, you've switched from having a single neuron to a system of 27 neurons, each with 27 weights. The weight matrix \\(W\\) is now of shape \\($27 \\times 27$\\), where each row represents the weights associated with a specific neuron.\n",
        "\n",
        "2. **Weight Matrix \\(W\\)**: Each row of the weight matrix \\(W\\) corresponds to a set of weights for a specific neuron. There are 27 rows, which means there are 27 neurons in this layer, and each neuron has 27 weights corresponding to the 27 possible input characters.\n",
        "\n",
        "3. **Input Matrix \\(xenc\\)**: The input matrix \\(xenc\\) remains the same, with a shape of \\($5 \\times 27\\$). Each row represents a one-hot encoded input character.\n",
        "\n",
        "4. **Matrix Multiplication**: The operation \\(xenc @ W\\) is still a matrix multiplication. It multiplies the input matrix \\(xenc\\) with the weight matrix \\(W\\) to produce an output matrix of shape \\($5 \\times 27$\\). This operation calculates the weighted sum of inputs for each input character across all neurons.\n",
        "\n",
        "5. **Output**: The resulting matrix represents the outputs of all 27 neurons for each input character. Each row corresponds to an input character, and each column corresponds to a neuron. So, for example, the element at position (i, j) represents the output of the j-th neuron for the i-th input character.\n",
        "\n",
        "6. **Parallel Evaluation**: Yes, parallel processing is a key feature of neural networks. Modern neural networks can have millions of parameters and be trained on massive datasets. Being able to process multiple inputs or multiple neurons in parallel significantly speeds up training and inference. This is one of the reasons why GPUs, which are designed for parallel processing, are often used for training and running neural networks.\n",
        "\n",
        "7. **Multiple Neurons**: Having multiple neurons allows the network to learn more complex patterns and representations in the data. Each neuron can learn to respond to different features in the input, and together they can capture a richer understanding of the data.\n",
        "\n",
        "To summarize, the new code defines a layer of 27 neurons, each with 27 weights. It takes 5 input characters, represented in a one-hot encoded format, and computes the output of all 27 neurons for each input character using matrix multiplication. The output matrix represents the activations of all neurons for each input character. This is done in parallel for all 5 inputs and all 27 neurons. Parallel processing is a key feature of neural networks that makes them computationally efficient."
      ],
      "metadata": {
        "id": "o8uNxLi59_Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation: Processing data in parallel\n",
        "\n",
        "1. **Dot Product**: The dot product between two vectors is the sum of the element-wise products of the vectors. In the context of neural networks, it's used to calculate the weighted sum of the inputs for a neuron.\n",
        "\n",
        "2. **Matrix Multiplication**: Matrix multiplication can be thought of as a series of dot products. When you multiply two matrices, each element in the resulting matrix is the dot product of a row from the first matrix and a column from the second matrix. This operation can be used to compute the weighted sums for multiple neurons and multiple input examples simultaneously.\n",
        "\n",
        "3. **Batch Processing**: Neural networks often process data in batches, which means that multiple input examples are fed into the network at once. This allows for more efficient computation because the operations can be parallelized.\n",
        "\n",
        "4. **Neurons and Weights**: Each neuron in a neural network has a set of weights associated with it. The weights determine how much each input feature contributes to the output of the neuron. The weighted sum of the inputs is then passed through an activation function to produce the output of the neuron.\n",
        "\n",
        "5. **Matrix Representation of Weights**: In a fully connected layer of a neural network, the weights for all the neurons can be represented in a single matrix, where each column corresponds to the weights for a specific neuron.\n",
        "\n",
        "6. **Efficient Computation**: Matrix multiplication is a very efficient operation, especially on modern hardware like GPUs that are optimized for it. By representing the weights in a matrix and the input examples in another matrix, we can use matrix multiplication to compute the outputs of all the neurons for all the input examples in the batch simultaneously. This is much more efficient than computing them one at a time.\n",
        "\n",
        "7. **Matrix Multiplication in our Example**: In the code, \\(xenc\\) is a matrix where each row represents a one-hot encoded input character, and \\(W\\) is a matrix where each column represents the weights for a neuron. The operation \\(xenc @ W\\) computes the dot product between each input example and each neuron's weights. The resulting matrix represents the weighted sums (before activation) for each neuron and each input example.\n",
        "\n",
        "Using matrix multiplication, we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons where all those neurons have weights in the columns of those \\(W\\)'s.\n",
        "\n",
        "This ability to process data in parallel and perform efficient computations is one of the key reasons why neural networks are so powerful and versatile."
      ],
      "metadata": {
        "id": "PLth_wiYBtXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,27))\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGfS1g3xHuxr",
        "outputId": "e79d5dfb-42c1-448f-a75e-66b0bdcf8aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.4212, -0.5426, -1.7280,  0.9467, -0.3676, -1.2058, -0.8297,  1.2202,\n",
              "         -0.2394,  0.9902, -0.0166, -0.3268, -1.9340,  1.9481, -1.3969,  1.8343,\n",
              "          1.3558,  0.7375,  1.3718,  0.1089,  1.3102,  1.4131, -0.1217, -0.4301,\n",
              "         -0.4711, -0.0763, -0.6519],\n",
              "        [-0.8395,  0.1354, -0.3265, -0.6137, -2.1645,  1.2188, -0.6012, -0.2334,\n",
              "          0.1413, -0.0087, -0.0271,  0.0778,  0.2831,  0.2748, -1.1507,  0.2663,\n",
              "         -0.4083, -1.4141,  0.1477,  0.0858,  0.6381, -0.3847,  0.8881,  0.5601,\n",
              "          0.8998,  1.0595, -0.8282],\n",
              "        [ 0.3255, -0.8238, -0.0985, -0.8890, -1.2165, -0.8277,  0.4725,  0.7730,\n",
              "          0.1841, -0.5590, -0.2694,  2.0042, -0.7195, -1.0370,  0.7979, -0.8972,\n",
              "          0.2113, -1.1599,  0.7428,  1.1976, -0.0880, -0.9479,  0.7794,  0.6005,\n",
              "          0.3533,  0.5767,  0.0509],\n",
              "        [ 0.3255, -0.8238, -0.0985, -0.8890, -1.2165, -0.8277,  0.4725,  0.7730,\n",
              "          0.1841, -0.5590, -0.2694,  2.0042, -0.7195, -1.0370,  0.7979, -0.8972,\n",
              "          0.2113, -1.1599,  0.7428,  1.1976, -0.0880, -0.9479,  0.7794,  0.6005,\n",
              "          0.3533,  0.5767,  0.0509],\n",
              "        [ 1.9351,  0.2952, -0.6005, -0.2119,  0.1760, -0.7676,  1.3465, -0.6572,\n",
              "          0.5087, -2.0158, -0.2945,  0.7351,  0.2773, -1.1327, -0.3642, -0.7206,\n",
              "          0.4205, -1.3497,  1.8438,  0.3655,  1.0107,  1.1165, -0.6222,  0.3172,\n",
              "          1.7475, -0.7240, -0.6513]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp gives the log counts. negative number are below 1. postive number above 1.\n",
        "# we can use and interpret these as the equivalent of counts in the N matrix\n",
        "# probabilies will be these counts normalized\n",
        "(xenc @ W).exp()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_8pnCSHyRQ",
        "outputId": "1d6b035b-d7ac-4fff-e5a1-9b9e6ae74426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0888, 0.5812, 0.1776, 2.5773, 0.6924, 0.2995, 0.4362, 3.3877, 0.7871,\n",
              "         2.6917, 0.9836, 0.7213, 0.1446, 7.0151, 0.2474, 6.2606, 3.8799, 2.0906,\n",
              "         3.9424, 1.1151, 3.7070, 4.1087, 0.8854, 0.6505, 0.6243, 0.9265, 0.5211],\n",
              "        [0.4319, 1.1450, 0.7215, 0.5413, 0.1148, 3.3830, 0.5481, 0.7918, 1.1517,\n",
              "         0.9914, 0.9732, 1.0809, 1.3273, 1.3162, 0.3164, 1.3052, 0.6648, 0.2431,\n",
              "         1.1591, 1.0896, 1.8929, 0.6806, 2.4305, 1.7508, 2.4592, 2.8850, 0.4368],\n",
              "        [1.3847, 0.4388, 0.9062, 0.4111, 0.2963, 0.4370, 1.6040, 2.1663, 1.2022,\n",
              "         0.5718, 0.7638, 7.4199, 0.4870, 0.3545, 2.2209, 0.4077, 1.2353, 0.3135,\n",
              "         2.1017, 3.3121, 0.9158, 0.3876, 2.1801, 1.8230, 1.4238, 1.7801, 1.0522],\n",
              "        [1.3847, 0.4388, 0.9062, 0.4111, 0.2963, 0.4370, 1.6040, 2.1663, 1.2022,\n",
              "         0.5718, 0.7638, 7.4199, 0.4870, 0.3545, 2.2209, 0.4077, 1.2353, 0.3135,\n",
              "         2.1017, 3.3121, 0.9158, 0.3876, 2.1801, 1.8230, 1.4238, 1.7801, 1.0522],\n",
              "        [6.9250, 1.3433, 0.5485, 0.8090, 1.1924, 0.4641, 3.8441, 0.5183, 1.6631,\n",
              "         0.1332, 0.7449, 2.0857, 1.3195, 0.3222, 0.6947, 0.4864, 1.5227, 0.2593,\n",
              "         6.3205, 1.4412, 2.7475, 3.0541, 0.5368, 1.3733, 5.7400, 0.4848, 0.5214]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp gives the log counts. negative number are below 1. postive number above 1.\n",
        "# we can use and interpret these as the equivalent of counts in the N matrix\n",
        "# probabilies will be these counts normalized\n",
        "logits = (xenc @ W)\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims = True)\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlgbcnI_I4h5",
        "outputId": "776893c4-0ab5-4c9e-e73f-59b9152bc449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0018, 0.0117, 0.0036, 0.0520, 0.0140, 0.0060, 0.0088, 0.0684, 0.0159,\n",
              "         0.0543, 0.0199, 0.0146, 0.0029, 0.1416, 0.0050, 0.1264, 0.0783, 0.0422,\n",
              "         0.0796, 0.0225, 0.0748, 0.0829, 0.0179, 0.0131, 0.0126, 0.0187, 0.0105],\n",
              "        [0.0136, 0.0360, 0.0227, 0.0170, 0.0036, 0.1063, 0.0172, 0.0249, 0.0362,\n",
              "         0.0311, 0.0306, 0.0340, 0.0417, 0.0413, 0.0099, 0.0410, 0.0209, 0.0076,\n",
              "         0.0364, 0.0342, 0.0595, 0.0214, 0.0764, 0.0550, 0.0773, 0.0906, 0.0137],\n",
              "        [0.0368, 0.0117, 0.0241, 0.0109, 0.0079, 0.0116, 0.0427, 0.0576, 0.0320,\n",
              "         0.0152, 0.0203, 0.1974, 0.0130, 0.0094, 0.0591, 0.0108, 0.0329, 0.0083,\n",
              "         0.0559, 0.0881, 0.0244, 0.0103, 0.0580, 0.0485, 0.0379, 0.0473, 0.0280],\n",
              "        [0.0368, 0.0117, 0.0241, 0.0109, 0.0079, 0.0116, 0.0427, 0.0576, 0.0320,\n",
              "         0.0152, 0.0203, 0.1974, 0.0130, 0.0094, 0.0591, 0.0108, 0.0329, 0.0083,\n",
              "         0.0559, 0.0881, 0.0244, 0.0103, 0.0580, 0.0485, 0.0379, 0.0473, 0.0280],\n",
              "        [0.1470, 0.0285, 0.0116, 0.0172, 0.0253, 0.0099, 0.0816, 0.0110, 0.0353,\n",
              "         0.0028, 0.0158, 0.0443, 0.0280, 0.0068, 0.0148, 0.0103, 0.0323, 0.0055,\n",
              "         0.1342, 0.0306, 0.0583, 0.0648, 0.0114, 0.0292, 0.1219, 0.0103, 0.0111]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-hHxwhaJXMz",
        "outputId": "1699db01-d7f7-4323-b5e3-fb2761ba39e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMHNYWZAJdNb",
        "outputId": "21faba8f-cc95-4582-a36d-05da5fceed4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each of our 5 example we now have a row that came out of a neural network.\n",
        "# We can interpret the rows as probabilities."
      ],
      "metadata": {
        "id": "q32BEr0nKGxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfrlwjEMLZC3",
        "outputId": "c7692d59-cc27-4884-ab2b-0e4b1050ce88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0018, 0.0117, 0.0036, 0.0520, 0.0140, 0.0060, 0.0088, 0.0684, 0.0159,\n",
              "        0.0543, 0.0199, 0.0146, 0.0029, 0.1416, 0.0050, 0.1264, 0.0783, 0.0422,\n",
              "        0.0796, 0.0225, 0.0748, 0.0829, 0.0179, 0.0131, 0.0126, 0.0187, 0.0105])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation: From the raw outputs of the neurons to the probabilities that we're interested in\n",
        "\n",
        "1. **Neurons Output Numbers**: Right now, our neurons produce a set of numbers (known as activations) as output. These numbers are computed by taking the dot product of the input vector and the weight vector, followed by an activation function.\n",
        "\n",
        "2. **Interpretation of Outputs**: However, we want these numbers to represent probabilities, which must be positive and sum to 1. The numbers produced by the neurons don't naturally have these properties.\n",
        "\n",
        "3. **Log Counts**: One way to handle this is to interpret the output numbers as \"log counts.\" Log counts are the natural logarithms of counts, and they can be positive or negative. This is a useful intermediate step because it's easier to transform log counts into probabilities.\n",
        "\n",
        "4. **Exponentiation**: To convert log counts into counts, we can use the exponential function. Exponentiating a log count gives us a count that is always positive.\n",
        "\n",
        "5. **Softmax Function**: To ensure that our counts sum to 1 and thus form a probability distribution, we use the softmax function. The softmax function exponentiates its input (turning log counts into counts) and then normalizes these counts so that they sum to 1.\n",
        "\n",
        "6. **Probabilities**: After applying the softmax function, we end up with a set of numbers that can be interpreted as probabilities. Each number represents the probability of a particular character being the next character in the sequence.\n",
        "\n",
        "7. **Final Output**: For each input example, we have a probability distribution over the next character in the sequence. This is what we want to achieve with our neural network.\n",
        "\n",
        "In summary, the neural network produces a set of numbers as output. We interpret these numbers as log counts, transform them into counts, and then normalize them to form a probability distribution over the next character in the sequence. This step-by-step process is how we go from the raw outputs of the neurons to the probabilities that we're interested in."
      ],
      "metadata": {
        "id": "LlFewfTPHBwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation: The neural network in action\n",
        "\n",
        "1. **Input Encoding**: The input to the neural network is a character, represented as a one-hot encoded vector. In this case, the character '.' has been encoded into a vector of length 27, where the first element is 1 and all other elements are 0. This vector precisely identifies the character we're dealing with.\n",
        "\n",
        "2. **Neuron Activations**: We feed this one-hot encoded vector into the neural network. The neurons in the network have weights associated with them, and we calculate the dot product between the input vector and the weights (this is the `xenc @ W` operation). The result of this dot product is a set of numbers (logits), where each number corresponds to the activation of a particular neuron.\n",
        "\n",
        "3. **Exponentiation**: We exponentiate the logits to get the counts (`logits.exp()`). Exponentiating the logits ensures that we have positive values, as required for counts.\n",
        "\n",
        "4. **Normalization**: We then normalize the counts so that they sum to 1. This is done by dividing each count by the sum of all counts. The result of this normalization is a set of numbers that can be interpreted as probabilities (`probs`). This is because they are all positive and sum to 1, which are the requirements for a probability distribution.\n",
        "\n",
        "5. **Interpretation of Outputs**: Each element in the `probs` vector represents the probability that a particular character is the next character in the sequence, given the input character. The neural network has thus transformed our input character into a probability distribution over the next character.\n",
        "\n",
        "6. **Prediction**: We can use this probability distribution to make predictions about the next character. The character with the highest probability is the neural network's best guess for the next character in the sequence.\n",
        "\n",
        "7. **Training and Optimization**: During training, we adjust the weights of the neural network to minimize the difference between the predicted probabilities and the actual outcomes (the true next characters). This process, known as backpropagation, is how the neural network learns to make better predictions over time.\n",
        "\n",
        "In summary, the neural network takes in a one-hot encoded character as input, applies a series of mathematical operations to transform this input into a probability distribution over the next character, and then uses this probability distribution to make predictions. The goal of training is to adjust the neural network's weights so that its predictions become increasingly accurate."
      ],
      "metadata": {
        "id": "5XR885f0PDa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The forward pass\n",
        "\n",
        "Here's the seasoned ML practitioner's explanation of the forward pass of the neural network:\n",
        "\n",
        "1. **Input Encoding:**\n",
        "   - The input tensor `xs` represents the indices of characters in the sequence. Each index corresponds to a character in the vocabulary.\n",
        "   - We need to convert these indices into a format that can be fed into the neural network. For this, we use one-hot encoding. In one-hot encoding, each index is transformed into a binary vector where only the corresponding index is set to 1, and all other entries are set to 0. This gives us the `xenc` tensor.\n",
        "\n",
        "2. **Neural Network Weights:**\n",
        "   - The neural network has a weight matrix `W`, which represents the parameters of the network. Each neuron has a set of weights corresponding to the inputs it receives.\n",
        "   - In this network, there are 27 neurons (one for each character in the vocabulary), and each neuron receives 27 inputs (one for each character in the one-hot encoding).\n",
        "   - The weight matrix `W` is randomly initialized.\n",
        "\n",
        "3. **Forward Pass:**\n",
        "   - The forward pass is the process of computing the output of the neural network given the input. In this case, the input is the one-hot encoded characters `xenc`.\n",
        "   - We multiply the input `xenc` by the weight matrix `W` to get the `logits`. This is a linear transformation, and it represents the raw predictions of the network.\n",
        "   - The `logits` represent the log-counts of the bigrams. They are unnormalized and can have any real values.\n",
        "\n",
        "4. **Softmax Transformation:**\n",
        "   - The softmax transformation converts the unnormalized `logits` into a probability distribution over the possible next characters.\n",
        "   - First, we take the exponential of the `logits` to get the `counts`. This ensures that all values are positive.\n",
        "   - Then, we normalize the `counts` by dividing each row by the sum of the row. This gives us the `probs` tensor, which represents the probabilities of each next character given the current character.\n",
        "   - Each row of the `probs` tensor is a probability distribution that sums to 1.\n",
        "\n",
        "The output of the forward pass is the `probs` tensor, which contains the predicted probabilities for the next character in the sequence given the current character. This is what the neural network predicts based on its current weights. In the training process, we will adjust the weights to minimize the difference between these predictions and the actual next characters in the sequence."
      ],
      "metadata": {
        "id": "Kkecr9IUGp00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY --------------\n",
        "xs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lGtnnm3QwE0",
        "outputId": "b41f5268-6fe1-49e7-8878-7141eaf0fde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8XyptIQ2ir",
        "outputId": "afb23de7-c461-4646-b2e0-13f18f3fde69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g)"
      ],
      "metadata": {
        "id": "l8t6hO0aQ8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts (muliply in the first layer)\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "# btw: the last 2 lines here are together called a 'softmax'"
      ],
      "metadata": {
        "id": "5tL6k_s1RAsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqLDwEJrRFsE",
        "outputId": "cefb2374-5306-4f8e-8840-f97a959b6b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss function is negative log likelihood\n",
        "\n",
        "nlls = torch.zeros(5)\n",
        "for i in range(5):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
        "  print('input to the neural net:', x)\n",
        "  print('one-hot encoded:', xenc[i])\n",
        "  print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2dH8YebRJx_",
        "outputId": "c2d4986d-bc87-49c6-9916-4267c08bfdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "bigram example 1: .e (indexes 0,5)\n",
            "input to the neural net: 0\n",
            "one-hot encoded: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
            "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
            "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
            "label (actual next character): 5\n",
            "probability assigned by the net to the the correct character: 0.01228625513613224\n",
            "log likelihood: -4.399273872375488\n",
            "negative log likelihood: 4.399273872375488\n",
            "--------\n",
            "bigram example 2: em (indexes 5,13)\n",
            "input to the neural net: 5\n",
            "one-hot encoded: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
            "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
            "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.018050700426101685\n",
            "log likelihood: -4.014570713043213\n",
            "negative log likelihood: 4.014570713043213\n",
            "--------\n",
            "bigram example 3: mm (indexes 13,13)\n",
            "input to the neural net: 13\n",
            "one-hot encoded: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.026691533625125885\n",
            "log likelihood: -3.623408794403076\n",
            "negative log likelihood: 3.623408794403076\n",
            "--------\n",
            "bigram example 4: ma (indexes 13,1)\n",
            "input to the neural net: 13\n",
            "one-hot encoded: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 1\n",
            "probability assigned by the net to the the correct character: 0.07367686182260513\n",
            "log likelihood: -2.6080665588378906\n",
            "negative log likelihood: 2.6080665588378906\n",
            "--------\n",
            "bigram example 5: a. (indexes 1,0)\n",
            "input to the neural net: 1\n",
            "one-hot encoded: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
            "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
            "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
            "label (actual next character): 0\n",
            "probability assigned by the net to the the correct character: 0.014977526850998402\n",
            "log likelihood: -4.201204299926758\n",
            "negative log likelihood: 4.201204299926758\n",
            "=========\n",
            "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the negative log likelihood\n",
        "\n",
        "# ys = tensor([ 5, 13, 13,  1,  0])\n",
        "# we are interested in probs[0,5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]\n",
        "# we want a more efficient way than this\n",
        "probs[0,5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPZENHOpnEUl",
        "outputId": "a5f06bea-ba1d-4608-fb6f-22e16431365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0123),\n",
              " tensor(0.0181),\n",
              " tensor(0.0267),\n",
              " tensor(0.0737),\n",
              " tensor(0.0150))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69umoTloobNa",
        "outputId": "e5bc52d0-887a-4908-f3c7-4bce5ed76195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[torch.arange(5), ys]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIbsE5woh1Q",
        "outputId": "b9777099-a8d8-4f8b-a789-5a5e0cfd0ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[torch.arange(5), ys].log()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PTJ9ePKo5gx",
        "outputId": "9ec723d7-5299-4300-85d2-d5b8f0ce8543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-4.3993, -4.0146, -3.6234, -2.6081, -4.2012])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[torch.arange(5), ys].log().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM0LHNSbo8O5",
        "outputId": "88845c0d-c203-4c7d-dc2b-e75ea31a4dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.7693)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = -probs[torch.arange(5), ys].log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCA1vNeSpAcU",
        "outputId": "53a8ae02-aade-4b85-b6ec-cd633b48be87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7693)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder of micrograd\n",
        "\n",
        "```\n",
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "for k in range(20):\n",
        "  \n",
        "  # forward pass\n",
        "  ypred = [n(x) for x in xs]\n",
        "  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "  \n",
        "  # backward pass\n",
        "  for p in n.parameters():\n",
        "    p.grad = 0.0\n",
        "  loss.backward()\n",
        "  \n",
        "  # update\n",
        "  for p in n.parameters():\n",
        "    p.data += -0.1 * p.grad\n",
        "  \n",
        "  print(k, loss.data)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "NpaKJVE2kwvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization I"
      ],
      "metadata": {
        "id": "VAly5we33YnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- OPTIMIZATION ---------------"
      ],
      "metadata": {
        "id": "nx8r8L-qrWuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "CN88T5KMmomC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we now have full forward pass\n",
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts (muliply in the first layer)\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(5), ys].log().mean()"
      ],
      "metadata": {
        "id": "ym6npesamty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dEI99RqttCO",
        "outputId": "674018d0-07af-454b-f927-2a866f3fadd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7693049907684326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "W.grad = None\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Gif-kxcym_-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT7Yyg2dspel",
        "outputId": "bef3b5b1-0bda-41bb-df25-8153832111d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMRTcCREsr3V",
        "outputId": "cbdf55f1-fc50-42f7-b23f-5d0a564fcb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update\n",
        "W.data += -0.1 * W.grad"
      ],
      "metadata": {
        "id": "AponX8GjrzAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recalulate forward pass\n",
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts (muliply in the first layer)\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "loss = -probs[torch.arange(5), ys].log().mean()"
      ],
      "metadata": {
        "id": "CMozLMiMt8q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruQGirRvtmd9",
        "outputId": "7cc92bcc-fc70-437e-9265-120c23dc7944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7492127418518066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through the code and discuss the shape of the tensors at each step of the process, to help build up a clear intuition of how the neural network works.\n",
        "\n",
        "1. **Dataset Creation**: The code first creates a dataset of input-output pairs based on the words from the dataset. Each pair consists of two characters, where the first character is the input and the second character is the output.\n",
        "```python\n",
        "xs = torch.tensor(xs) # Shape: [num,]\n",
        "ys = torch.tensor(ys) # Shape: [num,]\n",
        "num = xs.nelement()   # Number of examples\n",
        "```\n",
        "Here, `num` represents the total number of character pairs in the dataset. Both `xs` and `ys` are 1-D tensors with a shape of `[num,]`.\n",
        "\n",
        "2. **Network Initialization**: The code initializes the weights `W` of the network. The network has 27 neurons (one for each character in the alphabet plus the '.'), and each neuron has 27 weights (one for each character in the alphabet plus the '.').\n",
        "```python\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
        "```\n",
        "Here, `W` is a 2-D tensor with a shape of `[27, 27]`.\n",
        "\n",
        "3. **Gradient Descent**: The code performs gradient descent optimization for 50 iterations.\n",
        "\n",
        "  - **Forward Pass**:\n",
        "    1. **One-Hot Encoding**: The input characters `xs` are one-hot encoded to create `xenc`.\n",
        "    ```python\n",
        "    xenc = F.one_hot(xs, num_classes=27).float() # Shape: [num, 27]\n",
        "    ```\n",
        "    Here, `xenc` is a 2-D tensor with a shape of `[num, 27]`.\n",
        "    2. **Dot Product**: The dot product between the one-hot encoded input `xenc` and the weights `W` produces `logits`.\n",
        "    ```python\n",
        "    logits = xenc @ W # Shape: [num, 27]\n",
        "    ```\n",
        "    Here, `logits` is a 2-D tensor with a shape of `[num, 27]`.\n",
        "    3. **Exponential**: The exponential of `logits` produces `counts`.\n",
        "    ```python\n",
        "    counts = logits.exp() # Shape: [num, 27]\n",
        "    ```\n",
        "    Here, `counts` is a 2-D tensor with a shape of `[num, 27]`.\n",
        "    4. **Softmax**: The `counts` are normalized to produce `probs`.\n",
        "    ```python\n",
        "    probs = counts / counts.sum(1, keepdims=True) # Shape: [num, 27]\n",
        "    ```\n",
        "    Here, `probs` is a 2-D tensor with a shape of `[num, 27]`.\n",
        "  - **Loss Computation**: The negative log-likelihood loss is computed.\n",
        "  - **Backward Pass**: The gradients of the loss with respect to the weights `W` are computed.\n",
        "  - **Weight Update**: The weights `W` are updated using the computed gradients.\n",
        "\n",
        "At each step of the process, the shape of the tensors involved is highlighted in the comments. In summary, the input-output pairs are transformed from 1-D tensors to 2-D tensors through one-hot encoding, then processed through the neural network to produce probability distributions over the next character in the sequence. The weights of the network are optimized to minimize the loss using gradient descent. The parallel processing of multiple examples is a key feature of neural networks, allowing for efficient training on large datasets."
      ],
      "metadata": {
        "id": "TBMmwoMvysz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization II"
      ],
      "metadata": {
        "id": "P-d2wMz13dYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------"
      ],
      "metadata": {
        "id": "7zJdUv7ouTqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSAAcZMpuWSE",
        "outputId": "58da7dc2-2976-44a4-a865-a37b3f3821d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(50):\n",
        "\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W.data += -50.0 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMNi1J3EuZ2g",
        "outputId": "20025c45-65c7-43e3-c336-e646e0799970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7686190605163574\n",
            "3.3788065910339355\n",
            "3.16109037399292\n",
            "3.0271859169006348\n",
            "2.9344842433929443\n",
            "2.867231607437134\n",
            "2.8166542053222656\n",
            "2.777146339416504\n",
            "2.7452542781829834\n",
            "2.7188303470611572\n",
            "2.696505546569824\n",
            "2.6773719787597656\n",
            "2.6608052253723145\n",
            "2.6463515758514404\n",
            "2.633664846420288\n",
            "2.622471570968628\n",
            "2.6125476360321045\n",
            "2.6037068367004395\n",
            "2.595794916152954\n",
            "2.5886809825897217\n",
            "2.5822560787200928\n",
            "2.5764293670654297\n",
            "2.5711236000061035\n",
            "2.5662729740142822\n",
            "2.5618228912353516\n",
            "2.5577263832092285\n",
            "2.5539441108703613\n",
            "2.550442695617676\n",
            "2.5471925735473633\n",
            "2.5441696643829346\n",
            "2.5413522720336914\n",
            "2.538722038269043\n",
            "2.536262035369873\n",
            "2.5339581966400146\n",
            "2.531797409057617\n",
            "2.529768228530884\n",
            "2.527859926223755\n",
            "2.5260636806488037\n",
            "2.5243701934814453\n",
            "2.522773265838623\n",
            "2.52126407623291\n",
            "2.519836902618408\n",
            "2.5184857845306396\n",
            "2.5172054767608643\n",
            "2.515990734100342\n",
            "2.5148372650146484\n",
            "2.5137407779693604\n",
            "2.512697696685791\n",
            "2.511704921722412\n",
            "2.5107581615448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "\n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    #p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    # ----------\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnh7MDTW9lf_",
        "outputId": "8562ed89-90f5-4411-bb23-8ef881b8d0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junide.\n",
            "janasah.\n",
            "pxzfay.\n",
            "a.\n",
            "nn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Linear Layer and its Relation to Bigram Counts\n",
        "\n",
        "In our neural network, the operation `logits = xenc @ W` is a matrix multiplication between the one-hot encoded input, `xenc`, and the weight matrix, `W`. This operation is equivalent to selecting a row from the weight matrix `W` corresponding to the input character.\n",
        "\n",
        "Let's break it down: when we one-hot encode the input character, we create a vector where only one element is 1, and all the others are 0. When we perform matrix multiplication with the weight matrix `W`, the result is the same as selecting the row from `W` corresponding to the position where the element is 1 in the one-hot encoded vector.\n",
        "\n",
        "In our count-based bigram model, we populated the matrix `N` with the counts of bigrams from the training data. In the gradient-based neural network framework, we initialized the weight matrix `W` randomly and let the loss guide us during optimization. After optimization, the operation `W.exp()` produces a matrix that is equivalent to the counts-based matrix `N`.\n",
        "\n",
        "The process of optimization in the neural network is guided by the loss function, which measures the difference between the predicted probabilities and the true labels. By iteratively updating the weight matrix `W` using gradient-based optimization, we arrive at a matrix that approximates the bigram counts observed in the training data.\n",
        "\n",
        "In summary, both the count-based bigram model and the neural network bigram model use a matrix to represent the relationships between characters. The count-based model populates this matrix directly from the training data, while the neural network model learns the matrix through optimization. After optimization, the matrix in the neural network model is equivalent to the counts-based matrix, representing the same bigram probabilities."
      ],
      "metadata": {
        "id": "zNP879iB4Ovj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building upon a Simple Neural Network for Language Modeling\n",
        "\n",
        "In our current implementation, we have a simple neural network that consists of a single linear layer followed by a softmax activation. This network models the bigram probabilities of our dataset. The network has one parameter tensor, \\(W\\), which holds the weights of the linear layer. Our loss function is the negative log-likelihood, the same as in our previous non-neural network, count-based bigram model.\n",
        "\n",
        "The primary advantage of using a neural network approach is that it allows for iterative improvement and scaling of the model. We can build upon this basic architecture by incorporating multiple previous characters as input, increasing the complexity of the network, and adding additional layers. As we progress towards more complex models, such as recurrent neural networks (RNNs) and transformers, the core machinery for training and optimization will remain similar. The forward pass of the network may become more intricate, but the fundamental concepts of computing the loss, performing backpropagation, and updating the weights will stay consistent.\n",
        "\n",
        "The output of our neural network, the logits, are the raw, unnormalized scores produced by the linear layer. These logits are transformed into probabilities through the softmax activation, which ensures that the probabilities sum to one. The logits play a crucial role in the computation of the loss and the subsequent optimization of the model parameters.\n",
        "\n",
        "As we continue to develop more advanced neural network models for language modeling, we will encounter architectures that can capture longer-range dependencies in the text and provide more powerful representations of the data. Nonetheless, the foundation we have established with our simple neural network will serve as the basis for these more sophisticated models. The process of training and optimizing these models will build upon the concepts we have already introduced."
      ],
      "metadata": {
        "id": "-LvneqfD20HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary\n"
      ],
      "metadata": {
        "id": "pqbGMqdw2UFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difference between `torch.Tensor` and `torch.tensor`\n",
        "\n",
        "`torch.Tensor` is a class, whereas `torch.tensor` is a function. They both create tensors, but there are some differences in their behavior and use cases.\n",
        "\n",
        "1. `torch.Tensor` (class constructor):\n",
        "   - It creates an uninitialized tensor of the specified size.\n",
        "   - The data type of the tensor is inferred from the default data type (`torch.get_default_dtype()`), which is initially set to `torch.float32`.\n",
        "   - It is often used when you need a tensor of a specific size but don't have the data yet.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   ```python\n",
        "   a = torch.Tensor(2, 3) # Creates an uninitialized tensor of size (2, 3)\n",
        "   ```\n",
        "\n",
        "2. `torch.tensor` (function):\n",
        "   - It creates a tensor from the provided data (e.g., a list, a NumPy array, or another tensor).\n",
        "   - The data type of the tensor is inferred from the input data, or you can explicitly set it using the `dtype` argument.\n",
        "   - It is often used when you already have the data and want to create a tensor with that data.\n",
        "\n",
        "   Example:\n",
        "\n",
        "   ```python\n",
        "   data = [1, 2, 3]\n",
        "   b = torch.tensor(data) # Creates a tensor with the data from the list\n",
        "   ```\n",
        "\n",
        "In general, you should use `torch.tensor` when you have the data available and want to create a tensor with that data. Use `torch.Tensor` when you need to create an uninitialized tensor of a specific size. However, for most use cases, `torch.tensor` is the preferred way to create tensors as it provides more control over the data type and other attributes of the tensor."
      ],
      "metadata": {
        "id": "igvDEAtE2_La"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding\n",
        "The reason we don't just feed integers into a neural network when dealing with categorical variables, like characters in a text, is that neural networks are better at interpreting data that is represented in a continuous, rather than discrete, format. When we feed integers into a neural network, the network may assume that there is a natural order to the categories, which is not true in the case of characters in a text.\n",
        "\n",
        "One-hot encoding is a common way to transform categorical variables into a format that can be provided to a machine learning algorithm. In one-hot encoding, we convert each integer into a binary vector with all zero values except for the index of the integer, which is marked with a 1.\n",
        "\n",
        "For example, in our case, we have 27 characters (including the special character `.`). Each character is represented by an integer index. The one-hot encoding process will convert each integer index into a binary vector of length 27, with a 1 at the position corresponding to the integer index and 0s elsewhere.\n",
        "\n",
        "This transformation makes it easier for the neural network to understand the categorical data. Instead of dealing with integers, the network will now process vectors where each character is distinctly represented. The network will not assume any order or hierarchy among the characters, and will be able to learn the relationships between them more effectively.\n",
        "\n",
        "In the code provided, the `F.one_hot` function from PyTorch is used to perform one-hot encoding on the integer indices in `xs`, resulting in a tensor `xenc` of shape `(5, 27)`. Each row in `xenc` is a one-hot encoded vector representing a character from the word `'emma'`."
      ],
      "metadata": {
        "id": "k9CBFwHl25-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot Product\n",
        "\n",
        "1. **Definition of Dot Product**: The dot product, also known as scalar product or inner product, is a mathematical operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. The dot product of two vectors is obtained by multiplying corresponding entries and then summing those products.\n",
        "\n",
        "2. **Dot Product Formula**: If we have two vectors \\( $\\mathbf{a} = [a_1, a_2, ..., a_n] $\\) and \\( $\\mathbf{b} = [b_1, b_2, ..., b_n] $\\), their dot product is calculated as:\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = a_1 \\cdot b_1 + a_2 \\cdot b_2 + \\ldots + a_n \\cdot b_n\n",
        "$$\n",
        "\n",
        "3. **Example with Two Vectors**: Let's take two vectors \\( $\\mathbf{a} = [2, 3]$ \\) and \\( $\\mathbf{b} = [4, 5] $\\). The dot product of these vectors is:\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = (2 \\cdot 4) + (3 \\cdot 5) = 8 + 15 = 23\n",
        "$$\n",
        "\n",
        "4. **Dot Product with Matrices**: The dot product concept can be extended to matrices. When you multiply two matrices, you're taking the dot product of the rows of the first matrix with the columns of the second matrix. This is known as matrix multiplication.\n",
        "\n",
        "5. **Example with Two Matrices**: Let's multiply two matrices \\( A \\) and \\( B \\):\n",
        "$$\n",
        "A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}\n",
        "$$\n",
        "To get the element at the first row and first column of the resulting matrix, we take the dot product of the first row of \\( A \\) with the first column of \\( B \\):\n",
        "$$\n",
        "(1 \\cdot 5) + (2 \\cdot 7) = 5 + 14 = 19\n",
        "$$\n",
        "We do this for every combination of rows from \\( A \\) and columns from \\( B \\) to get the resulting matrix:\n",
        "$$\n",
        "AB = \\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The dot product is a fundamental operation in linear algebra and has applications in various fields, including machine learning, physics, and engineering."
      ],
      "metadata": {
        "id": "8fKwpFKE20tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits (or log-odds) and log-counts\n",
        "\n",
        "Logits (or log-odds) and log-counts are related concepts, but they are used in different contexts. Let's explore each one:\n",
        "\n",
        "1. **Logits (log-odds):**\n",
        "   - The term \"logit\" is often used in the context of logistic regression and refers to the natural logarithm of the odds of an event happening.\n",
        "   - In a binary classification problem, the odds of an event happening are given by \\( \\frac{p}{1-p} \\), where \\( p \\) is the probability of the event.\n",
        "   - The logit is calculated as \\( $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$ \\).\n",
        "   - This transformation is used because it converts the probability domain \\([0, 1]\\) into a real number domain \\($(-\\infty, +\\infty)$\\), making it easier to model linear relationships.\n",
        "\n",
        "2. **Log-counts:**\n",
        "   - In the context of natural language processing and machine learning, the term \"log-counts\" often refers to the natural logarithm of the raw counts of events or features.\n",
        "   - Taking the logarithm of counts is a common transformation used to reduce the impact of extreme values and to make the data more suitable for certain algorithms.\n",
        "   - For example, in text classification or sentiment analysis, the raw term frequencies in documents can be transformed into log-counts to reduce the influence of very frequent words and to stabilize the variance.\n",
        "\n",
        "In the context of your bigram language model, if you're referring to \"log-counts\", it could mean taking the logarithm of the raw bigram counts in your data. This transformation might be useful for certain analyses or modeling approaches."
      ],
      "metadata": {
        "id": "IFxrS0Kx2tK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax function\n",
        "\n",
        "The softmax function is a mathematical tool that's often used in machine learning, particularly in classification problems. In simple terms, it takes a vector of real numbers and transforms it into a probability distribution. This is useful because we often want our models to output probabilities, which can be interpreted and used for decision-making.\n",
        "\n",
        "Here's a step-by-step explanation of how the softmax function works:\n",
        "\n",
        "1. **Input**: The input to the softmax function is a vector of real numbers (often called logits). Each element of this vector corresponds to a particular category or class.\n",
        "\n",
        "2. **Exponentiation**: The softmax function exponentiates each element of the input vector. This ensures that all the elements are positive, as probabilities must be positive.\n",
        "\n",
        "3. **Normalization**: The softmax function then divides each exponentiated element by the sum of all the exponentiated elements. This ensures that the resulting probabilities sum to 1, which is a requirement for a probability distribution.\n",
        "\n",
        "4. **Output**: The output is a vector of probabilities, one for each category or class. These probabilities tell us how likely each category is given the input data.\n",
        "\n",
        "The softmax function is useful because it allows us to convert the raw output of a machine learning model (which can be any real numbers) into a probability distribution, which is easier to interpret and work with. It's especially useful in classification problems where we want to know the likelihood of each possible category or class."
      ],
      "metadata": {
        "id": "bxPOi0_82pBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing a Neural Network\n",
        "\n",
        "The process of training a neural network involves finding the optimal weights that minimize the loss function. Here is a step-by-step breakdown of how a seasoned machine learning practitioner would optimize a neural network:\n",
        "\n",
        "1. **Initialization**: Start with an initial guess for the weights of the neural network. These weights can be initialized randomly or using some predefined method.\n",
        "\n",
        "2. **Forward Pass**: Input data is passed through the network, layer by layer, until the final output layer is reached. The raw output of the network is then transformed (e.g., through the softmax function) to produce a probability distribution over the possible classes.\n",
        "\n",
        "3. **Compute Loss**: Compare the predicted probabilities with the true labels using a loss function. This function quantifies how well the network's predictions match the actual data. The goal of optimization is to minimize this loss.\n",
        "\n",
        "4. **Backpropagation**: Compute the gradients of the loss with respect to the weights. This involves applying the chain rule of calculus to find how much each weight contributed to the loss. This process is known as backpropagation because it works backward from the output layer to the input layer.\n",
        "\n",
        "5. **Update Weights**: Adjust the weights in the direction that reduces the loss. This is typically done using an optimization algorithm like gradient descent or one of its variants. The optimizer uses the gradients computed during backpropagation to update the weights.\n",
        "\n",
        "6. **Iterate**: Repeat steps 2-5 for a number of epochs or until the loss stops decreasing.\n",
        "\n",
        "The key idea behind optimizing a neural network is to iteratively adjust the weights based on the computed gradients, which indicate the direction and magnitude of the changes needed to minimize the loss. This process leverages the fact that all operations in the loss computation are differentiable, making it possible to use gradient-based optimization techniques."
      ],
      "metadata": {
        "id": "CLXEB3MKkUzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization in Machine Learning\n",
        "\n",
        "Regularization is a crucial concept in machine learning, especially in training neural networks. It helps prevent overfitting, which occurs when a model is too complex and fits the training data too closely, capturing noise and outliers rather than the underlying pattern. Overfit models tend to perform poorly on unseen data.\n",
        "\n",
        "Regularization techniques add a penalty to the loss function, discouraging the model from becoming too complex. The penalty term is usually a function of the model's parameters (weights and biases) and encourages the model to have smaller parameter values, leading to a simpler model.\n",
        "\n",
        "In our neural network, we use L2 regularization, which is also known as weight decay or ridge regression. The regularization term is the sum of the squared values of the model's parameters, scaled by a regularization coefficient (0.01 in our case). The equation for the loss with regularization is:\n",
        "\n",
        "$$\n",
        "\\text{loss} = -\\text{probs}[torch.arange(num), ys].log().mean() + 0.01 \\cdot (W^2).mean()\n",
        "$$\n",
        "\n",
        "The first term is the negative log likelihood loss, which measures the model's performance on the training data. The second term is the L2 regularization, which discourages the model from having large parameter values.\n",
        "\n",
        "The regularization coefficient (0.01) determines the strength of the regularization. A higher value will put more emphasis on the regularization term, leading to a simpler model, while a lower value will put more emphasis on fitting the training data.\n",
        "\n",
        "Regularization helps to find a balance between fitting the training data and preventing overfitting. By adding a penalty to the loss function, regularization encourages the model to be simpler, which often leads to better generalization on unseen data.\n",
        "\n",
        "In summary, regularization is an essential technique in machine learning for preventing overfitting and improving model generalization. It works by adding a penalty to the loss function, encouraging the model to be simpler by having smaller parameter values."
      ],
      "metadata": {
        "id": "Yr3yrgBs6XcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization: Intuition and Connection to Smoothing\n",
        "\n",
        "Regularization is a crucial concept in machine learning that helps to prevent overfitting by discouraging overly complex models. In the context of our simple neural network, we use L2 regularization, which adds a penalty term to the loss function based on the squared values of the model's parameters.\n",
        "\n",
        "Let's break down the intuition behind this:\n",
        "\n",
        "1. **Initial Conditions**: If we initialize the weights \\(W\\) of the neural network to zero, all the logits (i.e., the outputs of the linear layer) will be zero. When we exponentiate these logits using the `exp` function, we get 1 for all entries. Consequently, when we normalize to get the probabilities, they will be uniformly distributed.\n",
        "\n",
        "2. **Uniform Probabilities**: When all the weights \\(W\\) are equal, the resulting probabilities for the next character in the sequence will be uniform. In other words, the model will assign equal probabilities to all possible next characters.\n",
        "\n",
        "3. **Incentivizing Near-Zero Weights**: Regularization works by adding a penalty term to the loss function that incentivizes the weights to be near zero. In our case, we use L2 regularization, which adds a term proportional to the sum of the squared values of the weights.\n",
        "\n",
        "4. **Gravity Towards Zero**: You can think of regularization as a gravitational force that pulls the weights towards zero. The strength of this force is determined by the regularization coefficient. The higher the coefficient, the stronger the force pulling the weights towards zero.\n",
        "\n",
        "5. **Connection to Smoothing**: In our counts-based bigram model, we used smoothing by adding a constant to the counts to avoid zero probabilities. This smoothing is equivalent to the regularization in our neural network model. The strength of the regularization is akin to adding more counts in the counts-based model.\n",
        "\n",
        "6. **Balancing Act**: The optimization process now has two competing objectives. On the one hand, it tries to adjust the weights to make the predicted probabilities match the observed data. On the other hand, it tries to keep the weights close to zero due to the regularization term.\n",
        "\n",
        "7. **Regularization as a Smoothness Constraint**: By incentivizing the weights to be near zero, regularization acts as a smoothness constraint on the model. It prevents the model from fitting the training data too closely, which can lead to overfitting. The optimization becomes a balancing act between fitting the data and keeping the model smooth and simple.\n",
        "\n",
        "In summary, regularization is a technique that helps prevent overfitting by adding a penalty term to the loss function, encouraging the model's weights to be near zero. This is conceptually similar to the smoothing we used in the counts-based bigram model. The strength of the regularization determines the balance between fitting the training data and keeping the model smooth and simple."
      ],
      "metadata": {
        "id": "3_9q6V8L9Vf-"
      }
    }
  ]
}