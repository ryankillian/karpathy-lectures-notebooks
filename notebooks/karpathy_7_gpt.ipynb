{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nQhrBr5mfngX",
        "De8RsCZrWbfC",
        "9NsYdrnlcEkw",
        "pGVviJpgcmBU",
        "4D17zOXqdbIq",
        "6d3FAq8jsxG6",
        "VDjCE5xJ5qVV",
        "2RPcEaW25_qt",
        "-7XNpnrF7D7J",
        "_2ndOtUELqNC",
        "BgQVPXobUTHs",
        "Ehr5DUjAWzZH",
        "iDlXKlj1biju",
        "1Q43hl6VL9XT",
        "o6g8NTR3PcPZ",
        "X9XI3jhxVZkY",
        "Nxv1sL_rXufS",
        "mWMoOFaEkuQK",
        "KSDCRK-BoM9z",
        "RCgP-ZInvrx3",
        "WdXPigRTBKq6",
        "2M-ApIRM3C_m",
        "vmnIo2ESBSp9",
        "k22rGpim1nfw",
        "nC7V72TrGzGv",
        "hHNdbK5kc0AI",
        "k3V0e-IxiNHn",
        "eqDDBPgLXSrA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction to Building GPT from Scratch\n",
        "\n",
        "Welcome to this Colab notebook where we will be closely following Andrej Karpathy's YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY). Throughout this journey, we'll be diving deep into the intricate details of the model, discussing its architecture, and understanding the building blocks that make it so powerful.\n",
        "\n",
        "Key components that this model encompasses are:\n",
        "- Positional Encodings\n",
        "- Multi-headed Self-Attention\n",
        "- Feed-forward Layer\n",
        "- Residual Connections\n",
        "- Layer Normalization\n",
        "- Dropout\n",
        "\n",
        "Additionally, this notebook is enriched with:\n",
        "- Detailed notes elucidating various parts of the architecture.\n",
        "- A concise summary of the seminal \"Attention is all you need\" paper.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bl5qqFumxiUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset processing"
      ],
      "metadata": {
        "id": "nQhrBr5mfngX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading and Exploring the data"
      ],
      "metadata": {
        "id": "De8RsCZrWbfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U301JCoVV9gz",
        "outputId": "b9a9be7e-e418-4c27-eca0-c8b65e066d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 14:32:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-08-28 14:32:27 (17.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "djZTI1cRWl79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECGV2uvrW4zk",
        "outputId": "74851845-bbca-4bea-ccca-3b8db5988180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgvLAqAqXHNk",
        "outputId": "4a86fc9a-ee26-4084-c628-fda58ba53586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auyb9N1sZ0ni",
        "outputId": "54796ea5-9ed3-40c8-edbc-fef666d71d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "9NsYdrnlcEkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "5a2OkjI9aZ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "pGVviJpgcmBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes a string, output a list of integers and vice versa\n",
        "encode = lambda s: [ stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([ itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUSe3Y70bBQ_",
        "outputId": "acc578a6-3358-40eb-f1fc-80f6716845a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 1, 58, 46, 43, 56, 43]\n",
            "hi there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)"
      ],
      "metadata": {
        "id": "iQoYb9kbcp9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_hKWeOAdA9Z",
        "outputId": "996d1540-5378-48f1-e57f-a71276fdf8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train, Validation split"
      ],
      "metadata": {
        "id": "4D17zOXqdbIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print((len(train_data), len(val_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoC4ayWXdeYz",
        "outputId": "f9d348d5-0b27-45f2-8e1a-842d5da24428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1003854, 111540)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader: Batches of chunks of data"
      ],
      "metadata": {
        "id": "o39Ua_gafXUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time dimension (chunks of data)\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPdVudITfbE2",
        "outputId": "382ec5a3-31e0-4d4a-c97b-2deb5e9116ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:block_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYI6HPjCiMep",
        "outputId": "9cff7dc5-ebd7-4333-ab15-99c38b92ab58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1: block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K44-ejbGiEhI",
        "outputId": "7018a02d-18a5-49ad-f609-3a0344b5b24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1: block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is: {context} the target is: {target} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwnvAJO9hO8A",
        "outputId": "dcc81d0d-a332-4c47-ef2f-781b067498a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is: tensor([18]) the target is: 47 \n",
            "when input is: tensor([18, 47]) the target is: 56 \n",
            "when input is: tensor([18, 47, 56]) the target is: 57 \n",
            "when input is: tensor([18, 47, 56, 57]) the target is: 58 \n",
            "when input is: tensor([18, 47, 56, 57, 58]) the target is: 1 \n",
            "when input is: tensor([18, 47, 56, 57, 58,  1]) the target is: 15 \n",
            "when input is: tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47 \n",
            "when input is: tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch dimension\n",
        "\n",
        "# we're sampling these chunks of text we're going to be actually every time\n",
        "# we're going to feed them into a Transformer we're going to have many batches\n",
        "# of multiple chunks of text that are all stacked up in a single tensor"
      ],
      "metadata": {
        "id": "b_mRoncfikkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack( [data[i:i+block_size]for i in ix])\n",
        "    y = torch.stack( [data[i+1:i+block_size+1]for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hXECnpb1jC4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3rXPS-qYOQ",
        "outputId": "a98d9b57-c205-4a5b-c556-a125e753148e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43,  1, 51, 39, 63,  1, 40, 43],\n",
            "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
            "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
            "        [53, 56, 43,  1, 21,  1, 41, 39]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 51, 39, 63,  1, 40, 43,  1],\n",
            "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
            "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
            "        [56, 43,  1, 21,  1, 41, 39, 51]])\n",
            "----\n",
            "when input: [43] the target: 1\n",
            "when input: [43, 1] the target: 51\n",
            "when input: [43, 1, 51] the target: 39\n",
            "when input: [43, 1, 51, 39] the target: 63\n",
            "when input: [43, 1, 51, 39, 63] the target: 1\n",
            "when input: [43, 1, 51, 39, 63, 1] the target: 40\n",
            "when input: [43, 1, 51, 39, 63, 1, 40] the target: 43\n",
            "when input: [43, 1, 51, 39, 63, 1, 40, 43] the target: 1\n",
            "when input: [58] the target: 46\n",
            "when input: [58, 46] the target: 43\n",
            "when input: [58, 46, 43] the target: 1\n",
            "when input: [58, 46, 43, 1] the target: 43\n",
            "when input: [58, 46, 43, 1, 43] the target: 39\n",
            "when input: [58, 46, 43, 1, 43, 39] the target: 56\n",
            "when input: [58, 46, 43, 1, 43, 39, 56] the target: 57\n",
            "when input: [58, 46, 43, 1, 43, 39, 56, 57] the target: 10\n",
            "when input: [39] the target: 58\n",
            "when input: [39, 58] the target: 47\n",
            "when input: [39, 58, 47] the target: 53\n",
            "when input: [39, 58, 47, 53] the target: 52\n",
            "when input: [39, 58, 47, 53, 52] the target: 12\n",
            "when input: [39, 58, 47, 53, 52, 12] the target: 1\n",
            "when input: [39, 58, 47, 53, 52, 12, 1] the target: 37\n",
            "when input: [39, 58, 47, 53, 52, 12, 1, 37] the target: 53\n",
            "when input: [53] the target: 56\n",
            "when input: [53, 56] the target: 43\n",
            "when input: [53, 56, 43] the target: 1\n",
            "when input: [53, 56, 43, 1] the target: 21\n",
            "when input: [53, 56, 43, 1, 21] the target: 1\n",
            "when input: [53, 56, 43, 1, 21, 1] the target: 41\n",
            "when input: [53, 56, 43, 1, 21, 1, 41] the target: 39\n",
            "when input: [53, 56, 43, 1, 21, 1, 41, 39] the target: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEYejbzXsaw2",
        "outputId": "fac4c434-8b5f-4d84-b8b0-48ba1c1063d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[43,  1, 51, 39, 63,  1, 40, 43],\n",
            "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
            "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
            "        [53, 56, 43,  1, 21,  1, 41, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader explanation\n",
        "\n",
        "### 1. **Chunking the Data**:\n",
        "- **Why?** Transformers, especially large ones like GPT, can be computationally expensive. Instead of feeding the entire text sequence into the Transformer, chunks of data are used.\n",
        "- **Block Size**: This is a term used for the maximum length of these chunks. It's also sometimes referred to as 'context length'. A block size of 8 means each chunk will be 8 characters long.\n",
        "\n",
        "### 2. **Multiple Examples in a Single Chunk**:\n",
        "- **Overlap Training**: For a block size of 8, there are actually 8 training examples. For example, for a sequence \"12345678\", the model learns that after \"1\" comes \"2\", after \"12\" comes \"3\", and so on.\n",
        "- **Why Plus One?**: When taking a chunk, it's actually of size `block_size + 1` (9 in the example). This is because, for training, one character is used as input and the next character as the expected output.\n",
        "\n",
        "### 3. **Training on Diverse Contexts**:\n",
        "- Training isn't just done on sequences of length 8 (or block size). It's done on sequences of length 1 to 8. This makes the Transformer accustomed to making predictions on contexts ranging from a single character up to the full block size.\n",
        "- **Advantage**: During inference, this flexibility means we can generate sequences starting from just one character. After reaching the block size, we would need to truncate or remove some of the context to continue generating.\n",
        "\n",
        "### 4. **Batch Dimension**:\n",
        "- **Batching**: Instead of processing chunks one-by-one, multiple chunks are stacked together and processed simultaneously for efficiency. This is particularly useful to fully utilize GPUs which excel at parallel processing.\n",
        "- **Independence**: Each chunk in a batch is processed independently. They don't share information.\n",
        "\n",
        "### 5. **Sampling Random Chunks**:\n",
        "- For training, random chunks are sampled from the dataset. This adds diversity and randomness, which is good for generalization.\n",
        "- **Seed Setting**: Setting a seed ensures reproducibility. This means the random chunks selected in one run will be the same in another run if the same seed is used.\n",
        "\n",
        "### 6. **Input and Target Tensors**:\n",
        "- **Inputs (X)**: These are tensors that contain the characters up to a certain point in the chunk.\n",
        "- **Targets (Y)**: These are tensors that contain the character that comes next after the characters in the input.\n",
        "- The provided code showcases this by printing the input and target for each example in the batch.\n",
        "\n",
        "In essence, the lecturer is demonstrating a foundational approach in training Transformers, especially for language modeling. By breaking down data into manageable chunks, creating overlapping training examples from these chunks, and efficiently batching these chunks, the model can be trained to understand context and predict subsequent characters in a sequence. This methodology ensures the model is versatile, efficient, and capable of generating text starting from minimal context."
      ],
      "metadata": {
        "id": "e5zGEdGVlPX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "BXhz2h5Ysueu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "6d3FAq8jsxG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO2UzsHCs2Ga",
        "outputId": "e4539fcd-62c3-4853-f754-cc1b0019e5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7866def400d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest Possible Model"
      ],
      "metadata": {
        "id": "VDjCE5xJ5qVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets):\n",
        "        # idx, targets are both (B,T) tensor of integers\n",
        "        # C (embeddings) is added here\n",
        "        logits = self.token_embedding_table(idx) # BTC\n",
        "\n",
        "        return logits\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out = m(xb, yb)\n",
        "print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp3F_XNQ3cUu",
        "outputId": "9c60bc68-4de6-4dec-94ed-1622d19c2878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY7u5yzE5bbs",
        "outputId": "d5c01021-8816-459c-9e24-53d4a784c209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.3323, -0.0872, -0.7470, -0.6074,  0.3418,  0.5343,  0.3957, -0.4919,\n",
              "        -0.0894, -1.3886,  1.2835, -0.3975,  2.0152,  1.6773, -0.3833,  1.5728,\n",
              "         1.9458,  0.7247, -0.4834, -0.3263,  0.3193, -0.4198, -0.6435, -0.3311,\n",
              "         0.7554, -1.2385,  0.4067,  0.9982, -0.6511,  1.2450,  0.2804,  0.8371,\n",
              "        -0.4119,  0.2115, -0.6240,  0.0203, -0.3418,  1.4934,  1.7307,  1.3354,\n",
              "        -0.2712,  0.4902,  0.6600, -1.6321, -0.7858,  1.7688,  2.6160, -0.5767,\n",
              "        -0.3628, -2.7428,  0.7428,  0.0737,  0.2050, -0.5497,  2.1261, -0.9240,\n",
              "         0.1048,  0.8324,  1.4287, -0.7789,  2.9275, -0.8525, -0.6716, -0.9572,\n",
              "        -0.9594], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Loss Function\n",
        "\n",
        "Add functionality to be able to evaluate the quality of model."
      ],
      "metadata": {
        "id": "2RPcEaW25_qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets):\n",
        "        logits = self.token_embedding_table(idx) # BTC\n",
        "\n",
        "        # Note 1:\n",
        "        # we have multi-dimensional input B,T,C\n",
        "        # Pytorch's cross_entropy wants B,C,T for inputs\n",
        "        # we need to reshape logits AND targets for Pytorch\n",
        "        # [4,8,65] -> [32,65]\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lil9tVW6FGB",
        "outputId": "cdfb72cf-2071-434a-8bde-1f35f598dda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.7032, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with negative log likelihood we expect this loss\n",
        "import numpy as np\n",
        "-np.log(1/vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isNhtnIo9Qup",
        "outputId": "63146e3e-eb91-4487-d824-63445c40d268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.174387269895637"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # the correct dimension of logits depending on whatever the target is\n",
        "  # should have a very high number\n",
        "  # and all the other dimensions should be very low number right"
      ],
      "metadata": {
        "id": "tMj6rZ5l8p5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Generate Function\n",
        "\n",
        "Add functionality to be able to generate from the model"
      ],
      "metadata": {
        "id": "-7XNpnrF7D7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx) # BTC\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B,T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx) # call forward() to get predictions (B,T,C)\n",
        "            logits = logits[:,-1,:] # focus on last time step, becomes (B,C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B,1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(torch.zeros((1,1), dtype=torch.long))\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHNhF253-v3W",
        "outputId": "e29529c4-eb89-41ba-aa84-4b5ed33bf653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.7827, grad_fn=<NllLossBackward0>)\n",
            "tensor([[0]])\n",
            "\n",
            "aIQXGbOnA-UpcjlXI;c.LsgHeMpg;c::tAtNA'KOkmHeXW ?-F\n",
            "HZ3R'rpNK'Xpdpcbe'N.ydDHqdh!WXXw\n",
            "So$uVHeTYTA?l&-L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Explanation of `generate` function\n",
        "\n",
        "The `generate` function in the `BigramLanguageModel` class is responsible for producing sequences of tokens based on a given context. Here's a step-by-step explanation of the function:\n",
        "\n",
        "### 1. **Input Parameters**:\n",
        "- **idx**: A 2D tensor of shape `(B, T)` where `B` is the batch size (number of sequences) and `T` is the length of each sequence (context).\n",
        "- **max_new_tokens**: The number of new tokens to generate for each sequence in the batch.\n",
        "\n",
        "### 2. **Token Generation Loop**:\n",
        "The main part of the function is a loop that runs for `max_new_tokens` iterations. In each iteration, a new token is generated for each sequence in the batch.\n",
        "\n",
        "### 3. **Get Logits**:\n",
        "Within the loop:\n",
        "- The forward method of the model (`self(idx)`) is called with the current `idx` tensor as input. This returns the logits for each token in the sequences. The shape of the logits tensor is `(B, T, C)`, where `C` is the number of classes (vocab size).\n",
        "\n",
        "### 4. **Last Token's Logits**:\n",
        "- From these logits, only the logits corresponding to the last token of each sequence are of interest when generating the next token. This is achieved with `logits[:, -1, :]`, which extracts the last token's logits for each sequence.\n",
        "\n",
        "### 5. **Probability Distribution**:\n",
        "- The logits are then converted into a probability distribution using the softmax function (`F.softmax`). This gives the probability of each token being the next token in the sequence.\n",
        "\n",
        "### 6. **Token Sampling**:\n",
        "- A new token is sampled for each sequence based on the probability distribution using the `torch.multinomial` function. This function samples a value from a given distribution. The result is a tensor of shape `(B, 1)` containing the indices of the sampled tokens.\n",
        "\n",
        "### 7. **Update Context**:\n",
        "- The newly sampled tokens (`idx_next`) are concatenated to the current sequences (`idx`) along the time dimension, updating the context for the next iteration.\n",
        "\n",
        "### 8. **Return Final Sequences**:\n",
        "Once the loop finishes and all the new tokens have been generated, the function returns the updated `idx` tensor, which contains the original sequences with the newly generated tokens appended.\n",
        "\n",
        "In summary, the `generate` function takes in a batch of initial sequences and extends each sequence by generating new tokens based on the model's predictions, for a specified number of iterations.\n"
      ],
      "metadata": {
        "id": "4ZMpAMJ6EilF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### The slicing operation `[:, -1, :]`\n",
        "\n",
        "### Step 1: Understand the Slicing Operation\n",
        "The slicing operation `[:, -1, :]` can be interpreted as:\n",
        "1. `:`: Take all elements along the first dimension (often the batch dimension).\n",
        "2. `-1`: Take only the last element along the second dimension (often the sequence or time dimension).\n",
        "3. `:`: Take all elements along the third dimension (often the feature or channel dimension).\n",
        "\n",
        "In simpler terms, for each item in the batch, we're extracting the last element along the sequence dimension, and for that element, we're taking all its features.\n",
        "\n",
        "### Step 2: Create a Sample Tensor\n",
        "Let's create a 3D tensor with dimensions `[B, T, C]` where:\n",
        "- `B` is the batch size (number of sequences).\n",
        "- `T` is the time (length of each sequence).\n",
        "- `C` is the number of channels (features for each time step).\n",
        "\n",
        "We'll use a batch size of 2, a sequence length of 3, and 4 features for each time step:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "tensor = torch.tensor([\n",
        "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
        "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
        "])\n",
        "print(\"Original Tensor:\")\n",
        "print(tensor)\n",
        "```\n",
        "\n",
        "The tensor represents two sequences. Each sequence has three time steps, and each time step has four features.\n",
        "\n",
        "### Step 3: Apply the Slicing Operation\n",
        "Now, we'll extract the last time step for each sequence:\n",
        "\n",
        "```python\n",
        "sliced_tensor = tensor[:, -1, :]\n",
        "print(\"\\nSliced Tensor (Last time step of each sequence):\")\n",
        "print(sliced_tensor)\n",
        "```\n",
        "\n",
        "### Step 4: Interpret the Results\n",
        "The `sliced_tensor` will contain the last time step of each sequence. For our sample tensor, this will extract the vectors `[9, 10, 11, 12]` and `[21, 22, 23, 24]`.\n",
        "\n",
        "This step-by-step demonstration provides a clear understanding of how the slicing operation `[:, -1, :]` can be used to extract specific parts of a tensor in PyTorch.\n"
      ],
      "metadata": {
        "id": "sCoYo3YIIa-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([\n",
        "    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
        "    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]\n",
        "])\n",
        "print(\"Original Tensor:\")\n",
        "print(tensor.shape)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLpIvwNhIq0P",
        "outputId": "e045cf93-c10e-470c-fed2-02a6a129a1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [ 5,  6,  7,  8],\n",
            "         [ 9, 10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15, 16],\n",
            "         [17, 18, 19, 20],\n",
            "         [21, 22, 23, 24]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sliced_tensor = tensor[:, -1, :]\n",
        "print(\"\\nSliced Tensor (Last time step of each sequence):\")\n",
        "print(sliced_tensor.shape)\n",
        "print(sliced_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSw22VP8ItYb",
        "outputId": "d0cdf494-ee2d-402e-c70b-cc9075ceb79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sliced Tensor (Last time step of each sequence):\n",
            "torch.Size([2, 4])\n",
            "tensor([[ 9, 10, 11, 12],\n",
            "        [21, 22, 23, 24]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Demonstration of `generate` function\n",
        "\n",
        "Let's create a simple sample data and process it through the `generate` function of the `BigramLanguageModel`.\n",
        "\n",
        "### Step 1: Set up the Environment and Model\n",
        "First, we need to make sure we have all the necessary libraries and components in place.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "```\n",
        "\n",
        "Assuming the class `BigramLanguageModel` has already been defined as provided, we can instantiate it:\n",
        "\n",
        "```python\n",
        "vocab_size = 100  # Let's assume a vocabulary size of 100 for simplicity\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "```\n",
        "\n",
        "### Step 2: Create Sample Data\n",
        "For demonstration purposes, let's create a sample data tensor of shape `(B, T)` where `B` is the batch size and `T` is the length of each sequence. We'll use a batch size of 3 and a sequence length of 5:\n",
        "\n",
        "```python\n",
        "sample_data = torch.tensor([[1, 2, 3, 4, 5],\n",
        "                            [6, 7, 8, 9, 10],\n",
        "                            [11, 12, 13, 14, 15]])\n",
        "```\n",
        "\n",
        "This tensor represents three sequences: `1-2-3-4-5`, `6-7-8-9-10`, and `11-12-13-14-15`.\n",
        "\n",
        "### Step 3: Use the Generate Function\n",
        "Now, we'll use the `generate` function to produce, say, 8 new tokens for each sequence:\n",
        "\n",
        "```python\n",
        "generated_sequences = m.generate(idx=sample_data, max_new_tokens=8)\n",
        "print(generated_sequences)\n",
        "```\n",
        "\n",
        "### Step 4: Interpret the Results\n",
        "The `generated_sequences` tensor will now contain the original sequences with 8 new tokens appended to each. Depending on the random initialization of the `BigramLanguageModel` and the inherent randomness in the `generate` function, the new tokens will vary each time the function is called.\n",
        "\n",
        "This demonstration should provide a clear understanding of how the `generate` function processes input data and extends sequences based on the model's predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XTTYvFK1E-3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blm_vocab_size = 100  # Let's assume a vocabulary size of 100 for simplicity\n",
        "blm = BigramLanguageModel(blm_vocab_size)\n",
        "sample_data = torch.tensor([[1, 2, 3, 4, 5],\n",
        "                            [6, 7, 8, 9, 10],\n",
        "                            [11, 12, 13, 14, 15]])\n",
        "generated_sequences = m.generate(idx=sample_data, max_new_tokens=8)\n",
        "print(generated_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i037qw7SK2__",
        "outputId": "48119cb2-2140-4f85-85c6-0882c894ee3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1,  2,  3,  4,  5,  6,  5, 39, 55, 28, 26, 60, 37],\n",
            "        [ 6,  7,  8,  9, 10, 19,  1, 59, 50, 59, 57, 47,  7],\n",
            "        [11, 12, 13, 14, 15, 31, 31,  0, 39, 27, 49, 60, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "_2ndOtUELqNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "VpdFfhRNLu2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "\n",
        "for steps in range(10000):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward();\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJFCTh5tMUJs",
        "outputId": "a69d63d4-8d86-40d2-b22b-6208ce40ba70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4837, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating from the model"
      ],
      "metadata": {
        "id": "BgQVPXobUTHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVz7eZfzPaBc",
        "outputId": "59306d0e-5517-4db7-d17c-71ed05214ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I irou t k rerchoubengherd.\n",
            "Shy ak stast meay VI asthathouisth'd K:\n",
            "Thischid, mave isele ver CE:\n",
            "We\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGM74W2qTJZe",
        "outputId": "7950e4f2-642e-4169-a17f-9247d1c7774d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fot:\n",
            "BUCLI wif by be! eadererosu, fathen'doueststotit e\n",
            "-g,ILE:\n",
            "F! s thar wnd nswind I: she---s. tt u helineta fe IN awou cof th n ga ano?\n",
            "Anthinongheomyoeldid, ngoreran,XZULAGELouler wisereoue ouloshond e dy Foung.\n",
            "An meen th is the;\n",
            "Sheaf mes,\n",
            "I rangre sp;\n",
            "BUT:\n",
            "Fourat sts tasoory ghe!\n",
            "Bud thrserrowithe ire t il d homyio l mbls othethive ther ll CE outitongngsthe, cld cestreshime ar, mm:\n",
            "CHA a esolathan th gelellavithalll whodwitisteto t?\n",
            "A'dit, f el hagucerd:\n",
            "YCla mentedond\n",
            "Nouns, br ig!\n",
            "GSINR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# much improved result!!"
      ],
      "metadata": {
        "id": "GHr2AdsCS7By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with GPU and validation\n",
        "\n"
      ],
      "metadata": {
        "id": "Ehr5DUjAWzZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1ygQ2GAW_Pz",
        "outputId": "ae699f76-04a9-4f49-963e-45386d4e0f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis b, bth\n",
            "\n",
            "HAPet bobe d e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar u qur, t.\n",
            "War dXENDoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
            "Swanousel lind me l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentofu n Boopetelaves\n",
            "MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar igr t m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myrangoube!\n",
            "KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n",
            "J\n",
            "CPU times: user 10.7 s, sys: 898 ms, total: 11.6 s\n",
            "Wall time: 15.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model's learning ability and the parameters being updated\n",
        "\n",
        "The code provided is for a Bigram Language Model. Let's dissect its learning ability step-by-step.\n",
        "\n",
        "### 1. **Model Structure**:\n",
        "The core model is the `BigramLanguageModel`, which is designed to be a simple language model that utilizes embeddings.\n",
        "\n",
        "- **Embedding Table**: The model has a single layer, `token_embedding_table`, which is an embedding layer. This layer converts token indices into dense vectors. Interestingly, the size of these embeddings is the same as the vocabulary size, making it a square matrix.\n",
        "\n",
        "### 2. **Learning Process**:\n",
        "Learning in this model is facilitated by adjusting the weights of the `token_embedding_table` to minimize prediction error.\n",
        "\n",
        "- **Forward Pass**: For each input token (or sequence of tokens), the model fetches its corresponding embedding (or sequence of embeddings) from the `token_embedding_table`. This embedding is treated as the logits for predicting the next token.\n",
        "- **Loss Calculation**: The loss is calculated by measuring the difference between the predicted logits and the actual next tokens using the cross-entropy loss.\n",
        "\n",
        "### 3. **Training Loop**:\n",
        "The training loop is where the learning actually takes place:\n",
        "\n",
        "- **Batch Sampling**: In each iteration, a batch of sequences (`xb`) and their corresponding next tokens (`yb`) are sampled.\n",
        "- **Model Evaluation**: The model's forward method is called with `xb` and `yb` to obtain the predicted logits and the associated loss.\n",
        "- **Backpropagation**: The loss is backpropagated through the model to compute gradients for all the model's parameters.\n",
        "- **Parameter Update**: The optimizer (`AdamW`) then updates the model's parameters (in this case, the embeddings) using these gradients. This step adjusts the embeddings in the direction that minimizes the prediction error.\n",
        "\n",
        "### 4. **Parameters Being Updated**:\n",
        "The only weights or parameters being updated during the training process are the embeddings in the `token_embedding_table`. Since this is the only learnable component of the model, all the learning capability is concentrated here.\n",
        "\n",
        "### 5. **Generative Ability**:\n",
        "The `generate` function allows the model to produce sequences of tokens. Starting from an initial context, the model predicts the next token, samples from this prediction, appends this token to the context, and repeats this process for a specified number of iterations.\n",
        "\n",
        "### Summary:\n",
        "The 'learning' ability of this code is encapsulated in the `token_embedding_table` of the `BigramLanguageModel`. During training, the model adjusts the embeddings in this table to better predict the next token in a sequence. The embeddings serve as a lookup table where each token is associated with a dense vector that represents the logits (or unnormalized probabilities) for predicting the next token. The only parameters being updated during training are the embeddings in this table."
      ],
      "metadata": {
        "id": "lwmXBICQPn2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## The estimate_loss function\n",
        "\n",
        "The `estimate_loss` function is designed to evaluate and provide an estimate of the model's loss on both the training and validation datasets. Let's break it down step-by-step:\n",
        "\n",
        "### 1. **Function Definition**:\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "```\n",
        "- The `@torch.no_grad()` decorator is used to ensure that the function runs in a context where gradient calculations are disabled, which is essential for evaluation tasks. This saves memory and computation.\n",
        "\n",
        "### 2. **Switch to Evaluation Mode**:\n",
        "```python\n",
        "model.eval()\n",
        "```\n",
        "- This line sets the model to evaluation mode. Certain layers like dropout or batch normalization behave differently during training and evaluation. So, it's essential to switch the mode when evaluating.\n",
        "\n",
        "### 3. **Evaluate Loss for Each Split**:\n",
        "```python\n",
        "for split in ['train', 'val']:\n",
        "```\n",
        "- The function evaluates the loss for both the training and validation data. The loop iterates over these two splits.\n",
        "\n",
        "### 4. **Initialize Loss Storage**:\n",
        "```python\n",
        "losses = torch.zeros(eval_iters)\n",
        "```\n",
        "- A tensor is initialized to store the loss values for a specified number of iterations (`eval_iters`). This tensor will hold the loss values for each iteration, and the mean of these values will be used to estimate the average loss.\n",
        "\n",
        "### 5. **Evaluate the Model**:\n",
        "```python\n",
        "for k in range(eval_iters):\n",
        "    X, Y = get_batch(split)\n",
        "    logits, loss = model(X, Y)\n",
        "    losses[k] = loss.item()\n",
        "```\n",
        "- Within the loop, batches of data are sampled using the `get_batch` function.\n",
        "- For each batch, the model's forward pass is executed to obtain the logits and the loss.\n",
        "- The computed loss is then stored in the `losses` tensor.\n",
        "\n",
        "### 6. **Store Mean Loss**:\n",
        "```python\n",
        "out[split] = losses.mean()\n",
        "```\n",
        "- The mean of all computed losses for the current split (train or val) is calculated and stored in the `out` dictionary.\n",
        "\n",
        "### 7. **Switch Back to Training Mode**:\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "- After evaluating the losses, the model is switched back to training mode in preparation for further training iterations.\n",
        "\n",
        "### 8. **Return Loss Estimates**:\n",
        "```python\n",
        "return out\n",
        "```\n",
        "- Finally, the function returns the dictionary `out` containing the estimated average losses for both the training and validation splits.\n",
        "\n",
        "In essence, the `estimate_loss` function provides a snapshot of the model's performance at a given point in training by calculating the average loss over a set number of batches for both training and validation datasets. Evaluating the model periodically during training is useful to monitor its progress, diagnose issues, and potentially apply early stopping if the validation loss starts to increase.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1L7LraviZnxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The \"mathematical trick\" in self-attention"
      ],
      "metadata": {
        "id": "iDlXKlj1biju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging past context with for loops, the weakest form of aggregation\n",
        "\n",
        "The lecturer is introducing the concept of self-attention, a critical component of Transformer models, but before diving directly into the complexities of self-attention, the lecturer starts with a simpler concept to help build intuition. Let's distill and explain the key ideas from this transcript:\n",
        "\n",
        "### 1. **Goal: Tokens Communicating with Each Other**:\n",
        "- In sequence models, the tokens (words, characters, or other discrete units) often exist in isolation. The goal is to let these tokens \"talk\" to each other, allowing information to flow between them.\n",
        "\n",
        "### 2. **Directional Communication**:\n",
        "- An essential point made is that tokens should only communicate with their past, not the future. This is because, in sequence prediction tasks, you don't have access to future information when predicting the next token.\n",
        "\n",
        "### 3. **Simplest Form of Communication: Averaging**:\n",
        "- As an initial approach to make tokens communicate, the lecturer introduces the idea of averaging. Specifically, for a given token in a sequence, its new representation is the average of its current and all preceding tokens.\n",
        "    - This is a basic way of letting a token \"know\" about its past, but it's a weak form of communication since it loses a lot of information about the exact order or importance of previous tokens.\n",
        "\n",
        "### 4. **Implementation of Averaging**:\n",
        "- The provided code demonstrates how this averaging can be done using nested loops. For every token, the code calculates the mean of all preceding tokens (including the current one). This operation is performed for each sequence in the batch.\n",
        "\n",
        "### 5. **Bag of Words (BoW)**:\n",
        "- The term \"backward\" or \"BoW\" is used to describe this averaging operation. It's reminiscent of the Bag of Words model in natural language processing, where a text is represented just by the counts of its words, without considering their order.\n",
        "\n",
        "### 6. **Efficiency and Future Improvements**:\n",
        "- The lecturer hints that the current implementation using for-loops is not efficient and that more efficient approaches will be introduced later. This is a setup for introducing the matrix operations that enable efficient self-attention in Transformers.\n",
        "\n",
        "### 7. **Setting the Stage for Self-Attention**:\n",
        "- This entire discussion serves as a foundation for the more complex self-attention mechanism. While averaging is a simple way to let tokens communicate, self-attention allows tokens to \"weigh\" the importance of other tokens, leading to a much richer form of communication. Instead of treating all past tokens equally (as in averaging), self-attention enables the model to decide which tokens are more relevant or important for the current context.\n",
        "\n",
        "In summary, the lecturer is guiding the audience through the initial steps of understanding how tokens in a sequence can share information with each other. Starting with a simple averaging method sets the stage for the more sophisticated and powerful self-attention mechanism that will be introduced later."
      ],
      "metadata": {
        "id": "9e1idlUff9Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want the tokens in T to talk to each other"
      ],
      "metadata": {
        "id": "ng-oh5RMb8Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjFhM_3VbtqC",
        "outputId": "69fc1bb6-bef6-45ac-a15f-eccc9cafe2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "bWyVqjEuc_zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP6Wu4_FebUu",
        "outputId": "f77ce0b7-2034-424e-9bed-96a944459f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj-NVNm_ed6_",
        "outputId": "dc5d983a-b9a4-4ea7-8d9b-49c2069eb830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The trick in self-attention: matrix multiply as weighted aggregation\n",
        "\n",
        "The lecturer is diving deeper into the mechanics of matrix multiplication as a precursor to understanding self-attention. Let's unpack the key concepts and intuitions being communicated:\n",
        "\n",
        "### 1. **Matrix Multiplication as a Form of Aggregation**:\n",
        "- The lecture starts with a fundamental idea: matrix multiplication can be seen as a form of weighted aggregation, where one matrix's elements can be used to aggregate or combine the elements of another matrix.\n",
        "\n",
        "### 2. **A Simple Matrix Multiply**:\n",
        "- With matrices `a` (a 3x3 matrix of ones) and `b` (a 3x2 matrix of random values), multiplying them results in matrix `c`. This operation aggregates columns of `b` based on the rows of `a`.\n",
        "- Since matrix `a` has rows filled with ones, the resulting matrix `c` effectively sums up the rows of matrix `b`.\n",
        "\n",
        "### 3. **Introducing the Concept of Time with Lower Triangular Matrices**:\n",
        "- The lecturer introduces `torch.tril()`, which returns the lower triangular part of a matrix. This is crucial for the self-attention mechanism, where you often want tokens to attend only to prior tokens (and not future ones).\n",
        "- When using this triangular matrix to multiply with `b`, the resulting matrix `c` aggregates different portions of `b` based on the number of ones in each row of the triangular matrix.\n",
        "\n",
        "### 4. **Aggregation Beyond Simple Summation: Averages**:\n",
        "- While the earlier steps showed aggregation in terms of summation, the lecturer then extends this to averaging. By normalizing the rows of the lower triangular matrix to sum to 1, the matrix multiplication effectively computes averages of rows in matrix `b`.\n",
        "\n",
        "### 5. **Manipulating Aggregation with Matrix Elements**:\n",
        "- The central idea is that by changing the elements of the multiplying matrix (in this case, the modified `a`), you can control the aggregation type and extent. For self-attention, this is pivotal as different tokens might need to be aggregated differently based on the context.\n",
        "\n",
        "### 6. **Foundation for Self-Attention**:\n",
        "- The entire discussion sets the stage for self-attention. In the Transformer model's self-attention mechanism, tokens are aggregated based on their importance or relevance, and this importance is dynamically learned. Instead of static ones, zeros, or normalized values, the Transformer learns weights (or attention scores) to aggregate tokens in a context-aware manner.\n",
        "\n",
        "In summary, the lecturer is building foundational knowledge on how matrix multiplication can be used for various aggregation operations. By understanding these basic mechanics, one can then appreciate the more dynamic and powerful aggregations offered by the self-attention mechanism in Transformer models."
      ],
      "metadata": {
        "id": "OHvs23wXgPD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VlV9wPcgSR4",
        "outputId": "d7d7e8a2-2679-4eb8-8f4b-3778a7626ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkGgsYGlcZ7",
        "outputId": "468b2b91-fbbf-4713-b0b6-37ad1b16c02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp4JPgHrlsGC",
        "outputId": "a1b13b3b-33e6-49c3-abdf-23a8bc5b302e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, dim=1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a=\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b=\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c=\")\n",
        "print(c)\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9clRs2DmPuM",
        "outputId": "1b676f3b-4f07-4986-9ff6-c0fae48786a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2: using matrix multiply for a weighted aggregation\n",
        "\n",
        "The lecturer is delving into a more efficient and elegant way to perform the aggregation operation, drawing upon the power of matrix multiplication in PyTorch. Here are the main ideas and intuitions being conveyed:\n",
        "\n",
        "### 1. **Efficient Weighted Aggregation Using Matrix Multiplication**:\n",
        "- The goal is to aggregate (or combine) sequences in the tensor `x` based on a set of weights. Previously, we've done this with loops. Now, the focus is on achieving the same with matrix multiplication, which is more computationally efficient.\n",
        "\n",
        "### 2. **Constructing the Weight Matrix**:\n",
        "- The `wei` matrix is constructed as a lower triangular matrix where each row represents the weights to be used for aggregation. Since the goal is to average the previous tokens, the values in this matrix are decreasing fractions, ensuring that each token aggregates information only from preceding tokens.\n",
        "- The matrix is then normalized so that each row sums to 1, ensuring that the multiplication operation results in a weighted average.\n",
        "\n",
        "### 3. **Batched Matrix Multiplication**:\n",
        "- PyTorch provides a way to perform batched matrix multiplication. Even if the matrices' dimensions aren't perfectly aligned, PyTorch can infer a batch dimension and apply the multiplication across each batch.\n",
        "- In the case of `wei @ x`, the operation is applied across each batch, performing a weighted aggregation for each sequence in `x`.\n",
        "\n",
        "### 4. **Interpreting the Result**:\n",
        "- The result of the multiplication, `xbow2`, represents `x` aggregated using the weights in `wei`. If you observe a specific row in `xbow2`, it's the result of aggregating the corresponding sequence in `x` based on the weights in `wei`.\n",
        "- The use of the lower triangular weight matrix ensures that the aggregation does not incorporate \"future\" tokens, maintaining the temporal integrity of the sequences.\n",
        "\n",
        "### 5. **Comparison with Previous Aggregation**:\n",
        "- The lecturer emphasizes that the results obtained from this matrix multiplication approach (`xbow2`) are identical to the previous loop-based method (`xbow`). This is confirmed using `torch.allclose(xbow, xbow2)`, which returns `True`, indicating that both tensors are numerically close.\n",
        "\n",
        "### 6. **Advantages of This Approach**:\n",
        "- The primary advantage of using matrix multiplication is efficiency. Matrix operations are highly optimized in libraries like PyTorch, and they can be parallelized easily on GPUs. This leads to faster computations compared to loop-based methods.\n",
        "- The approach provides a foundation for more advanced aggregation methods, such as self-attention in the Transformer model. By adjusting the weight matrix (`wei` in this example), different aggregation behaviors can be achieved.\n",
        "\n",
        "In summary, the lecturer is emphasizing the power and efficiency of matrix operations in PyTorch, demonstrating how they can be leveraged for tasks like aggregation. This understanding is crucial when moving towards complex architectures like the Transformer, where such operations are foundational."
      ],
      "metadata": {
        "id": "x8q3eMOaorZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our 'a'\n",
        "\n",
        "# reminder\n",
        "# B,T,C = 4,8,2 # batch, time, channels\n",
        "# x = torch.randn(B,T,C)\n",
        "\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(dim=1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg-FtWgTpdfw",
        "outputId": "2155d07e-39cd-4b37-8bdd-390b2c978ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPmciydJG4jg",
        "outputId": "cdcc87e7-8709-4bf1-a10e-e1d9415d7b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3iqkGp9G6z5",
        "outputId": "3ed1a047-2418-403e-8528-b8872daaf34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our 'b'\n",
        "# batched matrix multiply\n",
        "\n",
        "xbow2 = wei @ x # (T,T) @ (B,T,C) -> (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQtGvWDcGaFE",
        "outputId": "86542abd-3ef1-46ae-e8cf-3f167b1b4499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 3: adding softmax\n",
        "\n",
        "The lecturer is introducing a more sophisticated version of the aggregation operation that's foundational to the Transformer's self-attention mechanism. Let's break down the main ideas:\n",
        "\n",
        "### 1. **Softmax as a Normalization Tool**:\n",
        "- Softmax is a mathematical function that turns any sequence of numbers into a probability distribution. Its main characteristic is that it amplifies differences, making larger numbers significantly larger and smaller numbers tend towards zero, while ensuring the resulting sequence sums to 1.\n",
        "  \n",
        "### 2. **Constructing the Softmax-Ready Matrix**:\n",
        "- The `tril` matrix represents the lower triangular matrix filled with ones, ensuring that information from future tokens is not used.\n",
        "- The `wei` matrix starts as all zeros, representing equal attention or affinity to all past tokens.\n",
        "- The `masked_fill` function is used to set the values in the upper triangle (representing future tokens) to negative infinity. This ensures that, after the softmax, these positions will indeed become zero, enforcing the temporal structure.\n",
        "\n",
        "### 3. **Softmax and Aggregation**:\n",
        "- When applying softmax to `wei`, the negative infinities become zeros, and the zeros (representing equal attention) become fractions that sum to 1 within each row. This results in a matrix similar to the one obtained in the previous version, but this time derived from data-driven affinities rather than fixed weights.\n",
        "  \n",
        "### 4. **Interpreting the Result**:\n",
        "- The resulting `xbow3` is identical to the previous versions, but the process to achieve it is more dynamic and adaptable.\n",
        "- The zeros in `wei` represent \"affinities\" or interaction strengths. Right now, they're constant, meaning each token pays equal attention to all its past tokens. But in actual self-attention mechanisms, these affinities are learned and data-dependent.\n",
        "\n",
        "### 5. **Preview of Self-Attention**:\n",
        "- The lecturer hints at how this approach will evolve into the self-attention mechanism. Tokens will \"look\" at each other, and based on their values (or content), they'll assign different levels of attention or affinity to each other. This dynamic mechanism allows the model to decide which parts of the input are more relevant or interesting relative to others.\n",
        "- The matrix multiplication trick with a lower triangular matrix is key for efficient computation of these weighted aggregations.\n",
        "\n",
        "### 6. **Significance**:\n",
        "- The transition from a fixed aggregation mechanism to a data-driven, dynamic one is crucial. It allows the model to capture complex relationships and dependencies in the data, paving the way for the power and flexibility of the Transformer model.\n",
        "\n",
        "In summary, this segment prepares the foundation for introducing self-attention. By using softmax and matrix multiplication, we're moving towards a flexible and efficient mechanism where tokens can decide how much attention they pay to other tokens, based on the data they're processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "2prbdMOhJZZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ZNDUoxJcvh",
        "outputId": "a17283e8-52cc-40a3-a246-e901397acc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 4: self-attention\n",
        "\n",
        "The lecturer delves deep into the concept of self-attention, especially in the context of neural networks. Here are the key intuitions and ideas they are emphasizing:\n",
        "\n",
        "1. **Data-Dependent Interaction**:\n",
        "    - Traditional neural networks often process data in predefined ways. However, in real-world sequences, certain elements or tokens might be more relevant to some tokens than others. The self-attention mechanism enables a neural network to focus on different parts of the input data in a data-dependent manner.\n",
        "\n",
        "2. **Role of Keys and Queries**:\n",
        "    - Each token in a sequence emits two vectors: a 'query' and a 'key'.\n",
        "        - **Query**: Represents what a token is looking for. It's a signal about the token's interest or requirements.\n",
        "        - **Key**: Represents the content or the identity of the token. It's a descriptor of what the token offers or represents.\n",
        "    - The interaction between tokens is determined by the dot product of their respective queries and keys. If a token's query aligns well with another token's key, they will have a high affinity, meaning the first token finds the second one particularly relevant or interesting.\n",
        "\n",
        "3. **Affinity Matrix**:\n",
        "    - The result of the dot product between all queries and keys is an affinity matrix, which captures the relationships or affinities between every pair of tokens in a sequence.\n",
        "    - This matrix is not constant across batches, meaning different input sequences will produce different affinities based on their content.\n",
        "\n",
        "4. **Masking & Sequence Order**:\n",
        "    - In sequences, the order often matters. For instance, future words shouldn't influence past words in a sentence.\n",
        "    - To achieve this, an upper-triangle mask is applied to the affinity matrix, ensuring a token doesn't attend to future tokens. This introduces the concept of causality into the mechanism.\n",
        "\n",
        "5. **Normalization**:\n",
        "    - Raw affinities, derived from dot products, can have a wide range. To turn these into probabilities representing the importance or weight of each interaction, a softmax function is applied. This ensures the weights are between 0 and 1 and sum to 1, making them interpretable as probabilities.\n",
        "\n",
        "6. **Value Vector**:\n",
        "    - In addition to keys and queries, each token emits a 'value' vector.\n",
        "    - While keys and queries determine the relationship between tokens, the value vector represents the information a token communicates when it's deemed relevant by another token.\n",
        "    - Instead of directly aggregating the original data (X), the self-attention mechanism aggregates these value vectors based on the computed weights from the affinity matrix.\n",
        "    - Essentially, the value vector holds the \"message\" or information a token will send to others.\n",
        "\n",
        "7. **Dimensionality & Head Size**:\n",
        "    - The concept of a 'head' in self-attention refers to a single instance of the self-attention mechanism operating on a reduced dimensionality (head size).\n",
        "    - This smaller dimensionality makes the computation more manageable and allows for multiple heads to operate in parallel, each potentially capturing different types of relationships.\n",
        "\n",
        "In essence, self-attention allows each token in a sequence to dynamically determine which other tokens are most relevant to it, and then aggregate information from those tokens in a data-dependent manner. This mechanism is a cornerstone of models like Transformers, enabling them to capture complex relationships in data."
      ],
      "metadata": {
        "id": "PQ5D-CBHaUVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = k @ q.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "out.shape\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "580ix-o-aXUj",
        "outputId": "d8a9f97b-0815-4519-af56-ad6af98f1ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKuEusYLdUJZ",
        "outputId": "3948239f-e309-4096-a250-b830540ed95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
              "         [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
              "         [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4289, 0.5711, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5413, 0.1423, 0.3165, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0635, 0.8138, 0.0557, 0.0669, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4958, 0.0758, 0.2224, 0.0156, 0.1905, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3957, 0.1127, 0.3724, 0.0024, 0.1128, 0.0040, 0.0000, 0.0000],\n",
              "         [0.0229, 0.5252, 0.0084, 0.0047, 0.2768, 0.0983, 0.0637, 0.0000],\n",
              "         [0.0021, 0.0327, 0.0042, 0.0821, 0.0244, 0.8253, 0.0154, 0.0139]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5842, 0.4158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5148, 0.3227, 0.1624, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1818, 0.0991, 0.6131, 0.1060, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4658, 0.0065, 0.0221, 0.4951, 0.0105, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5680, 0.0253, 0.0580, 0.1483, 0.0664, 0.1339, 0.0000, 0.0000],\n",
              "         [0.4906, 0.0256, 0.0375, 0.0027, 0.3457, 0.0177, 0.0802, 0.0000],\n",
              "         [0.0167, 0.0192, 0.2619, 0.0270, 0.0554, 0.0856, 0.4455, 0.0886]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.7831, 0.2169, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0058, 0.9929, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3320, 0.4751, 0.1769, 0.0160, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0814, 0.1088, 0.5362, 0.2061, 0.0674, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1960, 0.1972, 0.1274, 0.1287, 0.2556, 0.0951, 0.0000, 0.0000],\n",
              "         [0.0356, 0.0119, 0.1669, 0.2186, 0.0318, 0.5158, 0.0195, 0.0000],\n",
              "         [0.0101, 0.0368, 0.0097, 0.0414, 0.3430, 0.1230, 0.0089, 0.4272]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without softmax\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = k @ q.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hoWiFCed-WT",
        "outputId": "0fbac75c-fa31-4473-d679-43091fda75e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629, -3.3334, -1.0226,  0.7836, -1.2566, -0.3126,  1.0876, -1.8044],\n",
              "        [-1.3011, -1.6556, -1.2606, -0.8014,  0.0187,  2.4152,  1.9652, -0.4126],\n",
              "        [ 0.5652,  0.1040,  0.0762, -0.3368, -0.7880, -0.1106, -0.2621, -0.8306],\n",
              "        [ 2.1616,  3.3782, -0.3813, -0.8496, -1.3204, -0.9931, -0.3158,  0.5899],\n",
              "        [-1.0674, -2.1825, -0.9843, -0.5602,  2.0363,  3.3449,  0.6091, -0.7987],\n",
              "        [ 1.9632,  1.0415, -1.4303, -1.1701,  0.8638, -2.5229,  1.2616, -0.5856],\n",
              "        [ 1.0765, -0.0557,  0.0749, -1.2927,  0.3719,  1.4187, -0.5484,  0.6433],\n",
              "        [-0.4530,  0.2927, -0.9547, -1.0260,  0.9258,  1.2196,  0.8048,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = k @ q.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cypLnx-4eUJN",
        "outputId": "e2913848-37f3-43a9-9dac-a1b168dfb9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.3011, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.5652,  0.1040,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 2.1616,  3.3782, -0.3813, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0674, -2.1825, -0.9843, -0.5602,  2.0363,    -inf,    -inf,    -inf],\n",
              "        [ 1.9632,  1.0415, -1.4303, -1.1701,  0.8638, -2.5229,    -inf,    -inf],\n",
              "        [ 1.0765, -0.0557,  0.0749, -1.2927,  0.3719,  1.4187, -0.5484,    -inf],\n",
              "        [-0.4530,  0.2927, -0.9547, -1.0260,  0.9258,  1.2196,  0.8048,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = k @ q.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpzriuMJed42",
        "outputId": "8d09cf86-263f-49b4-9077-6dc81c49434d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
              "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
              "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "v = value(x)\n",
        "wei = k @ q.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ v\n",
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zp4YgVze7hO",
        "outputId": "f5db2fb9-2c3a-4e51-d1e8-73a79511af4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
              "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
              "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y3JuAKkO74U",
        "outputId": "e21007d5-273e-446f-d4f1-4f9bb959f446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
              "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
              "        [ 0.2507,  0.1815, -0.0388, -0.2458, -0.1356,  0.2369, -0.1588, -0.3209,\n",
              "         -0.4772,  0.4530,  0.4388, -0.3604, -0.0859, -0.0803,  0.1115,  0.9138],\n",
              "        [ 0.3288,  0.0950, -0.1875, -0.0916, -0.0079,  0.0883, -0.0678, -0.1830,\n",
              "         -0.4008,  0.0761,  0.3542, -0.1453, -0.1970, -0.0976,  0.0109,  1.0278],\n",
              "        [ 0.6067, -0.4271, -0.2246,  0.2273, -0.1100, -0.2183, -0.3709, -0.1340,\n",
              "         -0.1130,  0.6494,  0.6441, -0.1387,  0.2489,  0.2713, -0.0351,  1.2031],\n",
              "        [ 0.2010,  0.8507,  0.6533,  0.2228,  0.3173,  0.8365,  0.6526,  0.3822,\n",
              "         -0.6315, -1.2205, -0.4374, -0.2859, -0.9985,  0.1108, -0.1001,  0.5346],\n",
              "        [ 0.1453,  0.4755,  0.1447, -0.2496, -0.0209,  0.4674,  0.0808, -0.2074,\n",
              "         -0.5866,  0.0157,  0.1711, -0.3741, -0.3699, -0.1248,  0.1164,  0.7404],\n",
              "        [-0.2268,  0.2806, -0.0834,  0.2215,  0.1804,  0.2529, -0.0778, -0.2663,\n",
              "         -0.1468,  0.1037,  0.0856,  0.1898, -0.0721, -0.0397,  0.3974,  0.4161],\n",
              "        [-0.1450,  0.2375, -0.1882,  0.3479,  0.1843,  0.1369, -0.0581,  0.1339,\n",
              "         -0.0594, -0.0362,  0.0767,  0.2613,  0.0609, -0.1358,  0.0764,  0.3417]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UclrS7jDO7ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code explanation\n",
        "\n",
        "### 1. Defining the Head Size\n",
        "```python\n",
        "head_size = 16\n",
        "```\n",
        "Here, we're defining the size of the attention \"head\". An attention head is essentially a set of parameters that learns to attend to different parts of the input. In the context of the Transformer architecture (where self-attention is most famously used), there can be multiple heads, but in this example, we are just looking at a single head.\n",
        "\n",
        "### 2. Key, Query, and Value Projections\n",
        "```python\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "```\n",
        "For every token in the input, we compute three vectors:\n",
        "- **Key (k)**: Represents what content a token contains.\n",
        "- **Query (q)**: Represents what content a token is looking for.\n",
        "- **Value (v)**: Contains the information a token has to offer if it's attended to.\n",
        "\n",
        "These are computed using three separate linear transformations (i.e., matrix multiplications) of the input. The input has a dimension of `C`, and we transform it to the `head_size` which is 16 in this case.\n",
        "\n",
        "### 3. Computing the Key, Query, and Value Matrices\n",
        "```python\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "v = value(x)\n",
        "```\n",
        "For each of the linear transformations defined above, we pass the input \\( x \\) through them to obtain the key, query, and value matrices. The shapes indicate that for each batch and for each time step, we have a 16-dimensional vector representing the key/query/value for that specific token.\n",
        "\n",
        "### 4. Attention Weights Calculation\n",
        "```python\n",
        "wei = k @ q.transpose(-2, -1)  # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "```\n",
        "Here, we're computing the attention weights. The idea is to calculate how much each token should attend to every other token, including itself.\n",
        "\n",
        "By multiplying the key matrix with the transposed query matrix, we're essentially computing the dot product between the key of one token and the query of another for all combinations of tokens. This results in a matrix of shape \\( (B, T, T) \\), where each row represents a token and each column in that row tells us how much that token should attend to every other token.\n",
        "\n",
        "### 5. Masking Future Information\n",
        "```python\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "```\n",
        "The self-attention mechanism in many applications, like language modeling, shouldn't allow information flow from the future. That is, when predicting a word in a sentence, you shouldn't use future words as context.\n",
        "\n",
        "The `torch.tril` function is used to get the lower triangular part of a matrix. This is used to mask the upper triangle (which represents future tokens) by setting them to negative infinity.\n",
        "\n",
        "### 6. Softmax Normalization\n",
        "```python\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "```\n",
        "The attention weights are passed through a softmax function to ensure they are normalized and sum to 1. This way, they can be treated as probabilities indicating how much each token should attend to every other token.\n",
        "\n",
        "### 7. Aggregating Information\n",
        "```python\n",
        "out = wei @ v\n",
        "```\n",
        "Finally, using the normalized attention weights, we aggregate information from the value vectors. This gives us a new representation for each token that's a weighted combination of all value vectors, based on the attention weights.\n",
        "\n",
        "To summarize, the self-attention mechanism allows each token to dynamically determine which other tokens are important (or relevant) to it and aggregate information from them accordingly. This is crucial in tasks like language modeling, where context is essential."
      ],
      "metadata": {
        "id": "A-YrQqdXuPHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `the self-attention mechanism`\n",
        "\n",
        "### Self-Attention Mechanism\n",
        "At a high level, the self-attention mechanism in Transformer models allows each position in an input sequence to focus on, or attend to, all positions in the same sequence. This is done to compute a representation of the sequence. The mechanism uses three vectors: **key**, **query**, and **value**, which are derived from the input.\n",
        "\n",
        "### Key, Query, and Value\n",
        "1. **Key (K)**: It's a set of vectors that represent the input data. When we want to fetch some information, we check the keys.\n",
        "2. **Query (Q)**: It's also a set of vectors that represent the input. Queries are like questions about certain parts of the data.\n",
        "3. **Value (V)**: For each key, there's an associated value. Once we've identified which keys are relevant (using the query), we'll use the associated values to fetch the desired information.\n",
        "\n",
        "The weights of the **key**, **query**, and **value** transformations (achieved using `nn.Linear` layers) are randomly initialized and get updated during training. As the model trains, these weights are fine-tuned to help the model better focus on important parts of the input sequence for various tasks.\n",
        "\n",
        "### Matrix Multiplication for Attention Scores\n",
        "\n",
        "Now, let's focus on the specific line:\n",
        "```python\n",
        "wei = k @ q.transpose(-2, -1)\n",
        "```\n",
        "\n",
        "#### Step-by-Step Explanation:\n",
        "\n",
        "1. **Transpose the Query Matrix**:\n",
        "   - `q.transpose(-2, -1)` transposes the last two dimensions of the query matrix `q`.\n",
        "   - Given `q` has a shape of `(B, T, 16)`, after transposing, it will have a shape of `(B, 16, T)`.\n",
        "\n",
        "2. **Matrix Multiplication**:\n",
        "   - The `@` operator in PyTorch performs matrix multiplication.\n",
        "   - `k @ q.transpose(-2, -1)` computes the matrix product of the key matrix `k` and the transposed query matrix.\n",
        "   - Let's break down the shapes:\n",
        "     - `k` has a shape of `(B, T, 16)`.\n",
        "     - Transposed `q` has a shape of `(B, 16, T)`.\n",
        "   - For matrix multiplication to be valid, the inner dimensions must match. Here, the inner dimension is 16 for both matrices.\n",
        "   \n",
        "3. **Resultant Matrix**:\n",
        "   - The result of the matrix multiplication is a tensor of shape `(B, T, T)`.\n",
        "   - Each entry `(i, j)` in this matrix represents the attention score between the `i-th` and `j-th` position in the sequence.\n",
        "   - Intuitively, the value at position `(i, j)` indicates how much the model should focus on the `j-th` position when encoding information for the `i-th` position.\n",
        "\n",
        "This matrix of attention scores (`wei`) will then be passed through a softmax function (after masking, which is covered in the subsequent lines) to get the actual attention weights. These weights determine how much each position in the sequence should contribute to the representation of every other position.\n",
        "\n",
        "In summary, the line `wei = k @ q.transpose(-2, -1)` computes the raw attention scores for all pairs of positions in the input sequence. These scores will be further processed to produce the actual attention weights and then used to compute a weighted sum of the value vectors to produce the output of the attention mechanism."
      ],
      "metadata": {
        "id": "Dn-o-mJtOMoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## some questions\n",
        "\n",
        "### Question 1:\n",
        "\n",
        "**Is \\( v \\) (B, T, 16)?**\n",
        "Yes, \\( v \\) is of shape (B, T, 16). The `value` linear layer transforms the input \\( x \\) to have a size of `head_size` (which is 16 in this case) for the last dimension.\n",
        "\n",
        "**Is \\( wei \\) (B, T, T)?**\n",
        "Yes, \\( wei \\) is of shape (B, T, T). The calculation `k @ q.transpose(-2, -1)` results in this shape. Let's break this down:\n",
        "\n",
        "1. \\( k \\) is of shape (B, T, 16).\n",
        "2. \\( q \\) is transposed at its last two dimensions to have shape (B, 16, T).\n",
        "3. The matrix multiplication of \\( k \\) and \\( q \\) produces a tensor of shape (B, T, T).\n",
        "\n",
        "**How can we do \\( wei @ v \\)?**\n",
        "This is a good observation. Indeed, if you try to directly multiply \\( wei \\) and \\( v \\) without understanding their shapes, it might seem incompatible. Here's how it works:\n",
        "\n",
        "1. \\( wei \\) is of shape (B, T, T).\n",
        "2. \\( v \\) is of shape (B, T, 16).\n",
        "3. The last dimension of \\( wei \\) matches the second-to-last dimension of \\( v \\). This allows the matrix multiplication to proceed, resulting in a tensor of shape (B, T, 16), which matches the shape of \\( out \\).\n",
        "\n",
        "The multiplication is essentially computing a weighted sum of the value vectors (\\( v \\)) using the attention weights (\\( wei \\)) for each sequence in the batch and for each time step.\n",
        "\n",
        "### Question 2:\n",
        "\n",
        "**What does \\( out \\) represent in the example?**\n",
        "\n",
        "In the context of self-attention, \\( out \\) represents the attended output for each time step of the sequence. In simpler terms, it's the information gathered at each time step, considering the entire sequence, weighted by the importance (or attention scores) given by \\( wei \\).\n",
        "\n",
        "The entries in the matrix \\( wei \\) (B, T, T) tell us how much attention each element in the sequence (T) should pay to every other element when producing the output.\n",
        "\n",
        "For example, if you look at the first row of \\( wei[0] \\):\n",
        "\n",
        "```\n",
        "[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
        "```\n",
        "\n",
        "It's telling us that when producing the output for the first element in the sequence, it should entirely consider itself (with a weight of 1.0000) and not consider any other elements (with a weight of 0.0000).\n",
        "\n",
        "The output tensor \\( out \\) is the result of this weighted aggregation of the values (\\( v \\)) according to the attention scores in \\( wei \\).\n",
        "\n",
        "In the context of models like Transformers, this self-attention mechanism allows each element in the sequence to gather information from the entire sequence, giving different importance to different elements based on the attention scores. This makes the model highly powerful, especially in tasks like language modeling, where the context and relationship between words or tokens are crucial."
      ],
      "metadata": {
        "id": "b46S-q6TSWYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## more questions\n",
        "\n",
        "Absolutely! Let's delve into self-attention step by step.\n",
        "\n",
        "### 1. Difference between Keys and Values:\n",
        "\n",
        "**Keys** and **Queries** are used to compute the attention scores (or weights). They are both derived from the input but serve different purposes.\n",
        "\n",
        "- **Keys (K)**: They can be thought of as the \"labels\" for the input data in the context of attention. When we calculate attention scores, we compare the queries to these keys.\n",
        "  \n",
        "- **Queries (Q)**: They can be thought of as the \"questions\" we ask to the keys. We use queries to search for specific information in our input data.\n",
        "  \n",
        "- **Values (V)**: Once we have our attention scores, we use them to create a weighted combination of the values. Think of values as the actual content associated with each key.\n",
        "\n",
        "### 2. Affinity Matrix:\n",
        "\n",
        "The affinity matrix (often referred to as the attention scores matrix) is computed by taking the dot product of the queries with the keys. Each entry in this matrix signifies how much a particular query aligns with a particular key.\n",
        "\n",
        "- **Interpretation**: If the value in the \\($i^{th}$\\) row and \\($j^{th}$\\) column of the affinity matrix is high, it indicates that the \\($i^{th}$\\) query strongly aligns with the \\($j^{th}$\\) key. This means that the \\($i^{th}$\\) query is \"paying a lot of attention\" to the \\($j^{th}$\\) position of the input.\n",
        "\n",
        "- **Keys matching Queries**: If a key is perfectly aligned with a query (i.e., they are identical), their dot product will be high, resulting in a high attention score. After applying the softmax function, this score will likely be close to 1 (or very high relative to other scores in the same row).\n",
        "\n",
        "### 3. Functionality added with the dot product of the affinity matrix and values:\n",
        "\n",
        "The dot product of the affinity matrix with the values effectively creates a weighted combination of the values. It's a way of selecting or highlighting certain values based on the attention scores.\n",
        "\n",
        "- **Interpretation**: Suppose the \\($i^{th}$\\) row of the affinity matrix has high values at positions \\(j\\) and \\(k\\). This means when computing the output for position \\(i\\), it will strongly consider the information (values) from positions \\(j\\) and \\(k\\).\n",
        "\n",
        "By performing this operation, the self-attention mechanism allows the network to focus on different parts of the input sequence when producing the output for each time step.\n",
        "\n",
        "### 4. Analogy of Self-Attention:\n",
        "\n",
        "Imagine you're reading a book and come across the pronoun \"he\". To understand who \"he\" refers to, you need to pay attention to other parts of the text. Here:\n",
        "\n",
        "- **Query**: The word \"he\" that you're trying to understand.\n",
        "- **Keys**: All the potential nouns in the previous sentences or paragraphs that \"he\" might refer to.\n",
        "- **Values**: The actual meaning or context associated with each noun.\n",
        "\n",
        "The attention scores (affinity matrix) tell you which noun(s) in the preceding text the pronoun \"he\" most likely refers to. By taking a weighted combination of the values (meaning or context), you can understand the pronoun's reference more clearly.\n",
        "\n",
        "In essence, self-attention allows the model to look at other parts of the input to gather context and produce a more informed output.\n",
        "\n",
        "---\n",
        "\n",
        "I hope this breakdown provides a clearer understanding of the self-attention mechanism. Let me know if you'd like to delve deeper into any of these aspects!"
      ],
      "metadata": {
        "id": "Jqeuiwv1WlL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analogy: Cocktail Party Effect\n",
        "\n",
        "Imagine you're at a bustling cocktail party. There are numerous people talking simultaneously, music playing in the background, and glasses clinking. Amidst all this noise, you're trying to focus on a single conversation with a friend.\n",
        "\n",
        "However, occasionally, out of the corner of your ear, you hear your name being mentioned in another conversation. Instantly, your attention shifts to that other conversation, even if just for a split second.\n",
        "\n",
        "Let's break this scenario down in the context of self-attention:\n",
        "\n",
        "1. **Input Sequence (Values)**: This is the cacophony of sounds at the party — music, multiple conversations, clinking glasses, etc. Each sound or conversation can be thought of as a \"value\" in the self-attention mechanism.\n",
        "\n",
        "2. **Query**: Your current focus or attention. Initially, it's on the conversation with your friend. However, your brain is constantly sending out \"queries\" to check if there's something more important or relevant to focus on, like your name being mentioned.\n",
        "\n",
        "3. **Keys**: Every sound source at the party, be it a conversation, music, or noise, emits a \"key\". These keys help your brain decide which sound source to focus on based on your current \"query\".\n",
        "\n",
        "4. **Affinity Matrix (Attention Scores)**: Your brain computes an \"attention score\" for every sound source. When you hear your name, the attention score for that particular conversation spikes, and even amidst the noise, you're able to tune into that specific conversation, at least momentarily.\n",
        "\n",
        "5. **Output**: Based on the attention scores, your brain gives more importance to certain sound sources over others. While you're mainly focused on your friend's conversation, you might occasionally pick up snippets from other conversations, especially if they're deemed relevant (like when your name is mentioned).\n",
        "\n",
        "---\n",
        "\n",
        "Through this analogy, the idea is to emphasize that self-attention allows a system (or your brain, in this case) to dynamically focus on different parts of the input based on relevance or context. Just as you can tune into different conversations at a party based on their relevance to you, self-attention allows a model to focus on different parts of an input sequence based on context and importance."
      ],
      "metadata": {
        "id": "V0i_4zyaV6WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 1: Attention as a Communication Mechanism*\n",
        "\n",
        "The lecturer is providing a conceptual understanding of attention mechanisms. Here are the distilled key intuitions and ideas:\n",
        "\n",
        "1. **Attention as a Communication Mechanism**:\n",
        "    - At its core, attention is about facilitating \"communication\" between different parts of the data. Just as in a community where individuals communicate and share information, in a neural network with attention, different nodes or parts of the data communicate their importance relative to a query.\n",
        "\n",
        "2. **Directed Graph Representation**:\n",
        "    - The lecturer invokes the concept of a directed graph to visually explain attention. In such a graph, nodes represent data points or tokens, and edges (or arrows) represent the \"communication lines\" or the influence one node has on another.\n",
        "    \n",
        "3. **Weighted Aggregation of Information**:\n",
        "    - Each node in the graph holds some vector of information. When it communicates with another node, it doesn't blindly send this information. Instead, it sends a weighted version of it. This weighting is determined by the attention mechanism, ensuring that only the most relevant pieces of information are emphasized or \"heard\" by the receiving node.\n",
        "\n",
        "4. **Data-Dependent Communication**:\n",
        "    - The \"weight\" or importance given to the information from one node to another isn't static. It's dynamic and depends on the content of the nodes. This adaptability allows the model to focus on different parts of the input based on context.\n",
        "\n",
        "5. **Structure of the Graph in the Given Context**:\n",
        "    - The lecturer specifically talks about a certain structure of the graph where there are 8 nodes (because of a block size of 8 tokens). The nodes have a cascading structure of communication, where, for instance, the second node receives information from the first and itself, while the eighth node aggregates information from all previous nodes and itself. This structure is particularly relevant to the example at hand but attention can be applied more broadly.\n",
        "\n",
        "6. **Flexibility of Attention**:\n",
        "    - The final idea is the universality and flexibility of attention. While the lecturer describes a specific structure, they emphasize that attention can be applied to any arbitrary directed graph. This adaptability is what makes attention mechanisms so powerful and applicable across various domains, not just language modeling.\n",
        "\n",
        "In essence, the lecturer paints attention not just as a mere mathematical operation but as a dynamic \"conversation\" happening between different parts of the data, ensuring that the most relevant pieces of information are highlighted based on the current context or query."
      ],
      "metadata": {
        "id": "56kOqLuObr-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 2: No Innate Notion of Space\n",
        "\n",
        "The lecturer is highlighting a crucial property of attention mechanisms in the context of deep learning and contrasting it with more traditional operations like convolutions. Here are the distilled key intuitions and ideas:\n",
        "\n",
        "1. **Attention Operates Over Sets**:\n",
        "    - One of the core properties of attention mechanisms is that they operate over sets of vectors. In a set, the order of elements doesn't matter, meaning that, by default, attention doesn't inherently recognize any spatial or sequential ordering in the data.\n",
        "\n",
        "2. **No Innate Notion of Space**:\n",
        "    - Unlike certain operations (like convolutions) that are inherently spatial and work with structured, grid-like data, attention doesn't have an innate understanding of where each vector \"sits\" in relation to others. It treats every vector in its input set equally, without considering its position.\n",
        "\n",
        "3. **Need for Positional Encoding**:\n",
        "    - Since attention doesn't have a built-in sense of order or position, if the position of data points matters (as it often does in sequences like sentences or time-series data), we must provide that information explicitly. This is often done using positional encodings, which attach spatial or sequential information to each vector, enabling the model to understand the relative positions of the vectors.\n",
        "\n",
        "4. **Contrast with Convolutions**:\n",
        "    - The lecturer contrasts attention with convolutional operations. Convolutions inherently operate in a spatial domain. When a convolutional filter is applied to an image, it \"slides\" over the image, capturing local spatial patterns. This spatial understanding is intrinsic to how convolutions work.\n",
        "    \n",
        "5. **Explicit Addition of Spatial Information**:\n",
        "    - With attention, if we want to introduce a notion of space or order, it has to be done deliberately. In the context provided, the lecturer mentions \"relative position encodings\", which are added to the vectors to imbue them with a sense of position.\n",
        "\n",
        "In summary, the lecturer is emphasizing the \"space-agnostic\" nature of attention mechanisms. While this property allows attention to be highly flexible and applicable across various data types, it also means that, in contexts where position matters, we must provide that information explicitly. This is a different paradigm from operations like convolutions that naturally understand and operate in a spatial domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "BsvAN5dzccLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 3: No Cross-talk Across Batch Dimension\n",
        "\n",
        "The lecturer is emphasizing a critical aspect of how attention mechanisms operate in batched computations, particularly in the context of deep learning frameworks like PyTorch or TensorFlow. Here are the distilled key intuitions and ideas:\n",
        "\n",
        "1. **Batch Processing**:\n",
        "    - Deep learning frameworks process data in batches to optimize computations and utilize parallel processing capabilities of hardware like GPUs. A batch consists of multiple independent examples grouped together.\n",
        "\n",
        "2. **No Cross-talk Across Batch Dimension**:\n",
        "    - One of the primary insights the lecturer is imparting is that, within attention mechanisms (and many other deep learning operations), individual examples within a batch don't communicate with or influence each other. Each example in the batch is processed independently.\n",
        "\n",
        "3. **Batched Matrix Multiply as Parallel Processing**:\n",
        "    - When operations like matrix multiplication are applied to batches of data, they effectively run in parallel across the batch dimension. It's akin to performing the same operation multiple times for each example, but doing it simultaneously for efficiency.\n",
        "\n",
        "4. **Visualization as Pools of Nodes**:\n",
        "    - To help visualize this concept, the lecturer uses the analogy of directed graphs. In the context provided, if we consider a batch size of four and eight nodes (tokens) for each example in the batch, then instead of visualizing it as a single pool of \\( 4 \\times 8 = 32 \\) nodes, it's more accurate to envision it as four separate pools, each containing eight nodes. Each of these pools operates independently, with nodes within a pool communicating, but nodes across different pools do not.\n",
        "\n",
        "5. **Relevance for Attention**:\n",
        "    - In the context of attention, this independence across batches means that the attention mechanism for one example doesn't \"see\" or get influenced by the data from another example in the same batch.\n",
        "\n",
        "In essence, the lecturer is highlighting the independent nature of processing within batches. While batching is a technical requirement for efficient computation, it's essential to understand that, from a model's perspective, each example within a batch is treated as its isolated problem, uninfluenced by its batchmates."
      ],
      "metadata": {
        "id": "MB9XTKPcdFu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 4: Encoder vs. Decoder Blocks\n",
        "\n",
        "The lecturer is diving deep into the structure and behavior of the Transformer architecture, particularly the difference between its encoder and decoder blocks in the context of self-attention. Here's a step-by-step breakdown of the critical intuitions:\n",
        "\n",
        "1. **Directed Graph Structure**:\n",
        "    - In the context of language modeling using Transformers, there's an inherent structure where tokens are processed sequentially. Future tokens (or tokens yet to be seen) don't have access to past tokens. This is visualized as a directed graph, where arrows (or edges) denote the flow of information.\n",
        "\n",
        "2. **Conditional Communication**:\n",
        "    - Not all tasks using Transformers need this sequential, unidirectional flow. In some tasks, like sentiment analysis, it might be beneficial for all tokens to communicate with each other without any restrictions. This is because the entire sentence's context might be crucial for determining its sentiment.\n",
        "\n",
        "3. **Encoder vs. Decoder Blocks**:\n",
        "    - **Encoder Block**: This block allows full communication between all tokens. In the context of the provided code, an encoder block would mean removing the masking code, which restricts the flow of information. This unrestricted flow means every token can attend to every other token.\n",
        "    - **Decoder Block**: In contrast, the decoder is designed for tasks like language modeling, where predicting the next word shouldn't be influenced by future words (as that would be cheating). Therefore, the masking (using a triangular matrix) ensures that a token can't see future tokens.\n",
        "\n",
        "4. **Purpose of Masking in Decoders**:\n",
        "    - The masking in decoders ensures an \"auto-regressive\" format, meaning the prediction for a particular token only considers the tokens that came before it and not any tokens that come after. This masking is vital for tasks like language modeling to ensure the model doesn't get information from future tokens, which would make the task trivial.\n",
        "\n",
        "5. **Flexibility of Attention**:\n",
        "    - A key takeaway is the inherent flexibility of attention mechanisms. They don't impose strict constraints on how tokens communicate. Instead, the structure of communication (whether tokens can attend to all other tokens or just some) is imposed externally based on the task's requirements.\n",
        "\n",
        "In essence, the lecturer emphasizes that while the Transformer architecture has a specific structure, its components, like the attention mechanism, are highly flexible and can be adapted to different tasks by tweaking their connectivity patterns. This adaptability is what makes Transformers so versatile across various NLP tasks."
      ],
      "metadata": {
        "id": "X7Qm_JskeBXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 5: Different types of attention mechanisms in the Transformer architecture.\n",
        "\n",
        "1. **Definition of Self-Attention**:\n",
        "    - **Origin of the Term**: The term \"self-attention\" originates from the fact that the tokens (or nodes) are attending to themselves. This means they are trying to gather information or context from their own set.\n",
        "    - **Mechanics**: In self-attention, the keys, queries, and values all originate from the same source data, \\( X \\). This means that the model looks at the same input data to determine the relationships (keys and queries) and gather context (values).\n",
        "\n",
        "2. **Cross-Attention**:\n",
        "    - **Scenario**: There are scenarios, especially in encoder-decoder architectures, where we want the decoder to pay attention not just to its own outputs, but also to the outputs of the encoder. This is common in tasks like machine translation, where the decoder needs to refer back to the source sentence when generating the target sentence.\n",
        "    - **Mechanics**: In cross-attention, while the queries might come from one source (like the decoder's output), the keys and values come from another source (like the encoder's output). The decoder is essentially querying information or context from the encoder's outputs.\n",
        "\n",
        "3. **Versatility of Attention**:\n",
        "    - The lecturer emphasizes that the attention mechanism is inherently versatile. While the provided example uses self-attention, the underlying mechanism can be adapted for a variety of configurations, including cross-attention.\n",
        "    - This adaptability allows the Transformer architecture to handle a wide range of tasks, from simple sequence-to-sequence problems to more complex tasks requiring understanding and using context from multiple sources.\n",
        "\n",
        "In essence, the primary takeaway is understanding the distinction between self-attention (tokens attending to themselves) and cross-attention (tokens attending to a different set of tokens). Recognizing these differences and the reasons behind them is essential for grasping the broader capabilities of Transformer models."
      ],
      "metadata": {
        "id": "Fs1_txbCe3mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note 6: \"Scaled\" Self-Attention\n",
        "\n",
        "The lecturer is delving into a specific aspect of the attention mechanism, the concept of \"scaled\" self-attention. Let's unpack the key ideas:\n",
        "\n",
        "1. **Scaled Self-Attention**:\n",
        "    - The basic self-attention mechanism involves multiplying the query and the key, taking a softmax, and then using this to aggregate values. However, the \"attention is all you need\" paper introduced a modification: dividing the product of the query and key by the square root of the head size (often referred to as \\( \\sqrt{\\text{DK}} \\) where DK is the dimension of the key).\n",
        "    - This scaling factor might seem arbitrary at first, but it serves a crucial purpose, as explained next.\n",
        "\n",
        "2. **The Issue with Variance**:\n",
        "    - If you consider the keys and queries as coming from a unit Gaussian distribution (zero mean, unit variance), when you multiply them, the resulting values will have a variance on the order of the head size.\n",
        "    - This increase in variance is problematic when followed by a softmax operation, as the softmax is sensitive to the scale of its inputs.\n",
        "\n",
        "3. **Softmax Behavior**:\n",
        "    - Softmax attempts to convert its inputs into a probability distribution. When the inputs to the softmax are all relatively close to each other (or small), the output probabilities are spread out or \"diffuse\".\n",
        "    - However, if the inputs have large magnitudes, the softmax output tends to be \"peaky\", concentrating most of the probability mass on a single input. This behavior isn't desirable in attention mechanisms, as it would imply that when aggregating information, we're mostly focusing on a single input and ignoring the rest.\n",
        "\n",
        "4. **The Role of Scaling**:\n",
        "    - By dividing by \\( \\sqrt{\\text{DK}} \\), we are effectively controlling the variance of the product of the query and key. This ensures that, especially during initialization, the inputs to the softmax are not too extreme, preventing the undesired \"peaky\" behavior.\n",
        "    - The goal is to ensure that during the initial stages of training, the attention mechanism doesn't overly focus on just one input but takes a more balanced view, aggregating information from multiple inputs.\n",
        "\n",
        "In essence, the scaling in \"scaled\" self-attention is a normalization technique designed to make the attention mechanism more stable and robust, especially during the early stages of training."
      ],
      "metadata": {
        "id": "KTwfyXJKf70i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Embedding Table, Positional Encodings and Self-Attention"
      ],
      "metadata": {
        "id": "1Q43hl6VL9XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBo27KpqML9B",
        "outputId": "c307574d-15c5-4e90-8f85-01238e0e054d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 14:33:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-08-28 14:33:13 (36.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKUFGzVZMAK7",
        "outputId": "e879d8b7-a24c-42e5-cd75-194d3e5bfdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2000, val loss 4.2047\n",
            "step 500: train loss 2.6911, val loss 2.7087\n",
            "step 1000: train loss 2.5196, val loss 2.5303\n",
            "step 1500: train loss 2.4775, val loss 2.4829\n",
            "step 2000: train loss 2.4408, val loss 2.4523\n",
            "step 2500: train loss 2.4272, val loss 2.4435\n",
            "step 3000: train loss 2.4130, val loss 2.4327\n",
            "step 3500: train loss 2.3956, val loss 2.4212\n",
            "step 4000: train loss 2.4041, val loss 2.3992\n",
            "step 4500: train loss 2.3980, val loss 2.4084\n",
            "\n",
            "Whent iknt,\n",
            "Thowi, ht son, bth\n",
            "\n",
            "Hiset bobe ale.\n",
            "S:\n",
            "O-' st dalilanss:\n",
            "Want he us he, vet?\n",
            "Wedilas ate awice my.\n",
            "\n",
            "HDET:\n",
            "ANGo oug\n",
            "Yowhavetof is he ot mil ndill, aes iree sen cie lat Herid ovets, and Win ngarigoerabous lelind peal.\n",
            "-hule onchiry ptugr aiss hew ye wllinde norod atelaves\n",
            "Momy yowod mothake ont-wou whth eiiby we ati dourive wee, ired thoouso er; th\n",
            "To kad nteruptef so;\n",
            "ARID Wam:\n",
            "ENGCI inleront ffaf Pre?\n",
            "\n",
            "Wh om.\n",
            "\n",
            "He-\n",
            "LIERCKENIGUICar adsal aces ard thinin cour ay aney Iry ts I fr af ve y\n",
            "CPU times: user 24.5 s, sys: 1.52 s, total: 26 s\n",
            "Wall time: 34.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Overview\n",
        "\n",
        "### 1. **Model Overview**:\n",
        "The model described is a `BigramLanguageModel`, which suggests that it predicts the next token based on the current token (or context). However, the actual structure of the model includes a self-attention mechanism, which means it considers a broader context (up to `block_size` tokens) when making predictions.\n",
        "\n",
        "### 2. **Model Components**:\n",
        "\n",
        "#### a. **Embeddings**:\n",
        "- **Token Embeddings (`token_embedding_table`)**:\n",
        "  - This is an embedding layer that converts token indices to continuous vectors.\n",
        "  - Weights: A matrix of size `(vocab_size, n_embd)`. Each row corresponds to the embedding vector for a particular token.\n",
        "  \n",
        "- **Position Embeddings (`position_embedding_table`)**:\n",
        "  - Another embedding layer, but this one encodes the position of a token within a sequence. This is essential because the self-attention mechanism is permutation invariant and doesn't inherently know the order of tokens.\n",
        "  - Weights: A matrix of size `(block_size, n_embd)`. Each row is the embedding vector for a particular position in the sequence.\n",
        "\n",
        "#### b. **Self-Attention (Head)**:\n",
        "- **Key, Query, Value Linear Layers**:\n",
        "  - These are three separate linear transformations that convert the input embeddings into key, query, and value representations.\n",
        "  - Weights for each linear layer: `(n_embd, n_embd)`. No biases are used.\n",
        "  - The self-attention mechanism computes attention scores (affinities) by taking the dot product of the query and key, scales them, applies a softmax, and then aggregates information from the value vectors based on these scores.\n",
        "\n",
        "#### c. **Output Layer (`lm_head`)**:\n",
        "- A linear layer that maps the output of the self-attention mechanism to a distribution over the vocabulary.\n",
        "- Weights: A matrix of size `(n_embd, vocab_size)`, and biases of size `(vocab_size,)`.\n",
        "\n",
        "### 3. **Learning Ability**:\n",
        "\n",
        "Given its structure, here's what this model can learn:\n",
        "\n",
        "- **Token Representations**: Through the token embedding layer, the model learns a dense representation for each token in the vocabulary. This representation captures semantic and syntactic aspects of the tokens, optimized for the task at hand (language modeling in this case).\n",
        "  \n",
        "- **Positional Information**: The model learns how the position of a token within a sequence affects its meaning and its prediction for the next token.\n",
        "\n",
        "- **Contextual Relationships**: Through the self-attention mechanism, the model can learn how different tokens in a sequence relate to one another. It can decide which tokens to focus on (pay attention to) when predicting the next token.\n",
        "\n",
        "- **Vocabulary Predictions**: The `lm_head` allows the model to produce a distribution over the entire vocabulary for the next token prediction. It learns which tokens are likely to follow a given context.\n",
        "\n",
        "### 4. **Training Process**:\n",
        "\n",
        "The model is trained using the AdamW optimizer and cross-entropy loss. At each iteration, a batch of sequences is sampled, and the model predicts the next token for each token in these sequences. The gradients are computed based on the difference between the model's predictions and the actual next tokens, and the model's weights are updated to minimize this difference.\n",
        "\n",
        "In summary, this `BigramLanguageModel` is capable of learning contextual representations of sequences and making informed predictions about the next token in a sequence. It does so by leveraging token embeddings, positional embeddings, a self-attention mechanism, and a final linear layer to produce predictions."
      ],
      "metadata": {
        "id": "yDdPPzlBkVdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Modifications\n",
        "\n",
        "The lecturer's modifications introduce a more nuanced architecture for the Bigram Language Model. Here's a step-by-step breakdown of the changes and their purposes:\n",
        "\n",
        "### 1. **Removal of Unnecessary Parameters**:\n",
        "- **Vocab Size in Constructor**: The lecturer points out that there's no need to pass `vocab_size` as a parameter to the model's constructor since it's already defined globally. This helps reduce redundancy and simplifies the code.\n",
        "\n",
        "### 2. **Introduction of Embeddings**:\n",
        "- **Embedding Dimensionality**: A new hyperparameter `n_embd` is introduced, set to 32. This denotes the size of the embeddings for each token.\n",
        "- **Token Embedding Table**: Instead of directly mapping each token to logits (probabilities for the next token), the model now first maps each token to an embedding using the `token_embedding_table`. The embeddings serve as a dense representation of tokens, capturing semantic information.\n",
        "  \n",
        "### 3. **Language Modeling Head**:\n",
        "- **LM Head Layer**: After obtaining the token embeddings, the model uses a linear transformation (`self.lm_head`) to map these embeddings back to the vocabulary's dimensionality. This transformation effectively predicts the logits (pre-softmax probabilities) for the next token in the sequence.\n",
        "- **Separation of Roles**: By introducing the `token_embedding_table` and `lm_head`, the model separates the roles of token representation (embeddings) and next-token prediction (logits). This structure is more modular and allows for richer representations.\n",
        "\n",
        "### 4. **Forward Pass Modifications**:\n",
        "- **Token Embeddings**: The forward pass first obtains token embeddings using the `token_embedding_table`.\n",
        "- **Logits Computation**: These embeddings are then passed through the `lm_head` to obtain the logits for each token in the sequence.\n",
        "\n",
        "### 5. **Overall Implications**:\n",
        "- **Richer Representations**: The introduction of embeddings allows the model to learn richer, dense representations for each token. These representations can capture semantic and syntactic nuances, potentially leading to better predictions.\n",
        "- **Modularity**: By separating token representation and prediction, the model becomes more modular. This modularity is essential for more advanced architectures, like Transformers, where embeddings are used in various parts of the model.\n",
        "\n",
        "In summary, the lecturer's modifications transition the Bigram Language Model from a simple lookup-based predictor to a model that leverages dense embeddings for tokens, providing a foundation for more advanced modeling techniques."
      ],
      "metadata": {
        "id": "piSFXoZlOa1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing an intermediary layer that transforms the embeddings\n",
        "\n",
        "This code represents a slight modification from the previous model, introducing an intermediary layer that transforms the embeddings before predicting the next tokens. Let's break down its learning ability and the parameters being updated.\n",
        "\n",
        "### 1. **Model Structure**:\n",
        "The modified model, `BigramLanguageModel`, now comprises two main components:\n",
        "\n",
        "- **Embedding Layer**: `self.token_embedding_table` converts token indices into dense vectors of size `n_embd`. This table holds the learned representations of each character/token in the vocabulary.\n",
        "- **Linear Transformation (Language Modeling Head)**: `self.lm_head` is a fully connected linear layer that transforms the embeddings from `n_embd` dimensions to the size of the vocabulary. This is used to produce logits that predict the next token.\n",
        "\n",
        "### 2. **Learning Process**:\n",
        "The model learns by adjusting both the embeddings and the weights of the linear transformation to minimize prediction error.\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - The model first fetches the embeddings for the input tokens from the `token_embedding_table`.\n",
        "  - These embeddings are then passed through the `lm_head` linear layer to produce logits for predicting the next tokens.\n",
        "- **Loss Calculation**: The loss is computed by comparing the predicted logits against the actual next tokens using the cross-entropy loss.\n",
        "\n",
        "### 3. **Training Loop**:\n",
        "This is where the iterative process of learning takes place:\n",
        "\n",
        "- **Batch Sampling**: In each iteration, a batch of sequences (`xb`) and their corresponding next tokens (`yb`) are sampled.\n",
        "- **Model Evaluation**: The model is run on `xb` and `yb` to get the predicted logits and the associated loss.\n",
        "- **Backpropagation**: The gradients of the loss concerning the model's parameters are computed.\n",
        "- **Parameter Update**: The optimizer (`AdamW`) updates both the embeddings in `token_embedding_table` and the weights and biases in `lm_head` using the computed gradients. This step is where the actual \"learning\" takes place, as it adjusts the parameters to reduce the prediction error.\n",
        "\n",
        "### 4. **Parameters Being Updated**:\n",
        "During the training process, both the embeddings in the `token_embedding_table` and the weights and biases in the `lm_head` linear layer are being updated. These represent the learnable parameters of the model.\n",
        "\n",
        "- **Embeddings**: These capture the learned representations of each token in the vocabulary.\n",
        "- **Linear Layer Weights and Biases**: These transform the embeddings to produce logits for predicting the next token.\n",
        "\n",
        "### 5. **Generative Ability**:\n",
        "Post-training, the `generate` method allows the model to produce sequences of tokens, building on the learned representations and transformations.\n",
        "\n",
        "### Summary:\n",
        "The 'learning' ability of this code resides in both the `token_embedding_table` and the `lm_head`. The model iteratively adjusts the embeddings and the linear transformation weights to better predict the subsequent token in a sequence. The learned parameters are the embeddings for each token and the weights and biases in the linear transformation layer."
      ],
      "metadata": {
        "id": "4Q68q7c1YbdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model's learning ability and the parameters being updated\n",
        "\n",
        "This code represents a slight modification from the previous model, introducing an intermediary layer that transforms the embeddings before predicting the next tokens.\n",
        "\n",
        "### 1. **Model Structure**:\n",
        "The modified model, `BigramLanguageModel`, now comprises two main components:\n",
        "\n",
        "- **Embedding Layer**: `self.token_embedding_table` converts token indices into dense vectors of size `n_embd`. This table holds the learned representations of each character/token in the vocabulary.\n",
        "- **Linear Transformation (Language Modeling Head)**: `self.lm_head` is a fully connected linear layer that transforms the embeddings from `n_embd` dimensions to the size of the vocabulary. This is used to produce logits that predict the next token.\n",
        "\n",
        "### 2. **Learning Process**:\n",
        "The model learns by adjusting both the embeddings and the weights of the linear transformation to minimize prediction error.\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - The model first fetches the embeddings for the input tokens from the `token_embedding_table`.\n",
        "  - These embeddings are then passed through the `lm_head` linear layer to produce logits for predicting the next tokens.\n",
        "- **Loss Calculation**: The loss is computed by comparing the predicted logits against the actual next tokens using the cross-entropy loss.\n",
        "\n",
        "### 3. **Training Loop**:\n",
        "This is where the iterative process of learning takes place:\n",
        "\n",
        "- **Batch Sampling**: In each iteration, a batch of sequences (`xb`) and their corresponding next tokens (`yb`) are sampled.\n",
        "- **Model Evaluation**: The model is run on `xb` and `yb` to get the predicted logits and the associated loss.\n",
        "- **Backpropagation**: The gradients of the loss concerning the model's parameters are computed.\n",
        "- **Parameter Update**: The optimizer (`AdamW`) updates both the embeddings in `token_embedding_table` and the weights and biases in `lm_head` using the computed gradients. This step is where the actual \"learning\" takes place, as it adjusts the parameters to reduce the prediction error.\n",
        "\n",
        "### 4. **Parameters Being Updated**:\n",
        "During the training process, both the embeddings in the `token_embedding_table` and the weights and biases in the `lm_head` linear layer are being updated. These represent the learnable parameters of the model.\n",
        "\n",
        "- **Embeddings**: These capture the learned representations of each token in the vocabulary.\n",
        "- **Linear Layer Weights and Biases**: These transform the embeddings to produce logits for predicting the next token.\n",
        "\n",
        "### 5. **Generative Ability**:\n",
        "Post-training, the `generate` method allows the model to produce sequences of tokens, building on the learned representations and transformations.\n",
        "\n",
        "### Summary:\n",
        "The 'learning' ability of this code resides in both the `token_embedding_table` and the `lm_head`. The model iteratively adjusts the embeddings and the linear transformation weights to better predict the subsequent token in a sequence. The learned parameters are the embeddings for each token and the weights and biases in the linear transformation layer."
      ],
      "metadata": {
        "id": "zbJfYixZQo1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings\n",
        "\n",
        "Let's delve into this updated model that introduces positional encoding.\n",
        "\n",
        "### **1. Introduction of Positional Encoding**:\n",
        "\n",
        "In language modeling tasks, the order in which tokens appear is critical. The previous model versions took into account only the identity of the tokens but not their positions. The updated model incorporates positional information, which will be crucial, especially when we move towards models like transformers that are inherently position-agnostic.\n",
        "\n",
        "### **2. Model Structure Changes**:\n",
        "\n",
        "- **Positional Embedding Layer**:\n",
        "  - The model now has an additional embedding table named `self.position_embedding_table`. This table will map each position in a sequence (from 0 to `block_size-1`) to a dense vector of size `n_embd`.\n",
        "  - This embedding is used to encode positional information, so the model knows, for example, that a particular token is the first, second, third in a sequence, and so on.\n",
        "  \n",
        "### **3. Forward Pass Changes**:\n",
        "\n",
        "- **Positional Embeddings Creation**:\n",
        "  - `pos_emb = self.position_embedding_table(torch.arange(T, device=device))`: This line creates the positional embeddings. For a sequence of length `T`, it will generate a matrix of shape `(T, n_embd)`, where each row is the embedding for that position.\n",
        "  \n",
        "- **Combining Token and Positional Embeddings**:\n",
        "  - `x = tok_emb + pos_emb`: This line adds the token embeddings (`tok_emb`) and the positional embeddings (`pos_emb`) together. This results in the `x` matrix which combines both token and positional information. Thanks to broadcasting in PyTorch, the positional embeddings are automatically expanded across the batch dimension to be added to the token embeddings.\n",
        "\n",
        "### **4. Impact on Learning**:\n",
        "\n",
        "While the lecturer mentioned that the positional information won't make a significant difference in this bigram model (since the model is simple), it sets the foundation for more complex models like transformers. In the transformer architecture, there's no inherent sense of position in the self-attention mechanism, so adding positional embeddings becomes crucial.\n",
        "\n",
        "### **5. Parameters Being Updated**:\n",
        "\n",
        "The training loop remains largely unchanged, but now the model has additional learnable parameters in the `position_embedding_table`. These embeddings will be adjusted during the training process to help the model better understand and utilize positional information.\n",
        "\n",
        "### **Summary**:\n",
        "\n",
        "This version of the model introduces the concept of positional encoding, ensuring each token is not only represented by its identity but also by its position in a sequence. While the bigram nature of the model means this addition has a limited immediate impact, it lays the groundwork for more sophisticated models where position plays a vital role, like transformers."
      ],
      "metadata": {
        "id": "Z1-ZGVFKXpPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `register_buffer`\n",
        "\n",
        "### 1. **What is a buffer in PyTorch?**\n",
        "\n",
        "In PyTorch, both parameters and buffers are a type of state internal to a `nn.Module`. While both hold tensors, there's a key difference:\n",
        "- **Parameters** are tensors that are learned during training (e.g., weights and biases in a neural network).\n",
        "- **Buffers** are tensors that aren't learned (their values don't get updated by backpropagation) but are still important for the forward computation. They need to be part of the model's state and should be saved along with the model.\n",
        "\n",
        "### 2. **Why use `register_buffer`?**\n",
        "\n",
        "The `register_buffer` method is used to add a tensor as a buffer in a `nn.Module`. This ensures that the tensor:\n",
        "- Is moved to the same device as the module (e.g., when calling `model.to(device)`).\n",
        "- Is saved when calling `torch.save` on the module, even though it's not a parameter that's being optimized.\n",
        "- Isn't considered during backpropagation (since it's not a parameter).\n",
        "\n",
        "### 3. **Understanding the specific line of code**\n",
        "\n",
        "```python\n",
        "self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "```\n",
        "\n",
        "Here:\n",
        "- `torch.tril` returns the lower triangular part of a matrix. For `torch.ones(block_size, block_size)`, it will generate a 2D tensor with ones below the diagonal and zeros above the diagonal.\n",
        "- The resulting tensor is registered as a buffer named 'tril' in the module.\n",
        "\n",
        "### 4. **Why is 'tril' needed as a buffer in this context?**\n",
        "\n",
        "Given the context you've provided earlier (the self-attention mechanism), this lower triangular matrix is likely used for masking. In autoregressive models, it ensures that a given position can only attend to earlier positions in the sequence (and itself) and not future positions. This mimics the sequential generation process where future tokens are unknown.\n",
        "\n",
        "By registering `tril` as a buffer, the model ensures that wherever the module goes (e.g., to a GPU), and whenever the model is saved, the `tril` tensor goes with it. It's essential for the forward pass but doesn't need to be learned or updated during training.\n",
        "\n",
        "In summary, `register_buffer` provides a mechanism to include essential tensors that should be a part of the model's state but shouldn't be updated during the optimization process."
      ],
      "metadata": {
        "id": "W4ZbhCFcsnT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Multi-headed Self-Attention\n"
      ],
      "metadata": {
        "id": "o6g8NTR3PcPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjqNeNWHPh1B",
        "outputId": "be7b537c-e7e5-4d63-9ed6-69a9a828bc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 17:55:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-08-28 17:55:26 (190 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 8 dimensional self-attention\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_heads(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b45fb22-e03a-412c-871b-af742cc25807",
        "id": "Io5dO5-LPiH_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2227, val loss 4.2226\n",
            "step 500: train loss 2.6592, val loss 2.6733\n",
            "step 1000: train loss 2.4980, val loss 2.5064\n",
            "step 1500: train loss 2.4291, val loss 2.4349\n",
            "step 2000: train loss 2.3716, val loss 2.3844\n",
            "step 2500: train loss 2.3417, val loss 2.3561\n",
            "step 3000: train loss 2.3149, val loss 2.3347\n",
            "step 3500: train loss 2.2918, val loss 2.3171\n",
            "step 4000: train loss 2.2895, val loss 2.2868\n",
            "step 4500: train loss 2.2748, val loss 2.2858\n",
            "\n",
            "Whent if bridcowd, whis byer that set bobe toe anthr-and mealleands:\n",
            "Warth foulque, vet?\n",
            "Wedtlay anes wice my.\n",
            "\n",
            "HDY'n om oroug\n",
            "Yowns, tof is heir thil; dill, aes isee sen cin lat Hetilrov the and Win now onderabousel.\n",
            "\n",
            "SFAUS:\n",
            "Shenser cechiry prugh aissthe, ye wing, u not\n",
            "To thig I whomeny wod mothake ont---An hat evibys wietit, stile weeshirecs poor gier; to\n",
            "To k danteref If sor; igre! mef thre inledo the af Pre?\n",
            "\n",
            "WISo myay I sup!\n",
            "Atied is:\n",
            "Sadsal the E'd st hoin couk aar tey Iry to I frouf voul\n",
            "CPU times: user 44.2 s, sys: 1.67 s, total: 45.9 s\n",
            "Wall time: 56.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturer's comment\n",
        "\n",
        "### 1. **Concept of Multi-headed Self-attention**:\n",
        "- At its core, multi-headed self-attention is about running the self-attention mechanism multiple times in parallel and then combining the results. This allows the model to capture various aspects or relationships in the data simultaneously.\n",
        "\n",
        "### 2. **Why Multiple Heads?**:\n",
        "- The data or sequence has a lot of nuanced information. For instance, in the context of text, words might relate to each other based on grammar, meaning, position, etc. By having multiple attention mechanisms running in parallel (each being a \"head\"), the model can capture these diverse relationships simultaneously.\n",
        "\n",
        "### 3. **Channel Analogy**:\n",
        "- The lecturer likens the heads to \"communication channels\". Just as in communications we might have multiple channels to relay different types of information, in multi-headed attention, each head can focus on different patterns or relationships in the data.\n",
        "\n",
        "### 4. **Dimensionality and Concatenation**:\n",
        "- Each head produces an output. When we have multiple heads, we concatenate their outputs along the channel (or feature) dimension. This brings all the diverse information together.\n",
        "\n",
        "### 5. **Relation to Convolutions**:\n",
        "- The idea of having multiple heads is compared to \"group convolutions\" in the world of Convolutional Neural Networks. In group convolutions, instead of having one large convolution operation, we perform several smaller, parallel convolutions. The parallelism in both techniques allows for capturing diverse features or patterns.\n",
        "\n",
        "### 6. **Benefits of Multi-headed Attention**:\n",
        "- The lecturer highlights the practical benefit of using multi-headed attention by mentioning the improved validation loss. This suggests that the model becomes more capable of understanding and representing the data when it can focus on it in multiple ways simultaneously.\n",
        "\n",
        "### 7. **Diverse Communication**:\n",
        "- The essence of multi-headed attention is allowing tokens (like words) to communicate in various ways. Tokens might want to talk about different aspects like consonants, vowels, or positions. Multiple heads facilitate this diverse communication, enhancing the model's ability to understand intricate relationships.\n",
        "\n",
        "In essence, multi-headed self-attention is a way to enrich the model's understanding of data by allowing it to view and process it from multiple perspectives concurrently."
      ],
      "metadata": {
        "id": "-Q4a6HkUTdT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do Transformers dynamically allocate attention to different aspects of the input?\n",
        "\n",
        "The ability of models like Transformers to dynamically allocate attention to different aspects of the input is intriguing. Let's break down how and why this happens:\n",
        "\n",
        "### 1. **Random Initialization**:\n",
        "- The model's parameters, including those in the attention heads, start with random values. This means that at the beginning, each head doesn't have a specific role. It's like each head is \"blind\" and doesn't know what to focus on.\n",
        "\n",
        "### 2. **Learning through Backpropagation**:\n",
        "- As the model is trained, it receives feedback in the form of gradients. These gradients indicate how each parameter should change to reduce the prediction error.\n",
        "- Different heads receive different gradients, pushing them to adjust in diverse ways.\n",
        "\n",
        "### 3. **Emergence of Specialization**:\n",
        "- Over time and through multiple iterations, certain heads start to recognize patterns or relationships that are beneficial for prediction. As they get \"rewarded\" (via gradient descent) for recognizing these patterns, they become more specialized in capturing them.\n",
        "- For instance, one head might start paying more attention to the syntactic structure, while another might focus on semantic relationships.\n",
        "\n",
        "### 4. **Benefit of Multiple Heads**:\n",
        "- Since each head is initialized differently and receives varied gradients, they diverge in their behavior. This divergence is crucial. If all heads were to focus on the same patterns, there would be redundancy, and the model wouldn't gain additional expressive power from having multiple heads.\n",
        "- The parallel nature of multi-head attention promotes this diversification. Each head operates independently during the forward pass, allowing them to \"explore\" different aspects of the data.\n",
        "\n",
        "### 5. **Regularization & Model Capacity**:\n",
        "- The model's capacity (how many parameters it has and its architecture) plays a role. A model with more heads has more capacity to learn varied relationships. However, if not regularized properly, it can also overfit.\n",
        "- Techniques like dropout applied to attention weights can ensure that no single head becomes overly dominant, promoting a balance among them.\n",
        "\n",
        "### 6. **Interplay with Other Layers**:\n",
        "- The Transformer architecture doesn't rely solely on attention. There are also feed-forward layers, normalization, and other components. These components interact with the outputs of the attention heads, further refining and processing the information.\n",
        "\n",
        "### 7. **Iterative Refinement**:\n",
        "- As data flows through a deep network like the Transformer, each layer has the opportunity to refine and reshape the representations. This iterative refinement allows higher layers to build on the specialized focuses of the lower layers, leading to more abstract and sophisticated understandings.\n",
        "\n",
        "In essence, the process is dynamic and emergent. No one tells the model to have one head focus on syntax and another on semantics. Instead, through the interplay of random initialization, backpropagation, and the model's architecture, these specializations naturally emerge as the most efficient way for the model to reduce its prediction error."
      ],
      "metadata": {
        "id": "HCkZiDmtUn4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Feed Forward Layer after Multi-headed Self-Attention"
      ],
      "metadata": {
        "id": "X9XI3jhxVZkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKYWeUU7Ve2z",
        "outputId": "5ea98cc7-9d3f-4a3e-bae2-2e59367ddeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 18:15:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-08-28 18:15:54 (37.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        # self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # self.net = nn.Sequential(\n",
        "        #     nn.Linear(n_embd, 4 * n_embd),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(4 * n_embd, n_embd),\n",
        "        #     nn.Dropout(dropout),\n",
        "        # )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 8 dimensional self-attention\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCzuAYeQVXb4",
        "outputId": "06b55c80-0d49-4f42-af8b-7bdb1e1c1dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1996, val loss 4.1995\n",
            "step 500: train loss 2.5993, val loss 2.6077\n",
            "step 1000: train loss 2.4629, val loss 2.4651\n",
            "step 1500: train loss 2.3974, val loss 2.3951\n",
            "step 2000: train loss 2.3297, val loss 2.3470\n",
            "step 2500: train loss 2.3018, val loss 2.3221\n",
            "step 3000: train loss 2.2828, val loss 2.2936\n",
            "step 3500: train loss 2.2495, val loss 2.2721\n",
            "step 4000: train loss 2.2435, val loss 2.2468\n",
            "step 4500: train loss 2.2286, val loss 2.2411\n",
            "\n",
            "And the Rorincowf,\n",
            "This by be mad thom obe to tarver-' my dall and bar hiphe us hat tot?\n",
            "Wedtlacoate aw crup and not, ut onour\n",
            "Yowns, tof it he cove lend lincath is ees, hain lat Het dulvets, and to poman is wables lill dite ullliser cecrivy prupt aiss hew youn's and knamopetell lownomthy wod moth keacal---A wher eiicks to thour rive cees ineds pood of he thu the hanterth fo so;; igis! my to thy ale ontat af Pried my of.\n",
            "WHINY ICHARD:\n",
            "Pois:\n",
            "Ardsal the Eget to uin cour ay andy Rry to chan the!\n",
            "An\n",
            "CPU times: user 42.4 s, sys: 1.5 s, total: 43.9 s\n",
            "Wall time: 53.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturer's comment\n",
        "\n",
        "The lecturer touches on several key points related to the Transformer architecture and the importance of the feedforward layers within it. Let's distill the main ideas:\n",
        "\n",
        "### 1. **Components of the Transformer**:\n",
        "- The Transformer model consists of various components like positional encodings, token encodings, and multi-headed attention. These components together allow the model to understand the sequence data with context.\n",
        "\n",
        "### 2. **Role of Feedforward Layers**:\n",
        "- After the multi-headed attention mechanism, where tokens \"communicate\" with each other, there is a need for further processing to \"think\" about or interpret the aggregated information.\n",
        "- The feedforward layers provide this post-attention processing. They act as a local, per-token transformation, where each token gets to process its information without further communication with other tokens.\n",
        "  \n",
        "### 3. **Simple MLP Structure**:\n",
        "- This \"thinking\" or interpretation mechanism isn't complex. It's a simple multi-layer perceptron (MLP) — basically a few linear layers interspersed with non-linear activations.\n",
        "  \n",
        "### 4. **Sequential Processing**:\n",
        "- The self-attention mechanism and the feedforward network operate sequentially. First, the model performs self-attention where tokens gather information from each other. Then, each token processes its information through the feedforward network.\n",
        "  \n",
        "### 5. **Independence of Feedforward Operations**:\n",
        "- An essential characteristic of the feedforward layers is that their operations are independent for each token. This contrasts with the attention mechanism where tokens interact with each other.\n",
        "  \n",
        "### 6. **Improvement in Model Performance**:\n",
        "- Introducing the feedforward layers improves the model's performance, as indicated by the reduction in validation loss. This suggests that giving the model an opportunity to \"reflect\" on the information it aggregates is beneficial.\n",
        "\n",
        "### 7. **Analogy of Communication and Reflection**:\n",
        "- The self-attention mechanism can be thought of as a \"communication\" phase where tokens exchange information. The subsequent feedforward layers act as a \"reflection\" phase where each token individually processes the information it has gathered.\n",
        "\n",
        "In essence, the lecturer emphasizes the dual nature of the Transformer's processing: first, a collaborative phase (attention) where tokens share information, followed by an individual phase (feedforward) where each token processes its data independently. Both phases together enable the model to understand and generate meaningful sequences."
      ],
      "metadata": {
        "id": "Yqr9_nysZBV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The role of the feedforward network in the context of the Transformer architecture\n",
        "\n",
        "Let's dissect the role of the feedforward network in the context of the Transformer architecture and understand its function for each token.\n",
        "\n",
        "### **What is the Feedforward Network in the Transformer?**\n",
        "\n",
        "In the given `BigramLanguageModel`, after the multi-headed self-attention mechanism, there's a feedforward neural network, represented by the `FeedFoward` class. This feedforward network consists of a linear layer followed by a non-linear activation (ReLU in this case).\n",
        "\n",
        "### **Role of the Feedforward Network**:\n",
        "\n",
        "1. **Token-Level Computation**:\n",
        "    - While the self-attention mechanism allows tokens to interact with each other and gather context, the feedforward network operates on each token independently. It's a per-token transformation.\n",
        "    \n",
        "2. **Enhancing Representations**:\n",
        "    - The purpose of the feedforward network is to further process and refine the information that each token has gathered from the self-attention mechanism. It's akin to each token having its mini-neural network that it passes through to generate a more sophisticated representation of itself.\n",
        "\n",
        "3. **Consistency in Dimension**:\n",
        "    - The input and output dimensions of the feedforward network are the same (in this case, `n_embd`). This ensures that the token representations remain consistent in size, making it possible to stack multiple Transformer blocks.\n",
        "\n",
        "### **How Does it Work for Each Token?**\n",
        "\n",
        "Given the code:\n",
        "\n",
        "```python\n",
        "self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "...\n",
        "\n",
        "x = self.sa_heads(x)\n",
        "x = self.ffwd(x)\n",
        "```\n",
        "\n",
        "1. After the multi-headed self-attention (`self.sa_heads`), each token in the sequence has a new representation. This representation is based on its interactions with other tokens in the sequence.\n",
        "    \n",
        "2. The matrix `x` has the shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the number of channels (or embedding size). Each row in this matrix corresponds to a token in the sequence.\n",
        "\n",
        "3. The feedforward network (`self.ffwd`) processes this matrix. It operates on the matrix one token at a time (across the batch). Each token's representation (a vector of size `C`) is passed through a linear layer followed by a ReLU activation.\n",
        "\n",
        "4. The output is another matrix of the same shape `(B, T, C)`. But now, each token's representation has been independently transformed by the feedforward network.\n",
        "\n",
        "### **In Simple Terms**:\n",
        "\n",
        "Think of the feedforward network as each token's personal mini-brain. After gathering information from other tokens via the attention mechanism, each token uses its mini-brain to process and refine this information further. This allows the token to generate a richer, more nuanced representation of itself, which can then be used for downstream tasks like language modeling, translation, etc.\n",
        "\n",
        "### **Why is this Important?**\n",
        "\n",
        "The combination of the self-attention mechanism (which provides contextual information) and the feedforward network (which refines this information) ensures that each token's representation is both contextually rich and sophisticated. This duality is a key reason behind the Transformer's impressive performance on various NLP tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Short Note on Token Processing in Feedforward Network**:\n",
        "\n",
        "In the Transformer's feedforward network, each token's representation is processed independently due to the design of the linear layer. Specifically, the linear layer in the feedforward network has both its input and output dimensions set to `n_embd`, which corresponds to the embedding size or the channel dimension `C` in the `(B, T, C)` shaped tensor. This design choice ensures that each token, represented by a vector of size `n_embd`, is individually transformed without inter-token interactions at this stage. This isolated processing for each token is contrasted with the self-attention mechanism, where tokens interact with each other to gather contextual information.\n",
        "\n",
        "---\n",
        "\n",
        "**Explanation on Consistent Input and Output Dimensions**:\n",
        "\n",
        "The feedforward network within the Transformer architecture maintains the same input and output dimensions, which in this context is `n_embd`. This design ensures a few critical aspects:\n",
        "\n",
        "1. **Consistency**: By keeping the dimensions consistent, each token's representation remains the same size throughout the processing, ensuring that the output of one Transformer block can be directly fed into another without any need for resizing or reshaping.\n",
        "\n",
        "2. **Stackability**: One of the powerful features of Transformers is the ability to stack multiple blocks on top of each other to create deeper models. Having consistent input and output dimensions in the feedforward network is crucial for this. It ensures that the output from one block can be seamlessly used as input to the next, without any dimensional mismatches.\n",
        "\n",
        "3. **Richer Representations**: While the size remains consistent, the actual content of the token representations undergoes transformation. This means that even though the token's vector size remains `n_embd`, its content or information can evolve and become richer as it passes through successive Transformer blocks.\n",
        "\n",
        "In essence, maintaining the same input and output dimensions in the feedforward network simplifies the architecture and allows for modularity, where multiple Transformer blocks can be stacked easily to capture deeper and more complex patterns in the data."
      ],
      "metadata": {
        "id": "PnZFK6iFZpNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with residual connections"
      ],
      "metadata": {
        "id": "Nxv1sL_rXufS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7CEk0bXXzNV",
        "outputId": "b60bf0c8-68f0-4568-f166-95f0f14bfb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 19:03:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-08-28 19:03:51 (19.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        #     nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        # self.ln1 = nn.LayerNorm(n_embd)\n",
        "        # self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x + self.sa(self.ln1(x))\n",
        "        # x = x + self.ffwd(self.ln2(x))\n",
        "        x = x + self.sa(x)\n",
        "        x = x + self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqdGluphXvz8",
        "outputId": "9bc0b693-f4c3-43b3-db28-d4fcadd09b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6255, val loss 4.6233\n",
            "step 500: train loss 2.3884, val loss 2.3849\n",
            "step 1000: train loss 2.2713, val loss 2.2696\n",
            "step 1500: train loss 2.1885, val loss 2.2101\n",
            "step 2000: train loss 2.1463, val loss 2.1821\n",
            "step 2500: train loss 2.1078, val loss 2.1549\n",
            "step 3000: train loss 2.0697, val loss 2.1435\n",
            "step 3500: train loss 2.0616, val loss 2.1207\n",
            "step 4000: train loss 2.0257, val loss 2.1109\n",
            "step 4500: train loss 2.0061, val loss 2.1032\n",
            "\n",
            "And they bridce?\n",
            "\n",
            "SORORD Edly madised bube to take Our my calalanss:\n",
            "Walt he us him to bardetle\n",
            "Hay, away, my feanstar merent:\n",
            "You some, tis heart milled,\n",
            "Whine miseet?\n",
            "Bucie;\n",
            "Stist in overs, and the now on you meself in you littishe courmby prave as splaw you lord.\n",
            "In am patelives home.\n",
            "Who my that\n",
            "To Winso what eis as the mosterion cence; ear poon of his but that non,\n",
            "Thef son; igrean shat thy flengath, af Prive my of that but hartioblist\n",
            "ardaple,\n",
            "And hellove hence asard:\n",
            "your his chan the wil\n",
            "CPU times: user 1min 30s, sys: 370 ms, total: 1min 31s\n",
            "Wall time: 1min 31s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturer's comment\n",
        "\n",
        "1. **Transformer Block Structure**: The lecturer first lays out the structure of a Transformer block, highlighting that it is composed of communication (via multi-headed self-attention) followed by computation (via a feed-forward network). Each token is processed independently during the feed-forward computation.\n",
        "\n",
        "2. **Residual (Skip) Connections**: The concept of residual or skip connections is introduced. These are direct pathways that allow the input of a layer to skip one or more layers and be added directly to the output. This addition creates a \"shortcut\" for the gradients during backpropagation, making it easier to train deep networks. The intuition is that this provides a gradient \"superhighway\" that eases the training of deep models by preventing the vanishing gradient problem.\n",
        "\n",
        "3. **Visualizing Residual Connections**: The lecturer prefers a visualization where data flows from top to bottom, and the residual connections act as forks that branch off from the main path, perform some computation, and then merge back into the main path. The supervision or gradients from the loss can then flow directly from the output to the input, unimpeded, through these residual pathways.\n",
        "\n",
        "4. **Residual Block Initialization**: An important point is raised about the initialization of these residual blocks. Initially, these blocks contribute very little to the residual pathway, almost as if they're not there. However, as training progresses, they start to become more active and contribute more significantly to the model's output.\n",
        "\n",
        "5. **Projection in Residual Connections**: After the multi-headed self-attention operation, the result is projected back to its original dimensionality before being added to the input. This ensures that the dimensions align properly for the addition operation in the residual connection.\n",
        "\n",
        "6. **Dimensionality in Feed Forward Network**: The lecturer points out a detail from the original Transformer paper where the inner layer of the feed-forward network has a dimensionality that's four times larger than the input/output dimensionality. This expansion and subsequent compression of dimensions can allow the network to learn richer representations.\n",
        "\n",
        "7. **Training Observations**: With the incorporation of these enhancements, the model's validation loss improves. However, the lecturer notes that as the network becomes deeper and more complex, there's a potential for overfitting, where the training loss becomes significantly lower than the validation loss. Despite this, the generated text starts to resemble more coherent structures.\n",
        "\n",
        "In summary, the lecturer emphasizes the importance of residual connections in enabling the training of deeper neural networks by providing a direct path for gradients. The structure of the Transformer block, comprising communication and computation stages, and the specific design choices in dimensionality, play crucial roles in the model's performance."
      ],
      "metadata": {
        "id": "Fi2F9A4YkjkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Connections\n",
        "\n",
        "\n",
        "### **1. The Challenge with Deep Networks:**\n",
        "Deep neural networks, with many layers, are notoriously difficult to train. One primary reason is the vanishing gradient problem. As we backpropagate through the layers, gradients can become increasingly small, such that the weights of the initial layers hardly get updated. This leads to poor convergence and longer training times.\n",
        "\n",
        "### **2. Introducing Residual Connections:**\n",
        "Residual connections, also known as skip connections or shortcuts, offer a solution to this problem. They allow the output from one layer to \"skip\" one or more layers and be added directly to the output of a subsequent layer.\n",
        "\n",
        "### **3. Mathematical Representation:**\n",
        "Consider a neural network without residual connections. The output \\( H(x) \\) from some layers can be represented as:\n",
        "\\[ H(x) = F(x) \\]\n",
        "where \\( F(x) \\) is the transformation learned by the layers, and \\( x \\) is the input to those layers.\n",
        "\n",
        "In a network with residual connections, the transformation is modified to:\n",
        "\\[ H(x) = F(x) + x \\]\n",
        "Here, \\( x \\) is added back to the output, creating a shortcut. The function \\( F(x) \\) learns the \"residual\" or the difference between the input and output, rather than the direct mapping.\n",
        "\n",
        "### **4. Benefits:**\n",
        "- **Eases Training:** The direct paths ensure that gradients can flow directly through the shortcuts during backpropagation, alleviating the vanishing gradient problem.\n",
        "  \n",
        "- **Flexibility:** If certain layers in the network aren't beneficial, the network can set their weights such that the layers almost mimic an identity function, ensuring that the output is roughly equal to the input. Essentially, the network can choose to use the shortcuts if it determines that they're more beneficial than some of the layers.\n",
        "\n",
        "### **5. Visualization:**\n",
        "A visual representation can help understand residual connections better. Imagine a highway where the main road is the direct path from input to output. The layers in the neural network are like towns along this highway. With residual connections, there are overpasses or flyovers that allow you to skip these towns, ensuring a faster and more direct route.\n",
        "\n",
        "### **6. Application in Modern Architectures:**\n",
        "Residual connections were first introduced in the ResNet architecture, which won the ImageNet competition in 2015. Their introduction allowed the training of much deeper networks, with ResNet having variants with over 100 layers. Since then, they've become a staple in many deep learning architectures, including Transformers.\n",
        "\n",
        "### **7. Conclusion:**\n",
        "Residual connections are a simple yet powerful tool in the deep learning toolbox. By providing direct paths for data and gradients, they help combat the challenges posed by training very deep networks, ensuring faster convergence and better performance."
      ],
      "metadata": {
        "id": "Pd1hVCrckXIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Layer Normalization"
      ],
      "metadata": {
        "id": "mWMoOFaEkuQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f98910e-78ba-428c-88dd-547ed0eaed98",
        "id": "iONoVpr7kygq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 19:30:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-08-28 19:30:49 (31.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        # wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # out = self.dropout(self.proj(out))\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        #     nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkrYPuXTkvxd",
        "outputId": "55da55ae-d564-41d5-9b34-d79a0d319225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3103, val loss 4.3097\n",
            "step 500: train loss 2.3998, val loss 2.4007\n",
            "step 1000: train loss 2.2641, val loss 2.2661\n",
            "step 1500: train loss 2.1670, val loss 2.1905\n",
            "step 2000: train loss 2.1317, val loss 2.1678\n",
            "step 2500: train loss 2.0811, val loss 2.1301\n",
            "step 3000: train loss 2.0508, val loss 2.1236\n",
            "step 3500: train loss 2.0426, val loss 2.1049\n",
            "step 4000: train loss 2.0136, val loss 2.0946\n",
            "step 4500: train loss 1.9903, val loss 2.0969\n",
            "\n",
            "\n",
            "YORWARISANNES:\n",
            "A, Tursen, be madised bube don.\n",
            "Sagrad my dalatands:\n",
            "Watther us he hert?\n",
            "Fedelad ane away, my fears'd of of my\n",
            "Yourseld foit heart milend liblees if ensen contlatismand ove the the me now on you musel ling that.\n",
            "His my dervey: the baiss hew you lord.\n",
            "In Bon, this down my liked mothake on in on her eig as to they srive cenchiends poor goed; the the dantert,\n",
            "If so;\n",
            "Angrean whith dy ale of whith Prive my of.\n",
            "\n",
            "HKING EDWARD PAY I\n",
            "Sadave the Eneds, hoich must are ny ity to chan the wil\n",
            "CPU times: user 1min 42s, sys: 1.9 s, total: 1min 44s\n",
            "Wall time: 1min 57s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturer's comment\n",
        "\n",
        "The lecturer is conveying several crucial concepts related to layer normalization (LayerNorm) and its role in deep learning, especially within the context of Transformers. Here are the distilled key ideas:\n",
        "\n",
        "### **1. The Purpose of Normalization:**\n",
        "Neural networks benefit from normalized input features because it ensures consistency in the range and scale, aiding in better learning and convergence.\n",
        "\n",
        "### **2. BatchNorm vs. LayerNorm:**\n",
        "- **Batch Normalization (BatchNorm):** It normalizes across the batch dimension, ensuring that each feature has a zero mean and unit variance across different data points in a batch. However, it's sensitive to batch size and can behave differently during training and inference.\n",
        "  \n",
        "- **Layer Normalization (LayerNorm):** Unlike BatchNorm, LayerNorm normalizes across the feature dimension for each data point, making it independent of the batch size. This ensures consistent behavior during both training and inference.\n",
        "\n",
        "### **3. Mathematical Intuition:**\n",
        "For LayerNorm, the mean and variance are computed across the features (not the batch). Each data point is then normalized based on its own mean and variance. After normalization, scaling (using \\( \\gamma \\)) and shifting (using \\( \\beta \\)) operations are applied. Both \\( \\gamma \\) and \\( \\beta \\) are learnable parameters.\n",
        "\n",
        "### **4. Implementation Details:**\n",
        "The lecturer emphasizes that the switch from BatchNorm to LayerNorm is straightforward—instead of normalizing across columns (batch dimension), you normalize across rows (feature dimension). Also, LayerNorm doesn't need to maintain running statistics, making its implementation simpler than BatchNorm.\n",
        "\n",
        "### **5. Position of LayerNorm in Transformers:**\n",
        "The original Transformer paper had the normalization after the addition operation in the residual connection. However, a more recent approach, referred to as \"pre-norm\", applies normalization before the multi-head attention and feed-forward operations. The lecturer adopts this pre-norm formulation for the discussed implementation.\n",
        "\n",
        "### **6. Importance of LayerNorm in Deep Networks:**\n",
        "Layer normalization can be particularly beneficial in deeper networks. It helps in stabilizing the activations and gradients, leading to smoother training and better convergence. In the context of the lecture, the inclusion of LayerNorm led to a slight improvement in the model's performance.\n",
        "\n",
        "### **7. Final LayerNorm:**\n",
        "Typically, at the end of the Transformer block, before the final linear layer that decodes into the vocabulary, another LayerNorm is applied. This ensures that the outputs fed to the final decoding layer are also normalized.\n",
        "\n",
        "### **Conclusion:**\n",
        "Layer normalization is a vital technique, especially for models like Transformers. It ensures consistent feature scales across different layers of the network, aiding in stable training and better convergence. The independence from batch size and consistent behavior during training and inference make it a preferred choice over BatchNorm in many modern deep learning architectures."
      ],
      "metadata": {
        "id": "eIs-hOdLn_S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with dropout"
      ],
      "metadata": {
        "id": "KSDCRK-BoM9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEfxbtowoRas",
        "outputId": "3a0ec7ce-52f0-4c19-db22-9b983f325124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 19:54:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-08-28 19:54:54 (209 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiVo_b9qoOwj",
        "outputId": "f08b8a9f-a95e-4420-811b-75fffc9c1cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6612, val loss 4.6497\n",
            "step 100: train loss 2.6220, val loss 2.6385\n",
            "step 200: train loss 2.5073, val loss 2.5026\n",
            "step 300: train loss 2.4066, val loss 2.4263\n",
            "step 400: train loss 2.3367, val loss 2.3546\n",
            "step 500: train loss 2.2879, val loss 2.3083\n",
            "step 600: train loss 2.2341, val loss 2.2481\n",
            "step 700: train loss 2.1895, val loss 2.2107\n",
            "step 800: train loss 2.1485, val loss 2.1737\n",
            "step 900: train loss 2.1098, val loss 2.1483\n",
            "step 1000: train loss 2.0862, val loss 2.1257\n",
            "step 1100: train loss 2.0654, val loss 2.1208\n",
            "step 1200: train loss 2.0358, val loss 2.0813\n",
            "step 1300: train loss 2.0235, val loss 2.0690\n",
            "step 1400: train loss 1.9904, val loss 2.0444\n",
            "step 1500: train loss 1.9690, val loss 2.0388\n",
            "step 1600: train loss 1.9571, val loss 2.0419\n",
            "step 1700: train loss 1.9467, val loss 2.0161\n",
            "step 1800: train loss 1.9093, val loss 2.0044\n",
            "step 1900: train loss 1.9032, val loss 1.9833\n",
            "step 2000: train loss 1.8835, val loss 1.9947\n",
            "step 2100: train loss 1.8709, val loss 1.9825\n",
            "step 2200: train loss 1.8594, val loss 1.9616\n",
            "step 2300: train loss 1.8512, val loss 1.9550\n",
            "step 2400: train loss 1.8456, val loss 1.9464\n",
            "step 2500: train loss 1.8177, val loss 1.9407\n",
            "step 2600: train loss 1.8240, val loss 1.9290\n",
            "step 2700: train loss 1.8190, val loss 1.9360\n",
            "step 2800: train loss 1.8141, val loss 1.9314\n",
            "step 2900: train loss 1.8135, val loss 1.9384\n",
            "step 3000: train loss 1.7936, val loss 1.9167\n",
            "step 3100: train loss 1.7746, val loss 1.9105\n",
            "step 3200: train loss 1.7540, val loss 1.9022\n",
            "step 3300: train loss 1.7620, val loss 1.9071\n",
            "step 3400: train loss 1.7581, val loss 1.8944\n",
            "step 3500: train loss 1.7422, val loss 1.8915\n",
            "step 3600: train loss 1.7350, val loss 1.8992\n",
            "step 3700: train loss 1.7359, val loss 1.8847\n",
            "step 3800: train loss 1.7266, val loss 1.8907\n",
            "step 3900: train loss 1.7248, val loss 1.8720\n",
            "step 4000: train loss 1.7156, val loss 1.8641\n",
            "step 4100: train loss 1.7202, val loss 1.8785\n",
            "step 4200: train loss 1.7144, val loss 1.8712\n",
            "step 4300: train loss 1.7120, val loss 1.8571\n",
            "step 4400: train loss 1.7103, val loss 1.8695\n",
            "step 4500: train loss 1.7057, val loss 1.8621\n",
            "step 4600: train loss 1.6925, val loss 1.8372\n",
            "step 4700: train loss 1.6877, val loss 1.8415\n",
            "step 4800: train loss 1.6724, val loss 1.8441\n",
            "step 4900: train loss 1.6768, val loss 1.8412\n",
            "\n",
            "And they brince?\n",
            "\n",
            "STANLET:\n",
            "He made, to Backingham graves me?\n",
            "\n",
            "Tauss:\n",
            "Warthy, us he here?\n",
            "\n",
            "MEXENSBY:\n",
            "Why, Have thank's wizn must outs,\n",
            "Hoffice Bintomile dilike egrieve:\n",
            "Onecin lattemand ov the doest no.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "That cout humm on him speak in sollow you love.\n",
            "In Badieve a love my like to that\n",
            "To Willond, I pringle, and dour farcees,\n",
            "Tell pood my hourse of and thrugh for arm\n",
            "Ty tomef'd, I may poatince, Prive my of.\n",
            "\n",
            "HOMNSABRLAND:\n",
            "Night and by his great hoit courtear tey Rry tome\n",
            "And for th\n",
            "CPU times: user 4min 11s, sys: 2.15 s, total: 4min 13s\n",
            "Wall time: 4min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecturer's comment\n",
        "\n",
        "The lecturer is discussing the process and importance of scaling up a neural network model and the necessary modifications to prevent overfitting. Let's distill the key intuitions:\n",
        "\n",
        "### **1. Scaling Up the Model:**\n",
        "- **Purpose:** A larger model has the potential to capture more intricate patterns in the data, which can lead to better performance.\n",
        "  \n",
        "- **Adjustments Made:**\n",
        "  - **Number of Layers (`n_layer`):** More layers were introduced to increase the model's depth.\n",
        "  - **Number of Heads in Attention Mechanism (`n_head`):** By increasing the number of heads, the model can capture multiple types of relationships simultaneously.\n",
        "  - **Block Size:** The context for predictions was expanded from 8 characters to 256 characters, providing the model with a broader context to base its predictions on.\n",
        "  - **Embedding Dimension (`n_embd`):** The size of the embeddings was increased, allowing for richer representations of data.\n",
        "\n",
        "### **2. Regularization via Dropout:**\n",
        "- **Purpose of Dropout:** To prevent overfitting, especially when scaling up the model. Overfitting occurs when a model performs well on the training data but fails to generalize to unseen data.\n",
        "\n",
        "- **How Dropout Works:**\n",
        "  - **Random Deactivation:** During training, dropout randomly deactivates a subset of neurons in each forward-backward pass.\n",
        "  - **Effect:** This dynamic neuron deactivation effectively trains multiple sub-networks within the main network. At test time, all neurons are active, and the model behaves as an ensemble of all these sub-networks.\n",
        "  - **Intuition:** Dropout forces the network to become more robust, as it cannot rely on any single neuron's presence during training. It's akin to training an ensemble of networks, which can lead to better generalization.\n",
        "\n",
        "- **Application in the Model:** Dropout was added before residual connections and in other strategic locations.\n",
        "\n",
        "### **3. Results from Scaling Up:**\n",
        "- **Performance Improvement:** By scaling up the model and introducing dropout for regularization, the validation loss improved significantly from 2.07 to 1.48.\n",
        "- **Learning Rate Adjustment:** As the model grew in size, the learning rate was reduced slightly to ensure stable and effective training.\n",
        "\n",
        "### **Conclusion:**\n",
        "The lecturer emphasizes the value of scaling up neural networks for performance gains. However, as models become larger, there's an increased risk of overfitting. Techniques like dropout are essential to ensure that the model doesn't just memorize the training data but generalizes well to new, unseen data. The results after scaling demonstrate the efficacy of these strategies."
      ],
      "metadata": {
        "id": "a-W-l25CvTG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Explanation\n",
        "\n",
        "The code is an implementation of a simplified version of the Transformer model, commonly used in Natural Language Processing (NLP). The code aims to train this model on a given text to predict the next character based on a sequence of previous characters.\n",
        "\n",
        "Let's break down the learning ability of this code step-by-step:\n",
        "\n",
        "### **1. Data Preparation:**\n",
        "- A text file `input.txt` is read, and a vocabulary is built from the unique characters in the text.\n",
        "- The text is encoded into integers using a mapping from characters to integers (`stoi`) and vice-versa (`itos`).\n",
        "- The text data is split into training (90%) and validation (10%) datasets.\n",
        "\n",
        "### **2. Model Architecture:**\n",
        "- **Token & Position Embeddings:** Every input token (character) is transformed into a dense vector using token embeddings, and a position embedding is added to account for the order of tokens.\n",
        "  \n",
        "- **Transformer Block:** Consists of:\n",
        "  - **MultiHeadAttention:** Uses multiple heads to capture different types of relationships in the data.\n",
        "  - **FeedForward:** A simple feed-forward neural network that operates on each position separately.\n",
        "  - **LayerNorm:** Layer normalization applied before each sub-block (multi-head attention & feed-forward network) to stabilize activations.\n",
        "  \n",
        "- The model uses several of these blocks stacked on top of each other.\n",
        "\n",
        "- **Output Layer:** A linear layer that maps the output of the last Transformer block to the vocabulary size, effectively predicting the likelihood of each character being the next character in the sequence.\n",
        "\n",
        "### **3. Learning Process:**\n",
        "- **Objective:** The model is trained to minimize the cross-entropy loss between its predictions and the true next characters in the sequences.\n",
        "\n",
        "- **Backpropagation & Weight Update:** After computing the loss, gradients are computed using backpropagation, and model parameters (weights & biases) are updated using the AdamW optimizer.\n",
        "\n",
        "### **4. Parameters Being Updated:**\n",
        "All components of the model have trainable parameters that get updated during the training process:\n",
        "- **Embeddings:** `self.token_embedding_table` and `self.position_embedding_table`.\n",
        "- **Transformer Block:**\n",
        "  - Multi-head self-attention parameters in `self.sa` (key, query, value transformations).\n",
        "  - Feed-forward network parameters in `self.ffwd`.\n",
        "  - Layer normalization parameters in `self.ln1` and `self.ln2`.\n",
        "- **Output Layer:** Parameters in `self.lm_head`.\n",
        "\n",
        "The AdamW optimizer is responsible for updating these parameters based on the computed gradients.\n",
        "\n",
        "### **5. Model Evaluation:**\n",
        "At regular intervals (`eval_interval`), the model's performance is evaluated on both the training and validation datasets to monitor its learning progress.\n",
        "\n",
        "### **6. Text Generation:**\n",
        "After training, the model can generate new text based on a provided context using the `generate` method. This method repeatedly predicts the next character and appends it to the current sequence.\n",
        "\n",
        "### **Conclusion:**\n",
        "The learning ability of this code is centered around training the Transformer model to predict the next character in a sequence. All the parameters (weights and biases) in the embeddings, Transformer blocks, and output layer are being updated during the training process to minimize the prediction error."
      ],
      "metadata": {
        "id": "QaTzw-zZvLkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with scaling up"
      ],
      "metadata": {
        "id": "RCgP-ZInvrx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOsY2rnMv0CR",
        "outputId": "a48fc83c-cec1-467b-f36d-5e7a109e4c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 20:05:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-28 20:05:43 (58.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwUC-iK5vw0F",
        "outputId": "3f5912ec-c76b-4272-8d77-e57c413bc946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4753, val loss 4.4709\n",
            "step 500: train loss 2.0850, val loss 2.1490\n",
            "step 1000: train loss 1.6630, val loss 1.8240\n",
            "step 1500: train loss 1.4919, val loss 1.6843\n",
            "step 2000: train loss 1.3854, val loss 1.6082\n",
            "step 2500: train loss 1.3170, val loss 1.5631\n",
            "step 3000: train loss 1.2615, val loss 1.5305\n",
            "step 3500: train loss 1.2168, val loss 1.5075\n",
            "step 4000: train loss 1.1767, val loss 1.4937\n",
            "step 4500: train loss 1.1376, val loss 1.4822\n",
            "\n",
            "But with prisophecal gentleman makes of merely.\n",
            "\n",
            "BENVOLIO:\n",
            "Treason, and I boys:\n",
            "I more have borne more: if that were no more\n",
            "honour's mock'd to soundly. I love him all at home,\n",
            "Anon there I less: I hearinouse, made him right;\n",
            "My mistress hold on his own whenches\n",
            "Though that odds it.\n",
            "\n",
            "MARCIUS:\n",
            "Better, to it will turn do;--pointing allow,\n",
            "His gentleman's waxed unto answer that looks.\n",
            "This back's witchcrafting,--yhis then she's beast to married?\n",
            "I'ld had certainly you!\n",
            "\n",
            "RATCLIFF:\n",
            "My lord, I would a\n",
            "CPU times: user 45min 41s, sys: 4min 23s, total: 50min 5s\n",
            "Wall time: 51min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save text output to file"
      ],
      "metadata": {
        "id": "WdXPigRTBKq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the text using the model\n",
        "generated_text = decode(m.generate(context, max_new_tokens=20000)[0].tolist())\n",
        "\n",
        "# Save the generated text to a file\n",
        "with open('output.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Text saved to output.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IINEYKJx9wyG",
        "outputId": "ac264a6f-7acb-4aad-c58f-512a1cf80916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "2M-ApIRM3C_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model\n",
        "\n",
        "If you have trained a model on Google Colab using a GPU and now you want to switch to CPU (either for inference or for further work on a local machine or another environment), you should follow these steps:\n",
        "\n",
        "1. **Move the Model to CPU**:\n",
        "   \n",
        "   Before saving, it's a good practice to move the model to CPU.\n",
        "   \n",
        "   ```python\n",
        "   model.to('cpu')\n",
        "   ```\n",
        "\n",
        "2. **Save the Model**:\n",
        "\n",
        "   There are primarily two ways to save a model in PyTorch:\n",
        "\n",
        "   - **Save the entire model**:\n",
        "     \n",
        "     This will save the entire module using Python's pickle utility. It will include the model architecture and the weights.\n",
        "     \n",
        "     ```python\n",
        "     torch.save(model, 'model_path.pt')\n",
        "     ```\n",
        "\n",
        "   - **Save only the model parameters (recommended)**:\n",
        "     \n",
        "     This method is more portable as it avoids potential issues with specific model architectures. However, when you load the model back, you'll need to have the model architecture defined in the code.\n",
        "     \n",
        "     ```python\n",
        "     torch.save(model.state_dict(), 'model_weights_path.pt')\n",
        "     ```\n",
        "\n",
        "3. **Download the Saved Model**:\n",
        "\n",
        "   Once you've saved the model to the Colab filesystem, you'll probably want to download it to your local machine:\n",
        "\n",
        "   ```python\n",
        "   from google.colab import files\n",
        "   files.download('model_weights_path.pt')\n",
        "   ```\n",
        "\n",
        "4. **Loading Back the Model**:\n",
        "\n",
        "   Once you've saved and possibly downloaded your model, you can load it back as needed:\n",
        "\n",
        "   - **If you saved the entire model**:\n",
        "     \n",
        "     ```python\n",
        "     model = torch.load('model_path.pt')\n",
        "     ```\n",
        "\n",
        "   - **If you saved only the model parameters**:\n",
        "     \n",
        "     First, instantiate the model architecture, then load the weights.\n",
        "     \n",
        "     ```python\n",
        "     model = YourModelArchitecture()  # replace this with your model class\n",
        "     model.load_state_dict(torch.load('model_weights_path.pt'))\n",
        "     ```\n",
        "\n",
        "Always remember to switch the model to evaluation mode using `model.eval()` before performing inference, especially if you have layers like dropout or batch normalization in your architecture.\n"
      ],
      "metadata": {
        "id": "vmnIo2ESBSp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgtIqC24Bdp1",
        "outputId": "c4e945e4-560b-467e-fd20-527a49d63fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model_path.pt')\n"
      ],
      "metadata": {
        "id": "2sdbFbphBibT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights_path.pt')\n"
      ],
      "metadata": {
        "id": "aP8EGoGsBmNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note on changing runtimes\n",
        "\n",
        "In Google Colab, when you switch runtime types (e.g., from GPU to CPU or vice versa) or if your runtime is reset for any reason (like inactivity or reaching the maximum allocated time), the virtual machine you were using, including its file system, is recycled. As a result, all the data you had stored in it will be lost. This includes any models you saved, datasets you uploaded, etc.\n",
        "\n",
        "If you saved a model and then switched the runtime type without first downloading the model or saving it to Google Drive, then unfortunately that model would be lost.\n",
        "\n",
        "To prevent such losses in the future:\n",
        "\n",
        "1. **Download Files to Your Local System**: After saving a model or generating any file, you can download it directly to your local machine.\n",
        "   \n",
        "   ```python\n",
        "   from google.colab import files\n",
        "   files.download('path_to_your_file')\n",
        "   ```\n",
        "\n",
        "2. **Save Files to Google Drive**: You can also mount your Google Drive in the Colab notebook and save files directly to it. This way, even if the runtime is recycled, the files in your Google Drive will remain intact.\n",
        "\n",
        "   Here's how you can mount Google Drive:\n",
        "   ```python\n",
        "   from google.colab import drive\n",
        "   drive.mount('/content/gdrive')\n",
        "   ```\n",
        "\n",
        "   Once mounted, you can save files to your Google Drive:\n",
        "   ```python\n",
        "   model.save('/content/gdrive/My Drive/path_in_drive/my_model.h5')\n",
        "   ```\n",
        "\n",
        "Always ensure that your data is safely stored in a persistent location before making changes to your runtime or ending your session."
      ],
      "metadata": {
        "id": "55DP1RQQDlIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding Notes"
      ],
      "metadata": {
        "id": "k22rGpim1nfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder-only Transformers\n",
        "\n",
        "The lecturer is emphasizing the distinction between the encoder-decoder architecture of the original Transformer model and the decoder-only version implemented in models like GPT. Let's break down the key intuitions:\n",
        "\n",
        "### **1. Decoder-Only Transformer:**\n",
        "- **Characteristics:**\n",
        "  - The model only consists of the decoder part of the Transformer.\n",
        "  - It is used for generating text that's unconditioned on any external input, meaning it generates text based on the patterns it has learned without any specific prompt or context.\n",
        "  - It employs a triangular mask, giving it an auto-regressive property, ensuring that each token can only attend to previous tokens and not future ones.\n",
        "- **Use Cases:** Models like GPT use a decoder-only Transformer for tasks like language modeling.\n",
        "\n",
        "### **2. The Encoder-Decoder Transformer:**\n",
        "- **Origin:** The original Transformer paper introduced this architecture primarily for machine translation tasks.\n",
        "- **Characteristics:**\n",
        "  - **Encoder:** Processes the input tokens (e.g., a sentence in French) without any triangular mask, allowing all tokens to interact with each other. It encodes the information present in the input sentence.\n",
        "  - **Decoder:** Generates the output tokens (e.g., the translated sentence in English) while being conditioned on the encoded input. It uses a triangular mask for auto-regressive generation.\n",
        "- **Cross-Attention Mechanism:**\n",
        "  - An additional component in the decoder that allows it to attend to the encoder's outputs.\n",
        "  - While the decoder generates queries from its own tokens, the keys and values for the attention mechanism come from the encoder's outputs. This allows the decoder to condition its generation based on the encoded input.\n",
        "\n",
        "### **3. The Importance of Conditioning in Encoder-Decoder:**\n",
        "- **Special Tokens:** In tasks like translation, special tokens like \"start\" and \"end\" are used to signal the beginning and end of generation.\n",
        "- **Conditioning:** The decoder conditions its generation based on:\n",
        "  1. Its past tokens.\n",
        "  2. The full encoded context from the encoder.\n",
        "  \n",
        "### **4. Why Use Decoder-Only in the Given Context:**\n",
        "- **No Need for Conditioning:** In the presented scenario, there's no external input to condition upon. The model's objective is simply to generate text that mimics the patterns in the training dataset.\n",
        "- **Example:** The GPT model follows this decoder-only approach, making it apt for tasks like unconditioned text generation.\n",
        "\n",
        "### **Conclusion:**\n",
        "The choice between a decoder-only Transformer and an encoder-decoder Transformer is task-dependent. While the encoder-decoder architecture is well-suited for tasks like translation where output generation depends on a given input, the decoder-only architecture is apt for tasks like language modeling where generation is unconditioned.\n",
        "\n"
      ],
      "metadata": {
        "id": "ahhEqul41so7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nano GPT\n",
        "\n",
        "The lecturer is introducing and comparing the structure and components of \"nanoGPT\" to the code and concepts they have discussed earlier. Here are the key intuitions they aim to convey:\n",
        "\n",
        "### **1. Introduction to nanoGPT:**\n",
        "- **Files of Interest:** `train.py` and `model.py`.\n",
        "  - `train.py`: Contains all the necessary code for training the model. It is more intricate than the basic loop presented earlier, handling tasks like saving/loading checkpoints, learning rate decay, distributed training, etc.\n",
        "  - `model.py`: Holds the model definition and should be very familiar based on the prior discussions.\n",
        "\n",
        "### **2. Causal Self-Attention Block:**\n",
        "- **Structure:** The block produces queries, keys, and values; performs dot products; applies masking and softmax; and then pools the values.\n",
        "- **Comparison with Earlier Code:**\n",
        "  - The main difference is in the multi-headed attention implementation.\n",
        "  - The lecturer's previous code treated each head separately, then concatenated their outputs.\n",
        "  - In nanoGPT, all heads are batch-processed together, introducing an additional tensor dimension for the heads. This approach is more efficient but makes the code appear more complex due to the four-dimensional tensors.\n",
        "\n",
        "### **3. Multi-Layer Perceptron (MLP):**\n",
        "- **Non-linearity:** The MLP in nanoGPT uses the GeLU (Gaussian Error Linear Unit) activation function instead of ReLU. This change is motivated by OpenAI's use of GeLU in their models, and the desire to load their checkpoints.\n",
        "\n",
        "### **4. Transformer Blocks:**\n",
        "- These are identical to the ones previously discussed, comprising the communication (attention) phase and the computation (feed-forward) phase.\n",
        "\n",
        "### **5. GPT Model Structure:**\n",
        "- **Components:**\n",
        "  - Token and position encodings.\n",
        "  - Transformer blocks.\n",
        "  - A final layer normalization.\n",
        "  - A linear layer for output generation.\n",
        "- **Comparison with Earlier Discussion:** The structure is very similar to what was previously explained, but with added functionalities for checkpoint management and other details.\n",
        "\n",
        "### **6. Additional Details:**\n",
        "- The code in nanoGPT also distinguishes between parameters that should undergo weight decay and those that shouldn't.\n",
        "- The generation function in nanoGPT is also quite similar to the earlier version.\n",
        "\n",
        "### **Conclusion:**\n",
        "The lecturer wants the audience to understand that while the nanoGPT implementation contains additional functionalities and optimizations compared to the basic version they discussed, the core concepts and components remain largely the same. The audience should be able to recognize and understand the majority of the components in nanoGPT based on their earlier discussions."
      ],
      "metadata": {
        "id": "vnVv_zSo2hJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-training and Fine-tuning\n",
        "\n",
        "The lecturer delves into the processes and intricacies of training models like ChatGPT, particularly focusing on the contrast between pre-training and fine-tuning, and how the model evolves from a mere text generator to an assistant-like behavior. Here are the key intuitions:\n",
        "\n",
        "### **1. Two Distinct Stages for Training ChatGPT:**\n",
        "- **Pre-training:** Training a model on vast amounts of data (e.g., internet text) to generate coherent text. This is an unsupervised learning phase.\n",
        "- **Fine-tuning:** Refining the pre-trained model using specific, often smaller, datasets to make it suitable for particular tasks like answering questions.\n",
        "\n",
        "### **2. Pre-training in Depth:**\n",
        "- **Scale Difference:** The lecturer contrasts the small-scale model they discussed (10 million parameters trained on 300,000 tokens) with GPT-3 (175 billion parameters trained on 300 billion tokens). The latter is a massive model, requiring significant computational resources and infrastructure.\n",
        "- **Output Characteristic:** A pre-trained model behaves as a \"document completer.\" It doesn't necessarily answer questions but extends text based on its training. For instance, it might continue with more questions or generate content resembling news articles.\n",
        "\n",
        "### **3. Transition from Pre-training to Fine-tuning:**\n",
        "- **Need for Fine-tuning:** A pre-trained model can generate text, but its behavior is undefined. It might not answer questions directly or may provide irrelevant continuations. Fine-tuning aligns the model to specific tasks.\n",
        "  \n",
        "### **4. Fine-tuning in Detail:**\n",
        "- **Collection of Assistant-Like Data:** Training data is collected where questions are followed by answers, aligning the model to expect and generate answers for given questions.\n",
        "- **Rating and Reward Model:** The model's responses are rated by human reviewers. This feedback is used to train a reward model, which predicts the desirability of a given response.\n",
        "- **Reinforcement Learning (RL) Optimization:** Using the reward model, Proximal Policy Optimization (PPO) - an RL technique - is employed to adjust the model's response generation process. The goal is to make the model produce answers that are expected to get high rewards based on the reward model.\n",
        "\n",
        "### **5. Distinction between NanoGPT and ChatGPT:**\n",
        "- **NanoGPT:** Focuses on the pre-training stage, similar to the smaller model the lecturer discussed.\n",
        "- **ChatGPT:** Incorporates both pre-training and fine-tuning. While pre-training can be seen as a more general phase (akin to the work done in NanoGPT), the fine-tuning, especially with proprietary data, is what makes ChatGPT a responsive assistant.\n",
        "\n",
        "### **Conclusion:**\n",
        "The process of creating a responsive model like ChatGPT isn't merely about training it on vast datasets. It involves a nuanced two-step procedure: an initial broad learning phase (pre-training) followed by a targeted refinement (fine-tuning) to ensure the model behaves as a useful assistant rather than a generic text generator."
      ],
      "metadata": {
        "id": "RHi_sRFq6iuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "nC7V72TrGzGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A suitable dataset for one GPU\n",
        "\n",
        "If you're working with a single GPU on Colab, you're somewhat limited by memory and computational capacity. However, you can still work with reasonably large datasets. Here are a few suggestions that are larger than \"Tiny Shakespeare\" but still manageable on a single GPU:\n",
        "\n",
        "1. **Wikipedia Text**:\n",
        "   - While the full Wikipedia dump is massive, you can use a subset of it. There are pre-processed versions of Wikipedia available, where each article is a plain text file. You can choose a portion that fits within your memory constraints.\n",
        "\n",
        "2. **Project Gutenberg**:\n",
        "   - It offers over 60,000 free eBooks, including many classics. The dataset is substantial but not overwhelming for a single GPU.\n",
        "\n",
        "3. **Penn Tree Bank (PTB)**:\n",
        "   - It's a popular dataset for language modeling tasks. Though not huge, it's significantly larger than Tiny Shakespeare.\n",
        "\n",
        "4. **AG News**:\n",
        "   - It's a dataset for news categorization, but you can use the text for language modeling tasks. It contains 120,000 training samples and 7,600 test samples.\n",
        "\n",
        "5. **IMDb Movie Reviews**:\n",
        "   - This dataset contains 50,000 movie reviews for natural language processing or text analytics tasks. It's divided evenly with 25,000 reviews intended for training and 25,000 for testing.\n",
        "\n",
        "6. **Stack Exchange Data Dump**:\n",
        "   - This is a substantial dataset that contains questions, answers, and comments from Stack Exchange, but you can choose a subset that's manageable for your GPU.\n",
        "\n",
        "7. **Blog Authorship Corpus**:\n",
        "   - Contains over 600,000 posts from 19,000 bloggers. The data is available in a single text file, divided by author.\n",
        "\n",
        "**Tips for Managing Large Datasets on Colab**:\n",
        "\n",
        "- **Load Data Incrementally**: Instead of loading the entire dataset into memory, use Python generators or data loaders that fetch data in batches.\n",
        "  \n",
        "- **Compress Text**: Use efficient text encoding or compression techniques to handle more data in memory.\n",
        "\n",
        "- **Gradient Accumulation**: If the dataset is too large to fit in the GPU memory for training, you can use gradient accumulation. This involves forwarding and computing gradients on a mini-batch, but instead of updating the model immediately, you accumulate the gradients over multiple mini-batches and then perform a single update.\n",
        "\n",
        "- **Monitor Memory**: Keep an eye on GPU memory usage in Colab to avoid out-of-memory errors. You can do this using the `nvidia-smi` command.\n",
        "\n",
        "Remember, the idea behind the exercise is to leverage the \"transfer learning\" effect. Even if you can't access the most enormous datasets available, using a dataset significantly larger than \"Tiny Shakespeare\" should help demonstrate the concept effectively."
      ],
      "metadata": {
        "id": "WvrMnX0fJDpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is all you need (summarized)"
      ],
      "metadata": {
        "id": "hHNdbK5kc0AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract\n",
        "#### Key Points:\n",
        "1. Traditional sequence transduction models use recurrent or convolutional neural networks with both an encoder and a decoder.\n",
        "2. The best of these models also utilize an attention mechanism to connect the encoder and decoder.\n",
        "3. The authors introduce the \"Transformer\" which is a new architecture based solely on attention mechanisms. It does away with recurrent and convolutional structures.\n",
        "4. The Transformer is superior in quality, more parallelizable, and requires less training time.\n",
        "5. They achieved state-of-the-art results on English-to-German and English-to-French translation tasks.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "The authors are introducing a new architecture called the Transformer. Unlike traditional models that use recurrent or convolutional neural networks, the Transformer only uses attention mechanisms. This new model is not only better in performance but also requires less training time. They showcase its effectiveness in language translation tasks.\n",
        "\n",
        "### 1 Introduction\n",
        "#### Key Points:\n",
        "1. Recurrent neural networks (RNNs), especially long short-term memory (LSTM) and gated recurrent (GRU) networks, are the leading approaches for sequence modeling tasks like language translation.\n",
        "2. Despite their success, RNNs have a limitation: they process data sequentially, which makes parallelization difficult. This is problematic for longer sequences.\n",
        "3. Attention mechanisms, which allow models to focus on specific parts of the input, have become a key component in many sequence modeling tasks.\n",
        "4. The Transformer model proposed by the authors does away with recurrent networks and relies solely on attention.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "RNNs, especially LSTMs and GRUs, are popular for sequence tasks like translation. However, they process data one step at a time, making them hard to speed up through parallelization. To overcome this limitation, the authors introduce the Transformer model, which only uses attention mechanisms and skips the traditional RNN structure.\n",
        "\n",
        "### 2 Background\n",
        "#### Key Points:\n",
        "1. The aim to reduce sequential computation led to models like the Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks. They process data in parallel but struggle with long-distance dependencies in the data.\n",
        "2. The Transformer overcomes this by using constant operations regardless of position distances, though at the cost of some resolution. This resolution issue is addressed with Multi-Head Attention.\n",
        "3. Self-attention allows a sequence to refer to itself to compute a representation. It's been useful in various tasks such as reading comprehension and summarization.\n",
        "4. Some models use attention in a recurrent manner, but the Transformer is unique in that it uses only self-attention and does away with RNNs or convolutions.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "There have been other models trying to reduce the step-by-step nature of RNNs by using convolutions. While they can process data in parallel, they struggle with data that's far apart in the sequence. The Transformer handles this problem better, even though it sacrifices some detail. However, it makes up for this with a feature called Multi-Head Attention. What sets the Transformer apart is its exclusive use of self-attention without relying on RNNs or convolutions.\n",
        "\n",
        "---\n",
        "\n",
        "Overall, the paper is introducing the Transformer model, a novel approach to sequence modeling that relies solely on attention mechanisms, eliminating the need for traditional RNNs or convolutions. This approach not only achieves state-of-the-art results in translation tasks but also addresses the limitations of previous models, especially when dealing with long sequences."
      ],
      "metadata": {
        "id": "zQbhi0NAc49u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 Model Architecture\n",
        "\n",
        "#### Basic Overview:\n",
        "Most neural models used for transforming sequences (like turning a sentence from one language into another) have an encoder-decoder structure.\n",
        "\n",
        "1. **Encoder**: Takes in a sequence of symbols and turns them into a continuous representation.\n",
        "2. **Decoder**: Takes the continuous representation and generates an output sequence of symbols.\n",
        "\n",
        "The idea is similar to understanding a sentence in one language (encoding) and then thinking of how to say it in another language (decoding).\n",
        "\n",
        "The Transformer uses this structure but with a twist. It uses \"self-attention\" and certain types of layers in both the encoder and decoder.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "Imagine you're translating a book from English to French. The encoder's job is to understand the English, and the decoder's job is to write it out in French. The Transformer does this, but it has a special way of understanding and writing using something called \"self-attention.\"\n",
        "\n",
        "### 3.1 Encoder and Decoder Stacks\n",
        "\n",
        "#### Encoder:\n",
        "1. The encoder has 6 layers, stacked on top of each other. Think of this as 6 levels of understanding the input.\n",
        "2. Every layer has two main parts:\n",
        "   - **Multi-head self-attention mechanism**: Helps the model focus on different parts of the input for better understanding.\n",
        "   - **Position-wise fully connected feed-forward network**: A simple layer that transforms its input.\n",
        "3. There's something called a \"residual connection\" around each part. This helps in faster training and prevents the vanishing gradient problem. Simply, it's like a shortcut connection that skips one or more layers.\n",
        "4. The output from each of these parts goes through a normalization step to keep the model's outputs well-scaled and centered.\n",
        "5. All outputs have a dimension of 512, which means they're represented as vectors of length 512.\n",
        "\n",
        "#### Simplified Explanation (Encoder):\n",
        "The encoder has 6 steps (or layers) to understand the input. In each step, it pays special attention to different parts of the input and then does a simple transformation. It also has shortcuts to speed up understanding and keeps everything in a consistent format.\n",
        "\n",
        "#### Decoder:\n",
        "1. The decoder also has 6 layers, similar to the encoder.\n",
        "2. Every layer in the decoder has three main parts:\n",
        "   - Two are the same as in the encoder: multi-head self-attention and position-wise fully connected network.\n",
        "   - The third part is a **multi-head attention over the encoder's output**. This helps the decoder to focus on relevant parts of the input when generating the output.\n",
        "3. Just like the encoder, there are residual connections and normalization.\n",
        "4. An important feature in the decoder is that it prevents future positions from being used as input. This ensures that when predicting a word in the output, it only uses previous words, not future ones.\n",
        "\n",
        "#### Simplified Explanation (Decoder):\n",
        "The decoder has 6 steps to write out the translation. In each step, it not only pays special attention to the output it's generating but also looks back at the entire input to make sure it's translating correctly. It's careful to only use the words it has already translated to predict the next word, ensuring the translation makes sense in order.\n",
        "\n",
        "---\n",
        "\n",
        "In essence, the Transformer's architecture is about understanding the input deeply using attention mechanisms and then generating an output that refers back to this deep understanding, all while ensuring the sequence makes sense. The design choices, like the stacked layers, residual connections, and normalization, make the model powerful and efficient."
      ],
      "metadata": {
        "id": "sykJ8sBSdEN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Attention\n",
        "\n",
        "#### Basic Overview:\n",
        "At a high level, attention works by mapping a query and a set of key-value pairs to an output. The output is a weighted combination of values based on how well their corresponding keys match the query.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "Imagine you're in a room with several people, and you want to gather opinions on a topic. Instead of listening to everyone equally, you pay more attention to those who seem to have relevant insights (based on your query). This selective listening is the essence of attention.\n",
        "\n",
        "### 3.2.1 Scaled Dot-Product Attention\n",
        "\n",
        "#### Key Points:\n",
        "- The input consists of queries and keys of a certain size (dimension \\( dk \\)) and values of dimension \\( dv \\).\n",
        "- To determine how much attention (weight) to give each value, they compute the dot product of the query with all keys, scale it down, and then apply a softmax function.\n",
        "- They do this for multiple queries at once by packing them into matrices.\n",
        "- The formula is given by:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V ) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{dk}}\\right)V\n",
        "$$\n",
        "\n",
        "- There are two main types of attention functions: additive and dot-product. The Transformer uses the dot-product version, but scales it to ensure stability and effectiveness, especially when the dimension \\( dk \\) is large.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "This is a specific type of attention where they measure the relevance of keys to a query by taking the dot product (a measure of similarity). However, to avoid extremely large values which could mess up the softmax function, they scale the dot products down by a factor. This makes the attention mechanism stable and effective.\n",
        "\n",
        "### 3.2.2 Multi-Head Attention\n",
        "\n",
        "#### Key Points:\n",
        "- Instead of one set of attention weights, they compute multiple sets with different learned projections and then combine them. This allows the model to capture different types of relationships in the data.\n",
        "- The outputs from these multiple attention heads are concatenated and then transformed.\n",
        "- Using multiple heads allows the model to pay attention to different parts of the input simultaneously in various ways.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "Imagine trying to understand a sentence. Some words relate to each other based on grammar, some based on meaning, and some based on tone. Instead of trying to capture all these relationships with one attention mechanism, they use multiple 'heads' to pay attention in different ways and then combine what each head learns.\n",
        "\n",
        "### 3.2.3 Applications of Attention in our Model\n",
        "\n",
        "#### Key Points:\n",
        "1. **Encoder-Decoder Attention**: This allows the decoder to focus on relevant parts of the encoder's output, similar to traditional sequence-to-sequence models.\n",
        "2. **Self-Attention in Encoder**: Every position in the encoder can pay attention to all other positions in its previous layer. It's like each word in a sentence checking how it relates to every other word.\n",
        "3. **Self-Attention in Decoder**: Similar to the encoder, but with a catch - a word can only pay attention to preceding words to ensure the correct sequence is generated.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "The Transformer uses attention in three main ways:\n",
        "1. To let the decoder look back at the entire input, helping it make informed translations.\n",
        "2. To let each part of the input look at every other part, helping in understanding context.\n",
        "3. To let the output look at its previous parts, ensuring the translation makes sense in sequence.\n",
        "\n",
        "---\n",
        "\n",
        "In essence, attention mechanisms let the Transformer model focus selectively on parts of the input data. By using scaled dot-product attention and multi-head attention, the model can capture various relationships in the data, making its translations more accurate and context-aware."
      ],
      "metadata": {
        "id": "qzqfKr6XdQRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Position-wise Feed-Forward Networks\n",
        "\n",
        "#### Key Points:\n",
        "- Both the encoder and decoder layers have a feed-forward network applied at each position in a similar manner.\n",
        "- The network consists of two linear transformations separated by a ReLU activation.\n",
        "- The transformations are applied identically to each position, but the parameters differ between layers.\n",
        "- The idea can be seen as two 1x1 convolutions.\n",
        "- The input and output dimensionality is \\(d_{\\text{model}} = 512\\), and the inner-layer dimensionality is \\(df_f = 2048\\).\n",
        "\n",
        "#### Simplified Explanation:\n",
        "In addition to the attention mechanisms, each layer of the Transformer has a small network that processes each position (or word/token) separately. This network is a set of operations that help transform and refine the information before passing it to the next layer. Think of it as a mini-brain in each layer that helps in refining the data.\n",
        "\n",
        "### 3.4 Embeddings and Softmax\n",
        "\n",
        "#### Key Points:\n",
        "- Like other models, the Transformer uses embeddings to convert input and output tokens into vectors of a certain size (\\(d_{\\text{model}}\\)).\n",
        "- The decoder's output is turned into predicted probabilities for the next token using a linear transformation followed by a softmax function.\n",
        "- The weight matrix used for input and output embeddings is shared with the pre-softmax linear transformation. In the embedding phase, these weights are scaled by the square root of \\(d_{\\text{model}}\\).\n",
        "\n",
        "#### Simplified Explanation:\n",
        "To work with words or tokens, the model first turns them into vectors using embeddings. After processing, when the model wants to predict the next word, it uses a function to convert its internal representation into a set of probabilities for each possible word.\n",
        "\n",
        "### 3.5 Positional Encoding\n",
        "\n",
        "#### Key Points:\n",
        "- The Transformer doesn't inherently understand the order of a sequence (since it doesn't use recurrent or convolutional layers). To address this, it uses positional encodings.\n",
        "- Positional encodings are added to input embeddings to give the model information about the position of tokens in the sequence.\n",
        "- The paper uses sine and cosine functions to generate these positional encodings. This choice allows the model to potentially understand relative positions.\n",
        "- Another approach using learned positional embeddings was tested, but the sine-cosine method was chosen as it might help the model handle longer sequences than those seen during training.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "Imagine reading a sentence without knowing the order of the words; it would be confusing! Since the Transformer doesn't know the order of words by design, it gets a little help from \"positional encodings.\" These are like labels saying, \"this word is the 1st,\" \"this word is the 2nd,\" and so on. The cool part? Instead of simple labels, the Transformer uses wave-like patterns (sine and cosine functions) to denote these positions, helping it understand sequences better.\n",
        "\n",
        "---\n",
        "\n",
        "In this section, the paper discusses the additional components and techniques used in the Transformer model. These include refining networks (feed-forward networks), methods to convert words to vectors and back (embeddings and softmax), and a strategy to make the model understand the order of words in a sentence (positional encoding)."
      ],
      "metadata": {
        "id": "yBLl7y7qdYQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 Why Self-Attention\n",
        "\n",
        "#### Basic Overview:\n",
        "This section discusses the benefits and rationale of using self-attention over more traditional methods like recurrent and convolutional layers. The authors focus on three main criteria: computational complexity, parallelization, and the ability to capture long-range dependencies in a sequence.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "When building models for tasks like translating a sentence, it's crucial to choose the right building blocks. Here, they're explaining why they went with \"self-attention\" over other popular choices.\n",
        "\n",
        "### Criteria for Choosing Self-Attention:\n",
        "\n",
        "1. **Computational Complexity**: How much computation does each method need?\n",
        "2. **Parallelization**: How much of the computation can be done simultaneously, speeding up the process?\n",
        "3. **Path Length for Long-Range Dependencies**: How quickly can the model recognize relationships between words that are far apart?\n",
        "\n",
        "#### Insights:\n",
        "\n",
        "- **Recurrent layers** process one word at a time, meaning they have a high sequential operation count. This makes them slow as they can't fully utilize modern hardware that performs best when processing many things at once.\n",
        "  \n",
        "- **Self-Attention**, on the other hand, connects every word to every other word in a fixed number of steps, making it faster and better at capturing relationships between distant words.\n",
        "  \n",
        "- When looking at computational effort, self-attention is faster than recurrent layers, especially when the sentence (or sequence) length is shorter than the size of the word representations.\n",
        "  \n",
        "- **Convolutional layers** don't connect all words to each other unless they're stacked many times or use certain techniques. This stacking makes them slower and less effective at capturing long-distance relationships compared to self-attention.\n",
        "\n",
        "- A potential improvement they're considering is to restrict self-attention to a local neighborhood of words, making it even faster for very long sentences.\n",
        "\n",
        "- Interestingly, self-attention can also help make models more interpretable. By inspecting the attention patterns, we can see which parts of a sentence the model thinks are related or important.\n",
        "\n",
        "#### Key Takeaway:\n",
        "Self-attention offers a sweet spot. It's computationally efficient, can be highly parallelized, and excels at recognizing long-range relationships in data, making it a great choice for tasks like translation. Plus, it can give insights into how it's thinking, which is a bonus for understanding and trust.\n",
        "\n",
        "---\n",
        "\n",
        "In essence, this section of the paper is advocating for the use of self-attention by highlighting its advantages in terms of computational efficiency, parallel processing capabilities, and ability to capture relationships in data, especially over long distances."
      ],
      "metadata": {
        "id": "liq0Sd87dc-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5 Training\n",
        "\n",
        "#### Overview:\n",
        "This section explains how the Transformer model was trained, detailing the datasets used, the hardware setup, optimization strategies, and regularization techniques.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "\n",
        "1. **Training Data and Batching**:\n",
        "    - They trained the model on two datasets: English-German (with 4.5 million sentences) and English-French (with 36 million sentences).\n",
        "    - They used a technique called byte-pair encoding to represent sentences, which breaks down words into smaller chunks. This technique resulted in vocabularies of around 37,000 and 32,000 tokens for the two datasets, respectively.\n",
        "    - In training, they grouped sentences together based on length, and each batch (group) had approximately 25,000 source and 25,000 target tokens.\n",
        "\n",
        "2. **Hardware and Schedule**:\n",
        "    - They used a machine with 8 NVIDIA P100 GPUs.\n",
        "    - The base models took about 12 hours to train for 100,000 steps, while the larger models took 3.5 days for 300,000 steps.\n",
        "\n",
        "3. **Optimizer**:\n",
        "    - They used the Adam optimizer for training.\n",
        "    - The learning rate (how fast the model learns) was adjusted during training using a formula that increases it initially and then decreases it. This helps in stabilizing the training process.\n",
        "\n",
        "4. **Regularization**:\n",
        "    - They used dropout, a technique that randomly \"drops\" (ignores) some neurons during training to prevent overfitting. This was applied to various parts of the model.\n",
        "    - Label smoothing was used, which makes the model a bit uncertain, leading to better generalization and improved BLEU scores (a metric for translation quality).\n",
        "\n",
        "### 6 Results\n",
        "\n",
        "#### Overview:\n",
        "The results section provides performance metrics for the Transformer model and compares it to other models from previous research.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "\n",
        "1. **Machine Translation**:\n",
        "    - For the English-to-German translation task, their model outperformed all previously reported models, setting a new best score.\n",
        "    - Similarly, for English-to-French, their model achieved top performance while being more efficient in training.\n",
        "    - They mention some technical details about how they achieved these results, like averaging multiple model versions and using beam search for better translation outputs.\n",
        "\n",
        "2. **Model Variations**:\n",
        "    - They tested variations of the Transformer model to understand the impact of different components.\n",
        "    - For instance, changing the number of attention heads or the size of certain parameters had varying effects on performance.\n",
        "    - They found that their sinusoidal positional encoding (how they incorporated word order information) performed similarly to other methods.\n",
        "\n",
        "### 7 Conclusion\n",
        "\n",
        "#### Overview:\n",
        "The authors summarize their findings and hint at future directions for their research.\n",
        "\n",
        "#### Simplified Explanation:\n",
        "\n",
        "- They introduced the Transformer, a new type of model focused entirely on attention mechanisms. It doesn't rely on the recurrent layers used in many other models.\n",
        "- This model trains faster and achieves better results on translation tasks than previous models.\n",
        "- The authors are excited about using attention-based models for other tasks and are considering modifications to handle other types of data like images and audio.\n",
        "\n",
        "---\n",
        "\n",
        "In essence, this part of the paper details the practical aspects of training the Transformer, its performance on translation tasks, and the potential future applications and improvements of the model."
      ],
      "metadata": {
        "id": "_hHw9f4xd_Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encodings"
      ],
      "metadata": {
        "id": "k3V0e-IxiNHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Positional Encodings: The Basic Idea\n",
        "\n",
        "Imagine you're trying to read a sentence, but all the words are jumbled up. It would be hard to understand, right? The order of words in a sentence is crucial for understanding its meaning.\n",
        "\n",
        "Now, the Transformer's attention mechanism is fantastic at determining which words are important in relation to other words, but it doesn't inherently understand the order of words in a sequence. That's where positional encodings come in. They give the Transformer a sense of word order, or position.\n",
        "\n",
        "### The How: Using Sinusoidal Functions\n",
        "\n",
        "To give the Transformer this sense of word order, the authors of the paper injected information about the position of each word in the sequence. They did this using sinusoidal functions (sine and cosine functions).\n",
        "\n",
        "Here's a simple analogy:\n",
        "\n",
        "Imagine you're at a music festival, and there are different instruments playing at different frequencies. Some instruments produce low-pitched sounds, while others produce high-pitched sounds. If you listen carefully, you can identify each instrument by its unique frequency.\n",
        "\n",
        "Similarly, for each position of a word in a sentence, the Transformer uses a combination of sine and cosine functions at different frequencies to produce a unique positional encoding. By adding these positional encodings to the word embeddings (word representations), the model can then tell which word came before another, even if they are similar or related.\n",
        "\n",
        "### Why Sinusoids?\n",
        "\n",
        "You might wonder: why use sinusoidal functions? Why not just number the positions or use some other method?\n",
        "\n",
        "The clever part about using sinusoidal functions is that they can capture patterns in positions and can be easily extrapolated for longer sequences. This means that even if the Transformer was trained on shorter sentences, the sinusoidal positional encodings can help it handle longer sentences it hasn't seen before.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Think of positional encodings as \"labels\" that tell the Transformer where each word sits in a sentence. By using a combination of sine and cosine functions, the Transformer can understand word order, which is essential for tasks like translation or sentence understanding."
      ],
      "metadata": {
        "id": "KAK4AG_ViQP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When Did the Concept of Positional Encodings Arise?\n",
        "\n",
        "The idea of positional encodings, or ways to represent the position of data within a sequence, has been around in various forms for a while. However, it became particularly crucial with the advent of the Transformer architecture. Since Transformers rely heavily on self-attention mechanisms, which do not inherently understand the sequence's order, there was a need for a mechanism to provide that order information. That's where positional encodings came into the spotlight.\n",
        "\n",
        "### A Simple Example of Positional Encodings:\n",
        "\n",
        "Imagine you have a sentence: \"I love cats.\"\n",
        "\n",
        "For simplicity, let's assume our word embeddings (vector representations of words) are:\n",
        "\n",
        "- I: [0.5, 1.0]\n",
        "- love: [0.2, 0.8]\n",
        "- cats: [0.9, 0.3]\n",
        "\n",
        "Now, if we just used these embeddings in a Transformer, it wouldn't know the order of these words. To provide this information, we can use a basic form of positional encoding: just add a value based on the position to each embedding.\n",
        "\n",
        "**Step-by-step Positional Encoding**:\n",
        "\n",
        "1. **Choose an Encoding Strategy**:\n",
        "   \n",
        "   One of the simplest strategies is to use an incremental value based on the position of the word in the sentence.\n",
        "   \n",
        "2. **Assign Positional Values**:\n",
        "\n",
        "   - I: 1\n",
        "   - love: 2\n",
        "   - cats: 3\n",
        "\n",
        "3. **Combine Word Embeddings with Positional Values**:\n",
        "\n",
        "   For this example, let's just add the positional value to each component of the word embedding (though in reality, you might use a more complex method):\n",
        "\n",
        "   - I: [0.5 + 1, 1.0 + 1] = [1.5, 2.0]\n",
        "   - love: [0.2 + 2, 0.8 + 2] = [2.2, 2.8]\n",
        "   - cats: [0.9 + 3, 0.3 + 3] = [3.9, 3.3]\n",
        "\n",
        "The resulting vectors now contain information about both the meaning of the word (from the embedding) and its position in the sentence (from the encoding). When these enhanced vectors are fed into the Transformer, it has a better sense of word order.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The above method is a basic way to incorporate positional information. While it's straightforward, it may not capture relative positional relationships as effectively as sinusoidal encodings. However, it serves as a clear illustration of the concept. In practice, more complex and nuanced methods, like sinusoidal encodings, are used to capture a richer sense of position within a sequence."
      ],
      "metadata": {
        "id": "jTAbewa1iVmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Issue with Simple Addition:\n",
        "\n",
        "As you rightly pointed out, simply adding a position value to a word's embedding can inadvertently bring unrelated words closer in the vector space.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "Consider two words with embeddings:\n",
        "- Fish: [0.5, 0.5]\n",
        "- Duck: [0.6, 0.6]\n",
        "\n",
        "If \"Fish\" is at position 0 and \"Duck\" is at position 11:\n",
        "\n",
        "- Fish (position 0): [0.5 + 0, 0.5 + 0] = [0.5, 0.5]\n",
        "- Duck (position 11): [0.6 + 11, 0.6 + 11] = [11.6, 11.6]\n",
        "\n",
        "By this encoding, these two words would move far apart in the vector space. But if \"Duck\" was at position -0.1:\n",
        "\n",
        "- Duck (position -0.1): [0.6 - 0.1, 0.6 - 0.1] = [0.5, 0.5]\n",
        "\n",
        "Now, \"Fish\" and \"Duck\" would be identical in the vector space, despite being different words!\n",
        "\n",
        "### Practical Implications:\n",
        "\n",
        "1. **Distorting Semantic Meaning**: By simply adding positional values, we risk overshadowing the original semantic information captured in the embeddings. In extreme cases, as in our example, distinct words could end up having identical representations.\n",
        "\n",
        "2. **Limited Scalability**: As sentences get longer, the positional values increase, causing the embeddings to spread out further in the vector space. This could lead to challenges in training, as the model would need to handle a wider range of values.\n",
        "\n",
        "3. **Difficulties in Generalization**: If the model is trained on shorter sentences and then tested on longer ones, the new positional encodings for longer sentences might be outside the model's \"comfort zone\", leading to unpredictable results.\n",
        "\n",
        "### Does This Simple Way Work in Practice?\n",
        "\n",
        "While the method is intuitive and easy to understand, in practice, it has limitations for the reasons discussed above. The Transformer's sinusoidal positional encoding was designed to address some of these issues. It ensures that:\n",
        "\n",
        "- The positional encoding is bounded (values stay between -1 and 1).\n",
        "- The model can generalize to different sentence lengths because the sinusoidal pattern is consistent.\n",
        "- Word embeddings retain their semantic meaning while also incorporating positional information.\n",
        "\n",
        "### Step-by-Step Conclusion:\n",
        "\n",
        "1. Simple addition of positional values can distort the semantic information in word embeddings.\n",
        "2. This method can cause unrelated words to have similar or even identical representations.\n",
        "3. While it might work for very specific cases or small experiments, it's not robust or flexible enough for varied and complex tasks.\n",
        "4. More sophisticated methods, like the sinusoidal positional encodings used in the Transformer, are preferred because they efficiently capture position information without compromising the semantic content of embeddings."
      ],
      "metadata": {
        "id": "peSaK8u5iewr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different types of positional encodings\n",
        "\n",
        "Positional encodings are essential to give models like the Transformer information about the position of words, as they don't inherently understand sequence order. Let's go through some of the different types of positional encodings:\n",
        "\n",
        "### 1. **Learned Positional Encoding**:\n",
        "- **Description**: Instead of using a fixed positional encoding, the model has embeddings for positions, which are initialized randomly and then refined during training. The model \"learns\" the best way to represent position for the task at hand.\n",
        "- **Advantages**:\n",
        "  - Can adapt to specific characteristics of the data.\n",
        "- **Disadvantages**:\n",
        "  - Might overfit to specific sequence lengths seen during training.\n",
        "\n",
        "### 2. **Sinusoidal Positional Encoding**:\n",
        "- **Description**: Used in the original Transformer model, this encoding uses sine and cosine functions of different frequencies to generate positional encodings. It allows the model to potentially generalize to sequence lengths outside of the training set.\n",
        "- **Advantages**:\n",
        "  - Deterministic and doesn't need training.\n",
        "  - Can generalize to longer sequences.\n",
        "- **Disadvantages**:\n",
        "  - Not specialized to any particular dataset.\n",
        "\n",
        "### 3. **Absolute Positional Encoding**:\n",
        "- **Description**: Assigns a unique identifier (usually an integer) to each position in a sequence.\n",
        "- **Advantages**:\n",
        "  - Simple and easy to understand.\n",
        "- **Disadvantages**:\n",
        "  - Doesn't allow the model to generalize to sequence lengths not seen during training.\n",
        "\n",
        "### 4. **Relative Positional Encoding**:\n",
        "- **Description**: Instead of focusing on the absolute position of words, this encoding looks at the relative positions between them. This is useful, for example, when the relationship between words (like \"A is two words before B\") is more important than their absolute positions.\n",
        "- **Advantages**:\n",
        "  - Can capture more nuanced relationships between words.\n",
        "- **Disadvantages**:\n",
        "  - More complex to implement.\n",
        "\n",
        "### 5. **Fixed Bucket Positional Encoding**:\n",
        "- **Description**: Positions are divided into a set number of buckets, and each bucket is assigned a positional encoding. Words falling into the same bucket share the same encoding.\n",
        "- **Advantages**:\n",
        "  - Reduces the number of unique positional encodings, which can be computationally efficient.\n",
        "- **Disadvantages**:\n",
        "  - Loses fine-grained position information.\n",
        "\n",
        "### 6. **Timing Signal Positional Encoding**:\n",
        "- **Description**: A more complex function that generates a deterministic signal for each position, which can be added to embeddings. It tries to ensure that the resulting vectors are distinguishable and can capture complex patterns.\n",
        "- **Advantages**:\n",
        "  - Can capture intricate positional patterns.\n",
        "- **Disadvantages**:\n",
        "  - More complex to understand and implement.\n",
        "\n",
        "### Step-by-Step Summary:\n",
        "\n",
        "1. Positional encodings provide sequence information to models that don't inherently understand order.\n",
        "2. There are several types of positional encodings, each with its own advantages and disadvantages.\n",
        "3. The choice of positional encoding often depends on the specific requirements of the task and the model architecture.\n",
        "\n",
        "Different tasks and datasets might benefit from different types of positional encodings, so it's essential to consider the specific needs and characteristics of the task when choosing an encoding method."
      ],
      "metadata": {
        "id": "CnnOP2BCihfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gCR6mbPsiarC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "eqDDBPgLXSrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Tokenization`\n",
        "\n",
        "In machine learning, especially in natural language processing (NLP), \"In machine learning, especially in natural language processing (NLP), \"tokenization\" refers to the process of converting input text into smaller units, called \"tokens\". These tokens can be as small as characters or as long as words. Tokenization is one of the foundational steps in text preprocessing.\n",
        "\n",
        "**Why tokenize?**\n",
        "1. **Simplicity**: By breaking text down into smaller chunks, we can more easily analyze or process each piece.\n",
        "2. **Flexibility**: Some models might perform better on word-level tokens, while others might prefer characters or even subword units.\n",
        "3. **Vector Representation**: After tokenization, these tokens can be represented as vectors using techniques like one-hot encoding, word embeddings (like Word2Vec or GloVe), or more advanced methods like BERT embeddings.\n",
        "\n",
        "**Examples of tokenization**:\n",
        "\n",
        "1. **Word Tokenization**:\n",
        "    - Input: \"ChatGPT is great!\"\n",
        "    - Tokens: [\"ChatGPT\", \"is\", \"great\", \"!\"]\n",
        "\n",
        "2. **Character Tokenization**:\n",
        "    - Input: \"Chat\"\n",
        "    - Tokens: [\"C\", \"h\", \"a\", \"t\"]\n",
        "\n",
        "3. **Subword Tokenization** (useful for languages with compound words or to capture meaningful subword information):\n",
        "    - Input: \"ChatGPT is awesome\"\n",
        "    - Tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" aw\", \"esome\"]\n",
        "\n",
        "4. **Sentence Tokenization** (or sentence segmentation):\n",
        "    - Input: \"ChatGPT is great. It helps a lot!\"\n",
        "    - Tokens: [\"ChatGPT is great.\", \"It helps a lot!\"]\n",
        "\n",
        "5. **Byte-Pair Encoding (BPE)**:\n",
        "    - BPE is a type of subword tokenization method. Starting with character-level tokenization, BPE repeatedly merges the most frequently adjacent pairs of tokens. This can capture frequent subwords or even full words.\n",
        "    - Input: \"aaabdaaabac\"\n",
        "    - Tokens after BPE: [\"aaa\", \"b\", \"d\", \"aaa\", \"b\", \"ac\"]\n",
        "\n",
        "**Points to Remember**:\n",
        "1. Tokenization is not always straightforward. Some languages, like Chinese, don't use spaces between words. Some languages, like German, frequently form compound words.\n",
        "2. Post-tokenization, further preprocessing might be required, like lowercasing, stemming, or lemmatization, to ensure consistency and reduce vocabulary size.\n",
        "3. The choice of tokenizer can influence the model's performance. It's common to experiment with different tokenization strategies for a specific problem.\n",
        "\n",
        "In essence, tokenization is the act of chopping up text into pieces, called tokens, to make them more digestible for machine learning models.\" refers to the process of converting input text into smaller units, called \"tokens\". These tokens can be as small as characters or as long as words. Tokenization is one of the foundational steps in text preprocessing.\n",
        "\n",
        "**Why tokenize?**\n",
        "1. **Simplicity**: By breaking text down into smaller chunks, we can more easily analyze or process each piece.\n",
        "2. **Flexibility**: Some models might perform better on word-level tokens, while others might prefer characters or even subword units.\n",
        "3. **Vector Representation**: After tokenization, these tokens can be represented as vectors using techniques like one-hot encoding, word embeddings (like Word2Vec or GloVe), or more advanced methods like BERT embeddings.\n",
        "\n",
        "**Examples of tokenization**:\n",
        "\n",
        "1. **Word Tokenization**:\n",
        "    - Input: \"ChatGPT is great!\"\n",
        "    - Tokens: [\"ChatGPT\", \"is\", \"great\", \"!\"]\n",
        "\n",
        "2. **Character Tokenization**:\n",
        "    - Input: \"Chat\"\n",
        "    - Tokens: [\"C\", \"h\", \"a\", \"t\"]\n",
        "\n",
        "3. **Subword Tokenization** (useful for languages with compound words or to capture meaningful subword information):\n",
        "    - Input: \"ChatGPT is awesome\"\n",
        "    - Tokens: [\"Chat\", \"G\", \"PT\", \" is\", \" aw\", \"esome\"]\n",
        "\n",
        "4. **Sentence Tokenization** (or sentence segmentation):\n",
        "    - Input: \"ChatGPT is great. It helps a lot!\"\n",
        "    - Tokens: [\"ChatGPT is great.\", \"It helps a lot!\"]\n",
        "\n",
        "5. **Byte-Pair Encoding (BPE)**:\n",
        "    - BPE is a type of subword tokenization method. Starting with character-level tokenization, BPE repeatedly merges the most frequently adjacent pairs of tokens. This can capture frequent subwords or even full words.\n",
        "    - Input: \"aaabdaaabac\"\n",
        "    - Tokens after BPE: [\"aaa\", \"b\", \"d\", \"aaa\", \"b\", \"ac\"]\n",
        "\n",
        "**Points to Remember**:\n",
        "1. Tokenization is not always straightforward. Some languages, like Chinese, don't use spaces between words. Some languages, like German, frequently form compound words.\n",
        "2. Post-tokenization, further preprocessing might be required, like lowercasing, stemming, or lemmatization, to ensure consistency and reduce vocabulary size.\n",
        "3. The choice of tokenizer can influence the model's performance. It's common to experiment with different tokenization strategies for a specific problem.\n",
        "\n",
        "In essence, tokenization is the act of chopping up text into pieces, called tokens, to make them more digestible for machine learning models."
      ],
      "metadata": {
        "id": "91y4G-js9vsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Explanation of \"to tokenize\"`\n",
        "\n",
        "\n",
        "The lecturer's statement is a concise and accurate representation of the term \"tokenize\" in the context of machine learning and natural language processing (NLP).\n",
        "\n",
        "`when people say tokenize they mean convert the raw text as a string to some sequence of integers according to some vocabulary of possible elements`\n",
        "\n",
        "Let's break down the statement:\n",
        "\n",
        "1. **\"convert the raw text as a string\"**: This emphasizes that our starting point is typically raw, unprocessed text, which computers don't inherently understand.\n",
        "\n",
        "2. **\"to some sequence of integers\"**: This is the crux of tokenization in NLP. For computational models, especially neural networks, it's more efficient to work with numerical data. Therefore, we map textual data to integers.\n",
        "\n",
        "3. **\"according to some vocabulary of possible elements\"**: This highlights the importance of a vocabulary, which is essentially a mapping between words (or characters, subwords, etc.) and integers. For example, in a simple vocabulary, the word \"apple\" might be represented by the integer 5, \"banana\" by 6, and so on.\n",
        "\n",
        "When you tokenize a piece of text, you're essentially converting words (or other units) into numbers that reference entries in this predefined vocabulary. For instance, the sentence \"apple banana apple\" might be tokenized to [5, 6, 5] using the vocabulary mentioned above.\n",
        "\n",
        "In conclusion, the lecturer's description captures the essence of tokenization in NLP. It's about turning textual data, which is human-readable, into a machine-readable format using a consistent reference (the vocabulary)."
      ],
      "metadata": {
        "id": "Fte6tlS_92Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Tokenization tools in the context of natural language processing (NLP) and machine learning`\n",
        "\n",
        "### **1. The Big Picture:**\n",
        "When working with text data in NLP, we often need to convert raw text into a format that machines can understand. This process involves turning words or characters into numerical representations, which is where tools like `SentencePiece` and `tiktoken` come in.\n",
        "\n",
        "### **2. SentencePiece:**\n",
        "**Function**: `SentencePiece` is a data-driven text tokenizer and detokenizer mainly for Neural Network-based text generation tasks.\n",
        "\n",
        "#### Why is it needed?\n",
        "- **Language Agnostic**: Traditional tokenization methods are language-specific. `SentencePiece`, however, can tokenize any language's text.\n",
        "  \n",
        "- **Consistency**: It provides a consistent tokenization strategy, especially crucial for multilingual models.\n",
        "  \n",
        "- **Subword Tokenization**: Some words might not be in the model's vocabulary. By breaking down words into smaller units (or subwords), models can better handle rare words or even words they've never seen before.\n",
        "\n",
        "#### Inputs and Outputs:\n",
        "- **Input**: Raw text data.\n",
        "- **Output**: A sequence of tokens (words, subwords, or characters) representing the input text. It can also reverse this, turning tokens back into text.\n",
        "\n",
        "### **3. tiktoken:**\n",
        "**Function**: `tiktoken` is a tool to efficiently count the number of tokens in a text dataset without actually tokenizing the data.\n",
        "\n",
        "#### Why is it needed?\n",
        "- **Efficiency**: Full tokenization, especially for large datasets, can be computationally expensive and slow. `tiktoken` offers a quicker way.\n",
        "  \n",
        "- **Estimation**: Before training models or allocating resources, knowing the number of tokens in a dataset can be useful. `tiktoken` provides this insight without the overhead of full tokenization.\n",
        "\n",
        "#### Inputs and Outputs:\n",
        "- **Input**: Raw text data and a tokenizer's vocabulary.\n",
        "- **Output**: An estimated count of tokens that would be produced if the data were tokenized with the given tokenizer.\n",
        "\n",
        "### **In Summary**:\n",
        "Both `SentencePiece` and `tiktoken` are tools in the NLP toolkit that handle different aspects of tokenization. While `SentencePiece` is more about turning text into meaningful tokens (and vice versa), `tiktoken` is about quickly estimating how many tokens are in a text dataset. They help bridge the gap between human-readable text and machine-understandable numerical data."
      ],
      "metadata": {
        "id": "ODtM-VX8BuXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `SentencePiece`\n",
        "\n",
        "### **Background:**\n",
        "Before diving into SentencePiece, it's essential to understand why it exists. Traditional tokenization methods in NLP often rely on spaces or predefined vocabulary. However, these methods can struggle with languages that don't use spaces or have a vast and rich vocabulary. Enter SentencePiece.\n",
        "\n",
        "### **1. What is SentencePiece?**\n",
        "SentencePiece is a data-driven, unsupervised text tokenizer and detokenizer mainly for Neural Network-based text processing systems. It's language-agnostic, meaning you can use it for any language.\n",
        "\n",
        "### **2. How does it work?**\n",
        "Instead of relying on predefined segmentation rules, SentencePiece trains directly on your dataset and learns the best way to split the text. It does this by treating spaces just like any other character, allowing it to learn token boundaries that are optimized for your specific dataset.\n",
        "\n",
        "### **3. Subword Algorithms: BPE and Unigram**\n",
        "SentencePiece incorporates two main algorithms:\n",
        "\n",
        "- **Byte-Pair Encoding (BPE)**: Starts by defining a vocabulary of individual characters and iteratively merges frequent pairs of characters until a desired vocabulary size is reached. This method allows for efficient handling of rare or out-of-vocabulary words.\n",
        "  \n",
        "- **Unigram Language Model Tokenization**: This is a probabilistic tokenization method. It starts with a large vocabulary and prunes it based on the likelihood of tokens and their combinations in the dataset.\n",
        "\n",
        "### **4. Advantages:**\n",
        "- **Language Agnostic**: It doesn't rely on spaces or any pre-defined tokenization rules, making it suitable for any language.\n",
        "  \n",
        "- **Consistent tokenization**: Since it doesn't rely on a fixed vocabulary, it can consistently tokenize any text, even if it wasn't seen during training.\n",
        "  \n",
        "- **Customizable**: You can train SentencePiece on your dataset, allowing it to learn domain-specific tokenizations.\n",
        "\n",
        "### **5. Practical Uses:**\n",
        "SentencePiece has become a popular choice for tokenization in modern NLP models, especially transformer-based models like BERT and its variants, as it can handle a vast range of languages and domains without much customization.\n",
        "\n",
        "### **Conclusion:**\n",
        "In essence, SentencePiece offers a flexible and consistent tokenization approach that doesn't rely on language-specific rules. By learning tokenization directly from data, it can adapt to various languages and domains, making it a versatile choice for many NLP tasks."
      ],
      "metadata": {
        "id": "erhubM0mA47P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `tiktoken`\n",
        "\n",
        "### **Background:**\n",
        "In the realm of machine learning, especially natural language processing (NLP), understanding how many tokens are in a dataset is fundamental. It gives insights into computational requirements and potential model performance. However, counting tokens in very large datasets can be time-consuming. Here's where `tiktoken` comes into play.\n",
        "\n",
        "### **1. What is tiktoken?**\n",
        "`tiktoken` is a Python library from the Hugging Face team that counts tokens in text datasets efficiently, without the need to tokenize the data explicitly. It's like a \"peek\" into how many tokens are in a dataset without doing the heavy lifting of full tokenization.\n",
        "\n",
        "### **2. How does it work?**\n",
        "Instead of performing full tokenization, `tiktoken` reads the dataset and the tokenizer's vocabulary to estimate the token count. By only looking at subwords or characters that would form tokens without fully tokenizing them, it can quickly give an estimate.\n",
        "\n",
        "### **3. Why use tiktoken?**\n",
        "- **Efficiency**: Counting tokens for large datasets can be computationally expensive and slow. `tiktoken` offers a faster way.\n",
        "  \n",
        "- **Flexibility**: It works with various tokenization methods and models, especially those from the `transformers` library.\n",
        "  \n",
        "- **No Full Tokenization**: The library provides a token count without the overhead of generating tokens, making it lightweight and quick.\n",
        "\n",
        "### **4. Example of using tiktoken:**\n",
        "\n",
        "Let's say you want to count how many tokens are in a text dataset using BERT's tokenizer.\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "from tiktoken import Tokenizer, TokenCountReader\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Use tiktoken's Tokenizer class\n",
        "tiktoken = Tokenizer(tokenizer.get_vocab())\n",
        "\n",
        "# Example dataset\n",
        "text_data = [\"Hello, world!\", \"How's tiktoken working for you?\"]\n",
        "\n",
        "# Count tokens\n",
        "token_counts = sum(tiktoken.count_tokens(text) for text in text_data)\n",
        "\n",
        "print(f\"Total tokens: {token_counts}\")\n",
        "```\n",
        "\n",
        "This will give you the total token count for the text data without explicitly tokenizing it.\n",
        "\n",
        "### **Conclusion:**\n",
        "`tiktoken` offers an efficient and lightweight way to count tokens in large datasets. For practitioners who need to understand the token count for resource allocation, modeling considerations, or dataset analysis, it's a handy tool to have in the toolkit."
      ],
      "metadata": {
        "id": "IU-4Rze6BM0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Mini-batches and GPU's`\n",
        "\n",
        "Let's break down the concept of mini-batches and why they're particularly efficient for parallel processing on GPUs.\n",
        "\n",
        "### **1. The Basics: Training a Neural Network**\n",
        "Imagine you're trying to teach a robot to recognize pictures of cats. You have a big photo album, with thousands of pictures. You could show the robot each picture, one by one, and correct it every time it's wrong. This is akin to training a neural network with one example at a time, which we call **stochastic gradient descent**.\n",
        "\n",
        "### **2. The Problem with One-by-One:**\n",
        "If you teach the robot one picture at a time, it might focus too much on the specifics of the last picture it saw. It could get caught up on one unusual cat picture and forget the general features of most cats. This can lead to a lack of consistency in learning.\n",
        "\n",
        "### **3. Enter Mini-batches:**\n",
        "Instead of showing the robot one picture at a time, what if you showed it a small group of pictures, say 32, all at once? After looking at this small batch, you'd then correct its understanding. This group of pictures is what we call a **mini-batch**. By learning from a mini-batch, the robot doesn't get too caught up on any single picture but learns more general features from a set of pictures.\n",
        "\n",
        "### **4. Parallel Processing and GPUs:**\n",
        "Now, think of a GPU as a super-smart version of the robot with many eyes. Instead of looking at one picture with one eye, it can look at all 32 pictures from the mini-batch simultaneously with its many eyes. This is because GPUs are designed to handle many tasks at once, a feature called **parallel processing**.\n",
        "\n",
        "### **5. Efficiency Gains:**\n",
        "When you use a GPU, processing a mini-batch becomes much faster than processing each picture individually. It's like having a team of robots all working together at the same time, each analyzing a different picture from the batch.\n",
        "\n",
        "### **6. Balancing Act:**\n",
        "However, there's a balance to strike. If your mini-batch is too big, you might run out of memory on your GPU, just like how a team of robots might run out of table space if given too many pictures. On the other hand, if the mini-batch is too small, you might not be maximizing the GPU's capabilities.\n",
        "\n",
        "### **In Summary:**\n",
        "Mini-batches are like small groups of pictures that we use to teach a robot (or neural network) more efficiently. And GPUs, with their parallel processing capabilities, can look at all pictures in a mini-batch at once, speeding up the learning process. It's a combination of consistent learning with the power of parallel processing."
      ],
      "metadata": {
        "id": "S2kUQpQtMmny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## macOS and MLS\n",
        "\n",
        "The line you provided:\n",
        "\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "\n",
        "determines which device to use when training ML models using PyTorch. It checks if CUDA (NVIDIA's parallel computing platform) is available, which means it's checking for an NVIDIA GPU. If one isn't available, it defaults to using the CPU.\n",
        "\n",
        "On macOS, you typically won't have an NVIDIA GPU, but with the introduction of Apple's Metal and the Apple Silicon M1 chips, there's potential for GPU acceleration using Metal. The term 'mls' usually refers to the Metal-based version of PyTorch, which allows for GPU acceleration on Macs.\n",
        "\n",
        "### Step-by-step Adjustment:\n",
        "\n",
        "#### 1. **Checking for Metal support**:\n",
        "First, ensure that you're using a version of PyTorch that supports Metal. As of my last update in September 2021, there was ongoing work to integrate Metal support into PyTorch, but it wasn't in the main distribution yet. You might need a specialized version or branch of PyTorch.\n",
        "\n",
        "#### 2. **Modify the Device Line**:\n",
        "If you're confident in Metal's support in your PyTorch version, you can modify the device assignment line as follows:\n",
        "\n",
        "```python\n",
        "device = 'mls' if torch.metal.is_available() else 'cpu'\n",
        "```\n",
        "\n",
        "However, remember that `torch.metal.is_available()` is a hypothetical function. The actual function to check for Metal's availability might be named differently, based on the PyTorch version you have. Always refer to the documentation or the specific version's details.\n",
        "\n",
        "#### 3. **Install necessary dependencies**:\n",
        "Ensure you've installed any required libraries or dependencies to use Metal with PyTorch.\n",
        "\n",
        "#### 4. **Testing**:\n",
        "Once you've made these changes, run a small script to test whether your models are correctly training on the Metal backend.\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "- Training on Metal might offer improvements over CPU training, especially on M1 chips, but it's essential to understand that GPU acceleration's full benefits typically come from high-end dedicated GPUs.\n",
        "  \n",
        "- There could be certain limitations or differences in behavior when training on Metal compared to CUDA, so always ensure you test your models thoroughly and compare results.\n",
        "\n",
        "In summary, while adjusting for Metal support can provide performance benefits on macOS, always ensure compatibility, thorough testing, and keep abreast of developments in PyTorch's support for Metal."
      ],
      "metadata": {
        "id": "gqbpyl5FXuuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `torch.stack`\n",
        "\n",
        "### **1. Starting with the Basics:**\n",
        "Imagine you have a collection of individual sheets of paper, each with a different drawing on it. You want to compile these drawings into a neat, organized stack.\n",
        "\n",
        "### **2. Stacking:**\n",
        "What you'd naturally do is place one sheet over the other, ensuring they're aligned, forming a neat pile. In PyTorch, this action is what `torch.stack()` does to tensors (which you can think of as multi-dimensional arrays or, in our analogy, sheets of paper).\n",
        "\n",
        "### **3. Dimensionality:**\n",
        "In the real world, stacking papers adds a new dimension: the height of the stack. Similarly, when you stack tensors using `torch.stack()`, you add a new dimension to the tensors. This new dimension indicates the position of each tensor in the stack.\n",
        "\n",
        "### **4. An Example:**\n",
        "Let's say you have two tensors (or sheets), `A` and `B`, each of size `(3, 3)`, representing 3x3 drawings.\n",
        "\n",
        "If you stack them using `torch.stack([A, B])`, you'll get a new tensor of size `(2, 3, 3)`. The first dimension (of size 2) indicates the position in the stack (either the `A` sheet or the `B` sheet), and the other dimensions represent the content of the sheets.\n",
        "\n",
        "### **5. Control Over Stacking:**\n",
        "By default, `torch.stack()` adds this new dimension at the beginning, but you can control where to add it. Using an optional argument, you can decide the \"height\" (or depth) at which you want to stack the tensors.\n",
        "\n",
        "### **In Summary:**\n",
        "`torch.stack()` is like stacking sheets of paper on top of each other, creating a neat pile. In the world of tensors, it combines multiple tensors by adding a new dimension, essentially indicating the position of each tensor in the new stack. It's a way to organize and group tensors together in a specific order."
      ],
      "metadata": {
        "id": "IeS2f34wNHIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `nn.Module`\n",
        "\n",
        "### **1. Starting with the Basics:**\n",
        "Imagine you're constructing a building. Instead of starting from scratch each time, you'd ideally have a blueprint or a foundation that guides your construction process, ensuring consistency, and making the process more efficient.\n",
        "\n",
        "### **2. What is `nn.Module`:**\n",
        "In PyTorch, `nn.Module` acts as that foundational blueprint for all neural network modules - be it a single layer, a block of layers, or even your entire model. It provides essential functionalities, ensuring a consistent interface for training, inference, and more.\n",
        "\n",
        "### **3. Key Features:**\n",
        "\n",
        "- **Parameters Management:** Any attribute of a subclass that's of type `nn.Parameter` is automatically recognized as a trainable parameter by `nn.Module`. This means, when you're training a model, PyTorch knows which values need updating.\n",
        "  \n",
        "- **Sub-modules Handling:** Modules can contain other modules, allowing for a nested structure. This is useful for complex architectures where you might have blocks of layers doing specific operations.\n",
        "  \n",
        "- **Utilities:** It provides methods like `.to(device)`, which you can use to move your entire model to a GPU, and `.eval()`, to set your model to evaluation mode.\n",
        "\n",
        "### **4. An Example:**\n",
        "Let's say you're building a simple neural network with one hidden layer.\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "In the above code:\n",
        "- We're defining our network as a subclass of `nn.Module`.\n",
        "- The `__init__` method initializes two linear layers.\n",
        "- The `forward` method defines how data flows through the network.\n",
        "\n",
        "### **5. Why it's Essential:**\n",
        "Using `nn.Module`:\n",
        "- **Organizes your code:** Encourages a clean and modular structure.\n",
        "- **Eases Training:** With built-in functionalities, the training process becomes more straightforward.\n",
        "- **Facilitates Extensibility:** When building complex models, or using pre-existing ones, extending them becomes seamless.\n",
        "\n",
        "### **In Summary:**\n",
        "`nn.Module` is like the master blueprint for creating neural network structures in PyTorch. By subclassing `nn.Module`, you're setting the foundation to build, train, and evaluate your model, making the entire machine learning process more structured and efficient."
      ],
      "metadata": {
        "id": "zgjB0XfDO-fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `What are B, T, and C?`\n",
        "\n",
        "When working with sequences in deep learning, especially with PyTorch, you'll frequently come across tensors with dimensions labeled as `B`, `T`, and `C`. Let's break these down step-by-step:\n",
        "\n",
        "### **1. The Basics of Tensors:**\n",
        "A tensor is essentially a multi-dimensional array. The dimensions (or axes) of a tensor help organize and represent different types of data.\n",
        "\n",
        "### **2. What B, T, and C Represent:**\n",
        "\n",
        "- **B (Batch Size):**\n",
        "  - Represents the number of samples in a batch.\n",
        "  - In deep learning, instead of processing one data point at a time, we process a batch of data points simultaneously. This is especially useful for parallel processing, like on GPUs.\n",
        "\n",
        "- **T (Sequence Length / Time Steps):**\n",
        "  - Represents the length of a sequence.\n",
        "  - In tasks like language modeling or time series forecasting, data comes in sequences (e.g., words in a sentence or stock prices over days). `T` captures the length of these sequences.\n",
        "\n",
        "- **C (Channel / Feature Dimension):**\n",
        "  - Represents the number of channels or features for each time step.\n",
        "  - For example, in image processing, a color image has 3 channels (Red, Green, Blue). In natural language processing, this could be the embedding size of a word.\n",
        "\n",
        "### **3. Practical Examples:**\n",
        "\n",
        "- **For a Sentence:**\n",
        "  - Let's say you're processing sentences, and you have a batch of 32 sentences (`B=32`).\n",
        "  - Each sentence is represented as a sequence of 10 words (`T=10`).\n",
        "  - Every word is represented by a 300-dimensional embedding (`C=300`).\n",
        "  - The tensor representing this data will have a shape: `[32, 10, 300]`.\n",
        "\n",
        "- **For an Image:**\n",
        "  - You have a batch of 64 images (`B=64`).\n",
        "  - Each image is not a sequence but has a height and width.\n",
        "  - Each pixel has 3 color channels (`C=3` for RGB).\n",
        "  - If the height and width aren't sequenced, then `C` would come before the spatial dimensions.\n",
        "\n",
        "### **4. Why This Convention?**\n",
        "\n",
        "Using `B`, `T`, and `C` provides a standardized way to think about and organize data. Whether you're dealing with sequences like sentences, time series, or even images, this convention helps ensure that the data is processed correctly, especially when feeding it into neural networks.\n",
        "\n",
        "### **5. Interactions with Neural Networks:**\n",
        "\n",
        "When building models, especially recurrent neural networks (RNNs) or transformers, understanding the `B`, `T`, and `C` dimensions is crucial. For instance, RNNs process data with the time dimension `T` in mind, iterating over each time step.\n",
        "\n",
        "### **In Summary:**\n",
        "\n",
        "When you see `B`, `T`, and `C` in PyTorch, think of them as placeholders for organizing your data: `B` for batching multiple samples, `T` for sequences or time steps, and `C` for the features or channels at each step. This consistent structure ensures that when building or understanding models, especially sequence-based ones, you have a clear picture of the data's layout."
      ],
      "metadata": {
        "id": "5uPv5lfuPlRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `nn.Embedding`\n",
        "\n",
        "\n",
        "### **1. Introduction to Embeddings:**\n",
        "- At its core, an embedding is a mapping from discrete objects (like words or item IDs) to vectors of continuous values. This allows algorithms to work with them in a mathematical way, capturing relationships between the objects.\n",
        "\n",
        "### **2. Why We Need Embeddings:**\n",
        "- In many machine learning tasks, especially in NLP, we deal with categorical data like words or characters. Directly feeding them into models isn't efficient because they're symbolic, not numerical.\n",
        "- For example, representing the word \"apple\" as [0, 1, 0, 0, ...] and \"orange\" as [0, 0, 1, 0, ...] in a one-hot encoded vector doesn't tell us about the relationship between \"apple\" and \"orange\". Both vectors are orthogonal in high-dimensional space.\n",
        "\n",
        "### **3. What nn.Embedding Does:**\n",
        "- `nn.Embedding` in PyTorch is a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "- Given an index (or indices), it fetches the embedding for this index from the table.\n",
        "- It's like having a dictionary where you look up the vector for a word.\n",
        "\n",
        "### **4. Parameters of nn.Embedding:**\n",
        "- **num_embeddings:** Total number of discrete items (e.g., vocabulary size for words).\n",
        "- **embedding_dim:** Dimension of the embedding vector (e.g., 300 for a 300-dimensional vector for each word).\n",
        "\n",
        "### **5. Usage:**\n",
        "- After defining an embedding layer, when you pass an integer (or a tensor of integers) to it, you get the corresponding embedding vectors.\n",
        "- Example: If you've defined your embeddings for a vocabulary of size 10,000 to have a dimension of 300, when you pass an integer `42` to this embedding layer, you get a 300-dimensional vector representing the 42nd word in your vocabulary.\n",
        "\n",
        "### **6. Training and Learning:**\n",
        "- Initially, the embeddings might be random. But as you train your model on a task (like predicting the next word in a sentence), these embeddings adjust to capture semantic meanings of words.\n",
        "- For instance, in a well-trained model, the vector distance between \"king\" and \"queen\" might be similar to the distance between \"man\" and \"woman\", capturing gender relationships.\n",
        "\n",
        "### **7. Benefits:**\n",
        "- Embeddings reduce dimensionality. Instead of using a 10,000-dimensional one-hot vector for a vocabulary of size 10,000, you might use a 300-dimensional embedding.\n",
        "- They capture semantic relationships, as similar items will have embeddings that are closer in the vector space.\n",
        "\n",
        "### **In Summary:**\n",
        "`nn.Embedding` in PyTorch provides a way to convert discrete data like words into continuous, dense vectors. These vectors can be fed into neural networks, and during training, the embeddings adjust to capture the underlying relationships in the data. Think of it as a learnable translation layer that transforms raw categorical data into meaningful numerical representations."
      ],
      "metadata": {
        "id": "yR0Xyfn9QTcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.Embedding` usage\n",
        "\n",
        "### 1. **What is torch.nn.Embedding?**\n",
        "`torch.nn.Embedding` is a module in PyTorch that provides a simple lookup table to store embeddings of a fixed dictionary and size. In simpler terms, it converts discrete categorical data (like word indices) into continuous dense vectors, which are suitable for machine learning models.\n",
        "\n",
        "### Key Features:\n",
        "- **Size Parameters**: It takes two main parameters: `num_embeddings` (size of the dictionary) and `embedding_dim` (size of each embedding vector).\n",
        "- **Weights**: The weights of the embedding layer are learnable parameters. During training, these weights get updated to capture the semantic meaning or any other feature that the model finds useful.\n",
        "- **Padding Idx**: You can also specify a padding index. Whenever this index is encountered in the input data, the embedding layer will output a zero vector.\n",
        "\n",
        "### 2. **Why Use Embeddings?**\n",
        "Embeddings are essential for tasks involving categorical data, like natural language processing (NLP). Representing words as discrete indices isn't useful for neural networks as they thrive on continuous data. Embeddings convert these discrete indices into dense vectors that can capture semantic relationships between words or other categorical items.\n",
        "\n",
        "### 3. **Example**:\n",
        "\n",
        "Let's consider a simple example where we have a vocabulary of 5 words, and we want to represent each word with a 3-dimensional vector.\n",
        "\n",
        "``` python\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the embedding layer\n",
        "vocab_size = 5  # e.g., {\"hello\", \"world\", \"I\", \"am\", \"here\"}\n",
        "embedding_dim = 3\n",
        "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "# Get embeddings for word indices 2 (\"I\") and 4 (\"here\")\n",
        "word_indices = torch.tensor([2, 4])\n",
        "word_embeddings = embedding(word_indices)\n",
        "\n",
        "print(word_embeddings)\n",
        "# This will output two 3-dimensional vectors corresponding to the words \"I\" and \"here\"\n",
        "```\n",
        "\n",
        "Initially, the embeddings will be random. But during training, backpropagation will adjust these embeddings such that words with similar meanings or that often appear in similar contexts will have embeddings close to each other in the vector space.\n",
        "\n",
        "In NLP tasks, after training, the embedding layer becomes a rich source of word vectors, where the geometry of the vectors captures semantic relationships between words (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
        "\n",
        "### 4. **Advanced Usage**:\n",
        "- **Pre-trained Embeddings**: For many tasks, especially in NLP, researchers often use pre-trained embeddings (like Word2Vec or GloVe) to initialize the embedding layer. This gives the model a head start by using embeddings that already capture semantic meanings.\n",
        "  \n",
        "- **Freezing Embeddings**: In some cases, especially when using pre-trained embeddings, you might not want to fine-tune the embeddings further. You can \"freeze\" the embedding layer by setting `embedding.weight.requires_grad = False`, which will prevent the embeddings from being updated during training.\n",
        "\n",
        "In essence, `torch.nn.Embedding` provides a mechanism to convert discrete data into a form suitable for neural networks and allows the model to learn a dense representation of this data that captures the underlying relationships."
      ],
      "metadata": {
        "id": "AbxYPmkFrE0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional embeddings\n",
        "\n",
        "### 1. **The Need for Positional Information**:\n",
        "\n",
        "In sequences like sentences, the order of tokens is crucial for meaning. For instance, \"cat eats fish\" and \"fish eats cat\" have the same words but entirely different meanings. Traditional sequence models like RNNs and LSTMs inherently handle this by processing sequences one token at a time. However, the Transformer architecture, with its parallel processing of all tokens, lacks this inherent sense of order. This is why we introduce positional information.\n",
        "\n",
        "### 2. **Positional Embeddings**:\n",
        "\n",
        "The idea behind positional embeddings is to encode the position of each token in the sequence into a vector. This vector is then added to the token's embedding, ensuring that the model can distinguish between tokens based on their positions.\n",
        "\n",
        "### 3. **Example**:\n",
        "\n",
        "Let's consider the sentence: \"cat eats fish\".\n",
        "\n",
        "With token embeddings alone, the model might represent this as:\n",
        "\n",
        "```\n",
        "cat -> [0.1, 0.5]\n",
        "eats -> [0.3, 0.2]\n",
        "fish -> [-0.1, 0.4]\n",
        "```\n",
        "\n",
        "Now, let's introduce positional embeddings. For simplicity, let's assume our positional embeddings for positions 1, 2, and 3 are:\n",
        "\n",
        "```\n",
        "position 1 -> [0.01, 0.01]\n",
        "position 2 -> [0.02, 0.02]\n",
        "position 3 -> [0.03, 0.03]\n",
        "```\n",
        "\n",
        "When we add these positional embeddings to our token embeddings, we get:\n",
        "\n",
        "```\n",
        "cat (position 1) -> [0.1 + 0.01, 0.5 + 0.01] = [0.11, 0.51]\n",
        "eats (position 2) -> [0.3 + 0.02, 0.2 + 0.02] = [0.32, 0.22]\n",
        "fish (position 3) -> [-0.1 + 0.03, 0.4 + 0.03] = [-0.07, 0.43]\n",
        "```\n",
        "\n",
        "Now, even if the words \"cat\" and \"fish\" appear in different orders, their combined embeddings (token + position) will be different, allowing the model to distinguish between them.\n",
        "\n",
        "### 4. **Types of Positional Embeddings**:\n",
        "\n",
        "The above example uses a straightforward and static positional embedding. In practice, more complex functions can be used:\n",
        "\n",
        "- **Sinusoidal Positional Embeddings**: This is the original method proposed in the \"Attention Is All You Need\" paper. It uses sine and cosine functions of different frequencies to create positional embeddings.\n",
        "  \n",
        "  The intuition behind using sinusoidal functions is that they can allow the model to learn to attend to relative positions since for any fixed offset \\( k \\), \\( \\text{PE}_{\\text{pos}+k} \\) can be represented as a linear function of \\( \\text{PE}_{\\text{pos}} \\).\n",
        "  \n",
        "- **Learned Positional Embeddings**: Instead of using a fixed function, the embeddings for each position are initialized randomly and learned alongside the token embeddings during training.\n",
        "\n",
        "### 5. **Summation of Token and Positional Embeddings**:\n",
        "\n",
        "As seen in the example, the token and positional embeddings are summed together. This is a simple operation that effectively combines the two types of information. However, other operations, like concatenation, could be used, but summation is preferred for its simplicity and effectiveness.\n",
        "\n",
        "In conclusion, positional embeddings provide the Transformer architecture with a way to consider the order of tokens in a sequence, ensuring that it can capture the nuances and relationships that come from token order."
      ],
      "metadata": {
        "id": "KTDyKAyA0nB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch's optimizers\n",
        "\n",
        "### 1. **Understanding Optimization in Deep Learning**:\n",
        "Before we jump into PyTorch's optimizers, it's crucial to understand optimization in the context of deep learning. The primary goal in training a neural network is to minimize (or optimize) a loss function, which quantifies how far off our network's predictions are from the true values. Optimization is the process of adjusting the model's weights in a way that minimizes this loss.\n",
        "\n",
        "### 2. **What are Optimizers?**:\n",
        "Optimizers are algorithms or methods used to adjust the attributes of the neural network, such as weights and learning rate, to reduce the losses. PyTorch provides several optimization algorithms packaged into the `torch.optim` module.\n",
        "\n",
        "### 3. **Why Do We Need Optimizers?**:\n",
        "- **Navigating High-dimensional Spaces**: Neural networks, especially deep ones, have a vast number of weights. Optimizers help navigate this high-dimensional space to find a set of weights that results in the lowest loss.\n",
        "- **Escaping Local Minima/Plateaus**: The loss landscape can have multiple regions where the loss is minimal (local minima) or doesn't change much (plateaus). Advanced optimizers help networks escape or avoid getting stuck in these regions.\n",
        "- **Efficiency**: Some optimization algorithms converge faster than others, meaning the network reaches a low loss more quickly, saving both time and computational resources.\n",
        "\n",
        "### 4. **Why Do We Use Them?**:\n",
        "- **Versatility**: Different problems might benefit from different optimization strategies. PyTorch offers a variety of optimizers, allowing users to choose the best one for their specific problem.\n",
        "- **Adaptive Learning Rates**: Some optimizers can adjust the learning rate during training, which can lead to faster convergence and better performance.\n",
        "- **Momentum and Acceleration**: Some optimizers use concepts like momentum (considering the previous gradient) to avoid oscillations and accelerate convergence.\n",
        "\n",
        "### 5. **Functionality Provided by PyTorch's Optimizers**:\n",
        "- **Update Rules**: Each optimizer implements a specific update rule, i.e., how it adjusts the model's weights based on the computed gradients.\n",
        "- **Learning Rate Scheduling**: Many optimizers allow for adjusting the learning rate during training, either reducing it according to a schedule or adapting it based on recent weight updates.\n",
        "- **Weight Regularization**: Some optimizers support weight decay, which is a form of L2 regularization. This can help prevent overfitting.\n",
        "- **Storing/Maintaining Internal States**: For algorithms that consider past gradients (like Adam or RMSprop), PyTorch optimizers maintain and update internal states.\n",
        "\n",
        "### 6. **Popular Optimizers in PyTorch**:\n",
        "- **SGD (Stochastic Gradient Descent)**: Traditional method where each parameter is updated using the gradient of the loss with respect to that parameter.\n",
        "- **Adam**: Combines the benefits of two extensions of SGD - AdaGrad and RMSProp. It maintains a per-parameter learning rate that's adjusted individually for each parameter.\n",
        "- **RMSprop**: Maintains a moving average of the squared gradient for each weight, which is used to normalize the gradient before its used in the weight update.\n",
        "- **Adagrad**: Adapts the learning rates of all model parameters, giving lower rates for parameters associated with frequently occurring features.\n",
        "\n",
        "### 7. **Using PyTorch Optimizers**:\n",
        "Using an optimizer in PyTorch typically involves:\n",
        "1. Initializing the optimizer with the model's parameters and setting the learning rate.\n",
        "2. During training, after computing gradients using `backward()`, calling the optimizer's `step()` method to update the model's weights.\n",
        "\n",
        "In essence, optimizers in PyTorch (and deep learning in general) play a pivotal role in determining how weights are updated during training. The choice of optimizer and its parameters can significantly impact the efficiency of training and the final performance of a model."
      ],
      "metadata": {
        "id": "xuEDzSptOdaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `torch.optim` and `torch.optim.AdamW`\n",
        "\n",
        "### **1. The Need for Optimizers:**\n",
        "- Training a neural network involves adjusting its weights to reduce a cost function. This \"adjustment\" is essentially an optimization problem.\n",
        "- The optimizer decides how the weights of the network should be updated based on the gradient of the loss function.\n",
        "\n",
        "### **2. What is torch.optim?**\n",
        "- `torch.optim` is a module in PyTorch that provides implementations of various optimization algorithms, which are used to update the weights of neural networks during training.\n",
        "- Each optimizer in `torch.optim` offers a different approach to weight updates.\n",
        "\n",
        "### **3. Common Optimizers:**\n",
        "- **SGD (Stochastic Gradient Descent):** Updates weights using a fraction of the dataset.\n",
        "- **Momentum:** Considers the previous gradient direction to make updates, providing a kind of memory to the optimizer.\n",
        "- **RMSprop, Adam, etc.:** Adaptive methods that adjust learning rates based on the recent magnitudes of the gradients.\n",
        "\n",
        "### **4. Introduction to AdamW:**\n",
        "- `torch.optim.AdamW` is a variant of the Adam optimizer.\n",
        "- AdamW decouples weight decay from the optimization steps, which is believed to provide better regularization.\n",
        "  \n",
        "### **5. What Makes AdamW Special?**\n",
        "- Traditional weight decay (also known as L2 regularization) can be detrimental when used with adaptive gradient methods like Adam. This is because the regularization term gets intertwined with the gradient-based weight update.\n",
        "- AdamW separates the weight decay from the optimization step, ensuring that only the 'pure' gradient influences the adaptive learning rate.\n",
        "- This decoupling often results in better training performance and generalization.\n",
        "\n",
        "### **6. Parameters Specific to AdamW:**\n",
        "- **betas:** Coefficients used for computing running averages of the gradient and its square.\n",
        "- **eps:** A small number to prevent any division by zero in the implementation (usually a very small value).\n",
        "- **weight_decay:** Regularization term. Represents the rate at which the weights decay over iterations.\n",
        "\n",
        "### **7. Using AdamW in PyTorch:**\n",
        "After defining your model in PyTorch, you can set up the AdamW optimizer as:\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "```\n",
        "\n",
        "During training, after computing gradients, you'd use the optimizer to update the model's weights:\n",
        "\n",
        "```python\n",
        "loss.backward()  # Compute gradients\n",
        "optimizer.step() # Update weights using AdamW\n",
        "```\n",
        "\n",
        "### **In Summary:**\n",
        "`torch.optim` offers a suite of optimization algorithms to train neural networks in PyTorch. Among them, `torch.optim.AdamW` is a variant of the Adam optimizer that decouples weight decay from the optimization steps, often resulting in better training outcomes. Think of it as a refined tool in the toolbox that might offer better performance under certain conditions, especially when weight decay is involved."
      ],
      "metadata": {
        "id": "Kd6yRnQOVNBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `model.eval()`, `model.train()` and `torch.no_grad()`\n",
        "\n",
        "### **1. Model Modes in PyTorch:**\n",
        "- PyTorch models have two modes: **training mode** and **evaluation mode**.\n",
        "- These modes dictate how certain layers in the network operate, especially layers like dropout and batch normalization which behave differently during training and inference.\n",
        "\n",
        "### **2. Why Two Modes?**\n",
        "- **Training Mode:** During training, we want our model to learn and possibly benefit from certain regularizing layers like dropout. For example, dropout randomly sets a fraction of input units to 0 to prevent overfitting.\n",
        "- **Evaluation Mode:** When we evaluate or deploy the model, we want it to use its learned knowledge without any randomness. Here, dropout should not drop any units; batch normalization should use running statistics rather than batch-specific ones.\n",
        "\n",
        "### **3. What does `model.eval()` do?**\n",
        "- When you call `model.eval()`, you're setting the model to evaluation mode.\n",
        "- In this mode, layers like dropout won't drop activations, and batch normalization will use the running mean/variance instead of batch statistics.\n",
        "\n",
        "### **4. Importance of Switching Modes:**\n",
        "- If you forget to switch to evaluation mode when evaluating your model, the performance might be inconsistent due to the randomness introduced by layers like dropout.\n",
        "- Similarly, if you forget to switch back to training mode (`model.train()`) before resuming training, the model won't train as expected.\n",
        "\n",
        "### **5. Typical Usage in a Training Loop:**\n",
        "While training and evaluating a neural network model, it's common to see the following pattern:\n",
        "\n",
        "```python\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Switch to training mode\n",
        "    for batch in train_dataloader:\n",
        "        # Training code here...\n",
        "\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    with torch.no_grad():  # Turn off gradient computation\n",
        "        for batch in val_dataloader:\n",
        "            # Evaluation code here...\n",
        "```\n",
        "\n",
        "### **6. The `torch.no_grad()` Context:**\n",
        "- While in evaluation mode, it's a good practice to wrap the evaluation code inside the `torch.no_grad()` context to prevent unnecessary gradient computation, which saves memory and computational resources.\n",
        "\n",
        "### **In Summary:**\n",
        "`model.eval()` is a method in PyTorch that sets your model to evaluation mode. This is crucial when testing the model's performance or deploying it, as it ensures that the model gives deterministic outputs. Layers like dropout and batch normalization, which have different behaviors during training and evaluation, are the primary reasons for this mode switch. Always remember to toggle between `model.train()` and `model.eval()` appropriately to ensure your model operates correctly during both training and evaluation phases."
      ],
      "metadata": {
        "id": "FgYI2vAmaWyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The self-attention mechanism.\n",
        "\n",
        "We are focusing on the Transformer architecture, which is where this mechanism shines:\n",
        "\n",
        "### **1. The Idea of Attention:**\n",
        "At its core, attention is about weighing the importance of different inputs when producing an output. Imagine reading a sentence and emphasizing words that are more relevant to understanding the meaning. That's what the attention mechanism tries to replicate.\n",
        "\n",
        "### **2. Self-Attention:**\n",
        "Self-attention refers to the model attending to different words within the same input. For instance, in the sentence \"The cat, which already ate ..., was full,\" the word \"was\" is more closely related to \"cat\" than \"ate.\" Self-attention captures these relationships.\n",
        "\n",
        "### **3. The Mathematical Trick:**\n",
        "\n",
        "#### a) Linear Projections:\n",
        "For each word (or token) in the input, we create three vectors:\n",
        "- **Query (Q)**: Represents the word in question.\n",
        "- **Key (K)**: Represents other words we want to attend to.\n",
        "- **Value (V)**: Contains the information from the other words that we'll weigh.\n",
        "\n",
        "These vectors are obtained by multiplying the input embeddings with learned weights (linear projection).\n",
        "\n",
        "#### b) Calculating Attention Scores:\n",
        "For a given word's Query vector, we compute a score with every other word's Key vector. This is done using the dot product:\n",
        "$$ \\text{Score} = Q \\cdot K^T $$\n",
        "\n",
        "#### c) Softmax Scaling:\n",
        "The scores are divided by the square root of the dimension of the Key vectors (usually denoted as $ d_k $). This scaling makes the model more stable. Then, a softmax is applied, ensuring the scores are between 0 and 1 and sum up to 1.\n",
        "\n",
        "#### d) Weighted Sum:\n",
        "Using the softmax scores, we take a weighted sum of the Value vectors. This gives us a new representation of the word, emphasizing words it should \"attend\" to.\n",
        "\n",
        "### **4. The Intuition:**\n",
        "The dot product in the attention score determines how similar our word (Query) is to other words (Keys). The softmax ensures one word doesn't overly dominate the attention. The weighted sum of Values gives a new representation based on contextual relationships.\n",
        "\n",
        "### **5. Why is this a 'Trick'?**\n",
        "Traditional RNN-based architectures process sentences sequentially, capturing context in a cumulative manner. The self-attention mechanism, on the other hand, captures relationships between all words simultaneously, irrespective of their distance from each other in the sentence. This parallelism is computationally efficient and is one of the reasons Transformers are so powerful.\n",
        "\n",
        "### **In Summary:**\n",
        "The 'mathematical trick' in self-attention allows models like Transformers to weigh the importance of different parts of the input when producing an output, capturing intricate relationships between words irrespective of their positions. This parallel processing ability, combined with the power of attention, has made Transformers the state-of-the-art in many NLP tasks."
      ],
      "metadata": {
        "id": "kGA6PjlUbBLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `bag of words` (often abbreviated as `BoW`):\n",
        "\n",
        "### **1. The Basic Idea:**\n",
        "Imagine you have a basket (or \"bag\"), and every time you read a word in a document, you drop a token of that word into the basket. At the end, you don't care about the order in which the words appeared in the document; you just care about the words themselves and their frequency.\n",
        "\n",
        "### **2. Representation:**\n",
        "The BoW model represents text as a vector where each position corresponds to a unique word in the entire dataset (often called the vocabulary). The value at each position is the frequency of that word in the given text.\n",
        "\n",
        "### **3. Example:**\n",
        "Let's say our entire vocabulary is just three words: [\"apple\", \"banana\", \"cherry\"].\n",
        "For the sentence \"apple banana apple\", the BoW representation would be [2, 1, 0], since \"apple\" appears twice, \"banana\" once, and \"cherry\" not at all.\n",
        "\n",
        "### **4. Advantages:**\n",
        "- **Simplicity**: The BoW model is straightforward and easy to understand.\n",
        "- **Efficiency**: Since it's just counting words, it's computationally efficient.\n",
        "- **Effective for Many Tasks**: Despite its simplicity, BoW can be surprisingly effective for various NLP tasks, especially when combined with other techniques.\n",
        "\n",
        "### **5. Limitations:**\n",
        "- **Loss of Order**: BoW ignores the order of words, so \"dog bites man\" and \"man bites dog\" would have the same representation.\n",
        "- **Sparse Representations**: If the vocabulary is large, which is often the case, the BoW vectors will have lots of zeros, leading to memory inefficiencies.\n",
        "- **No Semantic Understanding**: BoW can't capture nuances or meanings of words. For instance, it wouldn't understand synonyms (\"happy\" and \"joyful\").\n",
        "\n",
        "### **6. Variants & Enhancements:**\n",
        "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Instead of raw counts, words are weighed by their importance in the document relative to the entire dataset.\n",
        "- **Bigrams, Trigrams, and n-grams**: Instead of just individual words, consecutive sequences of 2 (bigrams), 3 (trigrams), or more words can be considered.\n",
        "\n",
        "### **In Summary:**\n",
        "\"Bag of Words\" is a foundational technique in NLP that represents text as vectors based on word counts, disregarding the order. While it has limitations, especially in capturing semantic meanings or word orders, its simplicity and efficiency make it a popular starting point for many text analysis tasks."
      ],
      "metadata": {
        "id": "CAJ2tIkddkZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `The 'mathematical trick' in self-attention`\n",
        "\n",
        "### **1. Introduction to Self-Attention:**\n",
        "- The lecturer starts by introducing the idea of the self-attention block in the context of processing tokens. The essence of self-attention is allowing tokens to \"communicate\" or \"interact\" with each other.\n",
        "  \n",
        "### **2. Need for Efficient Implementation:**\n",
        "- There's a mathematical trick at the heart of an efficient implementation of self-attention, which the lecturer wants the listeners to understand before diving deep into the actual self-attention mechanism.\n",
        "\n",
        "### **3. Toy Example to Illustrate the Trick:**\n",
        "- A toy example with a tensor of shape `B x T x C` (Batch, Time, Channels) is introduced.\n",
        "- The goal is to make tokens communicate with each other. However, there's a catch: a token should not communicate with future tokens (for sequence-based tasks).\n",
        "  \n",
        "### **4. Weighted Averaging of Tokens:**\n",
        "- One way to make tokens \"talk\" to each other is by averaging them. This means a token at a certain position would consider the information from all preceding tokens (and itself) to form a new representation.\n",
        "- This averaging is a simple form of communication but is lossy as it lacks detailed interactions.\n",
        "\n",
        "### **5. Matrix Multiplication as Weighted Aggregation:**\n",
        "- The lecturer introduces the core trick: using matrix multiplication to achieve this weighted aggregation of tokens.\n",
        "- With the use of matrices, one can efficiently compute the weighted sum or average of tokens.\n",
        "- The `torch.tril()` function is introduced, which produces a lower triangular matrix. This matrix is crucial for ensuring tokens don't communicate with future tokens.\n",
        "  \n",
        "### **6. Softmax for Normalization:**\n",
        "- The lecturer then moves to the idea of using softmax for normalization. By using softmax, the weights can be made to sum up to 1, effectively turning the weights into probabilities.\n",
        "- The intuition here is that these probabilities will define how much attention a token should pay to other tokens.\n",
        "  \n",
        "### **7. Data-Dependent Affinities:**\n",
        "- The zeros in the weight matrix (used for averaging) are not always constant. In a more advanced setting, these weights are data-dependent. This means tokens will have varying levels of interest in other tokens based on the data.\n",
        "- The idea is that some tokens might find certain other tokens more relevant or interesting than others, and this affinity will be learned from the data.\n",
        "  \n",
        "### **8. Conclusion:**\n",
        "- The final takeaway is the power of matrix multiplication, specifically in a lower triangular fashion, to efficiently compute weighted aggregations of tokens. This trick forms the basis of the self-attention mechanism, where tokens can attend to or focus on other tokens based on learned affinities.\n",
        "\n",
        "In summary, the lecturer is setting the stage for introducing the self-attention mechanism in deep learning by first explaining the fundamental mathematical trick that allows for efficient computation. This trick involves using matrix multiplication for weighted aggregation, where the weights represent how much attention one token pays to others. The use of softmax ensures these weights are normalized, and the actual values of the weights can be data-dependent, allowing the model to learn intricate interactions between tokens."
      ],
      "metadata": {
        "id": "WgIY9sXbkx9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch's `torch.tril()`\n",
        "\n",
        "### **1. Basic Idea:**\n",
        "The function name `tril` is short for \"triangular lower\". It's used to extract the lower triangle of a matrix, setting all the elements above the diagonal to zero.\n",
        "\n",
        "### **2. Parameters:**\n",
        "`torch.tril(input, diagonal=0)`\n",
        "\n",
        "- `input`: The input tensor (typically a 2D matrix).\n",
        "- `diagonal`: Starting from the main diagonal, you can specify which diagonal to consider as the \"new main diagonal\". If `diagonal = 0`, it uses the main diagonal. If `diagonal = 1`, it includes one diagonal above the main, and so forth. Similarly, negative values consider diagonals below the main.\n",
        "\n",
        "### **3. Example:**\n",
        "\n",
        "Consider the matrix:\n",
        "$$\n",
        "M = \\begin{pmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Using `torch.tril(M)`:\n",
        "\n",
        "$$\n",
        "\\text{Result} = \\begin{pmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "4 & 5 & 0 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Here, all the values above the main diagonal are set to zero.\n",
        "\n",
        "If we were to use `torch.tril(M, diagonal=1)`:\n",
        "\n",
        "$$\n",
        "\\text{Result} = \\begin{pmatrix}\n",
        "1 & 2 & 0 \\\\\n",
        "4 & 5 & 6 \\\\\n",
        "7 & 8 & 9 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Now, it includes one diagonal above the main, but still zeros out anything above that.\n",
        "\n",
        "### **4. Practical Use Cases:**\n",
        "- **Masking**: In deep learning, especially in architectures like transformers, you often want to mask out certain values, especially in the self-attention mechanism to avoid \"looking ahead\" in sequences. `torch.tril()` can be used to create such masks.\n",
        "- **Matrix Computations**: In linear algebra, sometimes you only care about the lower triangular portion of a matrix, especially in factorizations.\n",
        "\n",
        "### **5. In Summary:**\n",
        "`torch.tril()` is a handy PyTorch function to extract the lower triangular part of a matrix. It's useful in various deep learning scenarios, especially when you want to create masks or work with specific portions of matrices."
      ],
      "metadata": {
        "id": "WHJl87HuklxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## `torch.nn.Linear`\n",
        "\n",
        "Certainly! Let's break down `torch.nn.Linear`.\n",
        "\n",
        "### What is `torch.nn.Linear`?\n",
        "\n",
        "`torch.nn.Linear` is a module in PyTorch that applies a linear transformation to the incoming data. In essence, it's a basic feed-forward layer in neural networks.\n",
        "\n",
        "Mathematically, if \\( x \\) is the input, the transformation it applies is:\n",
        "\\[ y = xA^T + b \\]\n",
        "Where:\n",
        "- \\( A \\) is the weight matrix.\n",
        "- \\( b \\) is the bias vector.\n",
        "- \\( A^T \\) denotes the transpose of matrix \\( A \\).\n",
        "\n",
        "### Parameters:\n",
        "- `in_features`: The number of input features (i.e., the size of each input sample).\n",
        "- `out_features`: The number of output features (i.e., the size of each output sample).\n",
        "- `bias`: A boolean flag that indicates whether to add a bias term. Default is `True`.\n",
        "\n",
        "### Internal Components:\n",
        "- `weight`: The weight matrix with a shape of `(out_features, in_features)`.\n",
        "- `bias`: The bias vector with a shape of `(out_features)`.\n",
        "\n",
        "### Example 1: Basic Usage\n",
        "\n",
        "Let's start with the simplest example: a single input and a single output.\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a linear layer\n",
        "linear = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "# Print the initial weights and bias\n",
        "print(linear.weight)\n",
        "print(linear.bias)\n",
        "```\n",
        "\n",
        "If you run this code, you'll see the randomly initialized weight and bias for this linear transformation.\n",
        "\n",
        "### Example 2: Transforming Data\n",
        "\n",
        "To see `torch.nn.Linear` in action, let's pass some data through it.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Sample input\n",
        "x = torch.tensor([[2.0], [3.0], [4.0]])\n",
        "\n",
        "# Pass the input through the linear layer\n",
        "y = linear(x)\n",
        "print(y)\n",
        "```\n",
        "\n",
        "In this example, for each input value, the output is the result of multiplying the input by the weight and adding the bias.\n",
        "\n",
        "### Example 3: Multi-dimensional Input and Output\n",
        "\n",
        "Now, let's consider a scenario where we have a 3-dimensional input and want a 2-dimensional output.\n",
        "\n",
        "```python\n",
        "# Define a linear layer\n",
        "linear_multi = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "# Sample input\n",
        "x_multi = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Pass the input through the linear layer\n",
        "y_multi = linear_multi(x_multi)\n",
        "print(y_multi)\n",
        "```\n",
        "\n",
        "Here, the input tensor has a shape of `(2, 3)`, indicating there are 2 samples, each with 3 features. The output has a shape of `(2, 2)` since we've defined the linear layer to produce 2-dimensional outputs.\n",
        "\n",
        "### Recap:\n",
        "\n",
        "`torch.nn.Linear` is a foundational building block in neural networks, representing a simple feed-forward layer. By stacking multiple such layers (possibly interspersed with activation functions), one can build deep feed-forward neural networks."
      ],
      "metadata": {
        "id": "KN7t1F-LmGJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a linear layer\n",
        "linear = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "# Print the initial weights and bias\n",
        "print(\"weight=\")\n",
        "print(linear.weight)\n",
        "print(\"bias=\")\n",
        "print(linear.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZGlZExFmX4N",
        "outputId": "2d16f551-f8fc-4a37-b8f5-a5b42feae422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight=\n",
            "Parameter containing:\n",
            "tensor([[-0.0368]], requires_grad=True)\n",
            "bias=\n",
            "Parameter containing:\n",
            "tensor([0.7415], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input\n",
        "x = torch.tensor([[2.0], [3.0], [4.0]])\n",
        "\n",
        "# Pass the input through the linear layer\n",
        "y = linear(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Kr_71zml18",
        "outputId": "43530bb2-10c3-4b19-a24c-3d88a50851b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6679],\n",
            "        [0.6311],\n",
            "        [0.5943]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a linear layer\n",
        "linear_multi = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "# Sample input\n",
        "x_multi = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Pass the input through the linear layer\n",
        "y_multi = linear_multi(x_multi)\n",
        "print(y_multi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO419KlGndGe",
        "outputId": "e9f6f0d7-e8bf-4aa0-b29a-3ed29a625a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5698,  0.5354],\n",
            "        [-1.8566,  1.4742]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_multi.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFxkmQgXpJsz",
        "outputId": "bfb41278-c2ab-46f9-d0ce-810d027d31b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.4159,  0.1278, -0.1408],\n",
              "        [ 0.0934, -0.2744,  0.4940]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## `torch.nn.Linear` in action\n",
        "\n",
        "### Step 1: Define the Linear Layer\n",
        "\n",
        "```python\n",
        "linear_multi = nn.Linear(in_features=3, out_features=2)\n",
        "```\n",
        "\n",
        "Here, we are defining a linear transformation layer that takes an input with 3 features and produces an output with 2 features.\n",
        "\n",
        "- The `in_features=3` parameter specifies that the input will have 3 features.\n",
        "- The `out_features=2` parameter specifies that the output will have 2 features.\n",
        "\n",
        "Internally, the `linear_multi` layer now has:\n",
        "\n",
        "1. A weight matrix of shape `(2, 3)`. This means there are 2 rows (one for each output feature) and 3 columns (one for each input feature). Each element of this matrix is a weight that defines the strength of the connection between an input feature and an output feature.\n",
        "2. A bias vector of shape `(2)`. Each element of this vector is added to one of the output features.\n",
        "\n",
        "### Step 2: Sample Input\n",
        "\n",
        "```python\n",
        "x_multi = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "```\n",
        "\n",
        "Here, we've created a sample input tensor with shape `(2, 3)`:\n",
        "\n",
        "- 2 samples (or rows).\n",
        "- Each sample has 3 features (or columns).\n",
        "\n",
        "Visualizing `x_multi`:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "1.0 & 2.0 & 3.0 \\\\\n",
        "4.0 & 5.0 & 6.0 \\\\\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "### Step 3: Linear Transformation\n",
        "\n",
        "```python\n",
        "y_multi = linear_multi(x_multi)\n",
        "```\n",
        "\n",
        "Here's where the magic happens. The input `x_multi` is passed through the linear layer, undergoing the transformation:\n",
        "\n",
        "$$\n",
        "y_{\\text{multi}} = x_{\\text{multi}} \\times \\text{weight}^T + \\text{bias}\n",
        "$$\n",
        "\n",
        "- `weight^T` is the transpose of the weight matrix.\n",
        "- `bias` is added to the result of the matrix multiplication.\n",
        "\n",
        "To get a clearer sense:\n",
        "\n",
        "Imagine the weight matrix (randomly initialized) looks something like this:\n",
        "\n",
        "$$\n",
        "\\text{weight} = \\begin{bmatrix}\n",
        "w_{11} & w_{12} & w_{13} \\\\\n",
        "w_{21} & w_{22} & w_{23} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "And the bias vector:\n",
        "\n",
        "$$\n",
        "\\text{bias} = \\begin{bmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The transformed output `y_multi` for the first sample would be:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "1.0 \\times w_{11} + 2.0 \\times w_{12} + 3.0 \\times w_{13} + b_1 \\\\\n",
        "1.0 \\times w_{21} + 2.0 \\times w_{22} + 3.0 \\times w_{23} + b_2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "The transformation for the second sample would be similar, using its feature values.\n",
        "\n",
        "### Step 4: Printing the Output\n",
        "\n",
        "```python\n",
        "print(y_multi)\n",
        "```\n",
        "\n",
        "This will show the 2x2 transformed output. Each row corresponds to one of the input samples, and each column corresponds to one of the output features.\n",
        "\n",
        "In essence, what the linear layer has done is project the 3-dimensional input data into a 2-dimensional space using the weight matrix and then shifted it using the bias vector."
      ],
      "metadata": {
        "id": "MCfucTASqD2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Cosine Similarity and Embeddings\n",
        "\n",
        "The operation inside the `nn.Linear` layer is a matrix multiplication (which involves many dot products), but it's not just a single dot product operation. To showcase the use of a dot product in neural networks, let's consider the concept of cosine similarity in the context of embeddings.\n",
        "\n",
        "### Cosine Similarity and Embeddings\n",
        "\n",
        "Embeddings are representations of items (words, users, products, etc.) in a dense vector format. These embeddings can capture semantic relationships, and one way to measure the similarity between two embeddings is to use cosine similarity, which is based on the dot product.\n",
        "\n",
        "Cosine similarity between two vectors \\( A \\) and \\( B \\) is given by:\n",
        "\n",
        "$$\n",
        "\\text{cosine similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( $A \\cdot B $\\) is the dot product of the two vectors.\n",
        "- \\( \\|A\\| \\) and \\( \\|B\\| \\) are the magnitudes (norms) of the vectors.\n",
        "\n",
        "Let's break this down step-by-step:\n",
        "\n",
        "### Step 1: Define Two Embeddings\n",
        "\n",
        "Let's consider word embeddings for the sake of this example. Suppose we have embeddings for the words \"king\" and \"queen\".\n",
        "\n",
        "```python\n",
        "# Sample embeddings (randomly initialized for this example)\n",
        "embedding_king = torch.tensor([2.0, 3.0, 1.0])\n",
        "embedding_queen = torch.tensor([2.5, 2.8, 1.2])\n",
        "```\n",
        "\n",
        "### Step 2: Compute the Dot Product\n",
        "\n",
        "The dot product of two vectors is the sum of the products of their corresponding components.\n",
        "\n",
        "```python\n",
        "dot_product = torch.dot(embedding_king, embedding_queen)\n",
        "```\n",
        "\n",
        "For our example:\n",
        "\n",
        "$$\n",
        "\\text{dot product} = (2.0 \\times 2.5) + (3.0 \\times 2.8) + (1.0 \\times 1.2)\n",
        "$$\n",
        "\n",
        "### Step 3: Compute the Magnitudes\n",
        "\n",
        "To normalize the similarity, we'll compute the magnitudes of the two embeddings.\n",
        "\n",
        "```python\n",
        "magnitude_king = torch.norm(embedding_king)\n",
        "magnitude_queen = torch.norm(embedding_queen)\n",
        "```\n",
        "\n",
        "### Step 4: Compute the Cosine Similarity\n",
        "\n",
        "Now, we can compute the cosine similarity using the formula:\n",
        "\n",
        "```python\n",
        "cosine_similarity = dot_product / (magnitude_king * magnitude_queen)\n",
        "```\n",
        "\n",
        "The cosine similarity will be a value between -1 and 1. A value of 1 means the embeddings are identical (in direction), a value of 0 means they are orthogonal, and a value of -1 means they are diametrically opposed.\n",
        "\n",
        "### Why is this important in Neural Networks?\n",
        "\n",
        "In many neural network applications, especially in Natural Language Processing (NLP), embeddings are used to represent words, sentences, or documents. By measuring the cosine similarity between embeddings, we can gauge how semantically similar two words or sentences are. This is crucial in tasks like document retrieval, sentiment analysis, and more.\n",
        "\n",
        "Moreover, the concept of dot products and cosine similarity extends to more advanced operations in neural networks, such as the attention mechanisms in transformers, where the similarity between query and key vectors determines the weightage given to a particular value vector."
      ],
      "metadata": {
        "id": "2hsiCblWsyx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `a single attention head`\n",
        "\n",
        "### **1. Introduction to Self-Attention with Multiple Heads:**\n",
        "- The lecturer begins by setting the stage to discuss the workings of a single attention head, one of potentially many in a self-attention mechanism.\n",
        "\n",
        "### **2. Setting up the Toy Example:**\n",
        "- The example still uses a `B x T` arrangement of tokens, but now, each token contains 32 channels of information, instead of the previous 2. This change increases the dimensionality of the data, making it a more realistic representation of actual use-cases.\n",
        "\n",
        "### **3. Revisiting the Weighted Averaging Concept:**\n",
        "- The lecturer reminds us that previously, a simple average was used to combine past information with current information. This was achieved using a lower triangular weight matrix to maintain the causality (not using future information).\n",
        "\n",
        "### **4. Making Attention Data-Dependent:**\n",
        "- Instead of uniformly averaging, self-attention aims to let tokens determine how much attention they pay to other tokens based on data. The motivation is that certain tokens may find specific other tokens more relevant.\n",
        "  \n",
        "### **5. Introduction of Queries and Keys:**\n",
        "- Every token emits two vectors: a **Query** and a **Key**.\n",
        "    - **Query**: Represents what the token is looking for.\n",
        "    - **Key**: Describes what the token contains or represents.\n",
        "- The affinity or interaction strength between two tokens is calculated by taking the dot product of their respective Queries and Keys.\n",
        "\n",
        "### **6. Matrix Multiplication for Calculating Affinities:**\n",
        "- The lecturer shows how to use matrix multiplication, with careful transposing, to calculate the affinities between all tokens in a batched manner. This gives a weight matrix (`way`) that determines how tokens should interact.\n",
        "\n",
        "### **7. Softmax Normalization:**\n",
        "- The raw outputs from the dot products are passed through a masking process to ensure causality (no future interactions). Then, they are normalized using softmax, converting them into a distribution of weights.\n",
        "\n",
        "### **8. Introduction of the Value Vector:**\n",
        "- In addition to Query and Key, each token also emits a **Value** vector.\n",
        "- This Value vector represents what the token will contribute during aggregation. Instead of directly aggregating information from the original tokens (`X`), the self-attention mechanism aggregates from these Value vectors.\n",
        "\n",
        "### **9. Output of a Single Self-Attention Head:**\n",
        "- The final output from a single attention head will have the same dimensionality as the head size. This output is a weighted aggregation of the Value vectors, based on the calculated affinities.\n",
        "\n",
        "### **Summary:**\n",
        "The lecturer is diving deep into the self-attention mechanism's core, illustrating how tokens can dynamically decide which other tokens they find relevant, based on data. This is achieved using Queries, Keys, and Values. Queries represent what a token seeks, Keys signify what a token offers, and Values indicate what a token contributes during aggregation. By computing affinities through the dot product of Queries and Keys, and then aggregating Value vectors based on these affinities, the self-attention mechanism allows each token to gather contextually relevant information from other tokens in a data-dependent manner."
      ],
      "metadata": {
        "id": "ju7AJXlFvksc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `a simple self-attention head using PyTorch`\n",
        "\n",
        "Let's dive deep into the concept of self-attention by building a simple self-attention head using PyTorch. We'll proceed step by step, and I'll explain the purpose and function of each step.\n",
        "\n",
        "### **1. Setup:**\n",
        "First, let's set up the environment with PyTorch and initialize some mock data.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define some mock data (batch size=1, sequence length=3, embedding size=4)\n",
        "x = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]], dtype=torch.float32)\n",
        "```\n",
        "\n",
        "### **2. Define the Self-Attention Head:**\n",
        "In the self-attention mechanism, every token is transformed into Query (Q), Key (K), and Value (V) representations using linear transformations. These representations are used to compute attention scores and aggregate information.\n",
        "\n",
        "```python\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, embed_size, head_size):\n",
        "        super(SelfAttentionHead, self).__init__()\n",
        "\n",
        "        # Linear transformations for Q, K, V\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
        "\n",
        "        # Compute attention weights using softmax\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Aggregate information based on attention weights\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output\n",
        "```\n",
        "\n",
        "### **3. Initialization and Forward Pass:**\n",
        "Now, we'll initialize our self-attention head and run our mock data through it.\n",
        "\n",
        "```python\n",
        "# Initialize the self-attention head with embedding size of 4 and head size of 2\n",
        "head = SelfAttentionHead(embed_size=4, head_size=2)\n",
        "\n",
        "# Run the mock data through the self-attention head\n",
        "output = head(x)\n",
        "print(output)\n",
        "```\n",
        "\n",
        "### **4. What's Happening Inside:**\n",
        "- **Linear Transformations:** The input sequence `x` is transformed into Query, Key, and Value representations using the defined linear layers. This allows each token to both ask questions (Query) and provide answers (Key and Value) about its contextual relevance.\n",
        "  \n",
        "- **Attention Scores:** These scores represent the relevance of each token to every other token. It's computed by taking the dot product of the Query of one token with the Key of every other token.\n",
        "  \n",
        "- **Attention Weights:** We use the softmax function to normalize the attention scores, turning them into probabilities. This allows the model to distribute its \"attention\" across the tokens.\n",
        "  \n",
        "- **Aggregation:** Using the attention weights, the model aggregates the Value representations of the tokens. If a token's Key matches well with another token's Query, its Value will be given more importance in the output.\n",
        "\n",
        "### **5. Interpretation:**\n",
        "The output tensor represents the input sequence after each token has \"attended\" to every other token in the sequence. This aggregated information captures the contextual relationships between tokens.\n",
        "\n",
        "By running the mock data through the self-attention head, you'll see how each token in the sequence has been transformed based on its relationship with other tokens. This is the essence of self-attention!"
      ],
      "metadata": {
        "id": "WR7-bU3XyLBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `auto-regressive`\n",
        "\n",
        "### **1. Base Terms:**\n",
        "- **Auto:** Refers to \"self\" or \"same\".\n",
        "- **Regressive:** Refers to \"going backward\" or \"using the past\".\n",
        "\n",
        "### **2. Basic Definition:**\n",
        "**Auto-regressive** models use their own previous outputs as inputs for future predictions.\n",
        "\n",
        "### **3. Analogy:**\n",
        "Think of writing a story where each new sentence you write is influenced by the previous sentences. The earlier parts of your story (the context) guide the development of the subsequent parts. In essence, the story \"regresses\" or \"looks back\" on itself to continue.\n",
        "\n",
        "### **4. In a Machine Learning Context:**\n",
        "An auto-regressive model, especially in time series forecasting, predicts future data points by using a combination of the previous data points as input. For example, if you're trying to predict tomorrow's stock price, you might use the stock prices from the last ten days as input to your model. The model, in essence, learns the patterns from its own previous outputs.\n",
        "\n",
        "### **5. In Deep Learning (especially NLP):**\n",
        "Auto-regressive models, like certain types of language models, generate sequences one part at a time and use what they've generated so far as context for producing the next part. For instance, when generating a sentence, after producing the word \"The\", the model might be more likely to produce \"cat\" next if it has learned the association between those words from the training data.\n",
        "\n",
        "### **6. Important Note:**\n",
        "Auto-regressive models inherently have a sequential nature because each prediction depends on the previous ones. This can make them slower in certain applications since they can't easily be parallelized.\n",
        "\n",
        "### **7. Why it Matters:**\n",
        "Understanding the past can often provide valuable insights into the future. Auto-regressive models capture this idea by leveraging previous data or outputs to make more informed predictions. It's a foundational concept, especially in time series analysis and sequence generation tasks.\n",
        "\n",
        "In essence, \"auto-regressive\" is all about using the past to predict the future in a structured, iterative manner."
      ],
      "metadata": {
        "id": "1f40GwD01bS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note1: Attention as Communication`\n",
        "\n",
        "### **1. Attention as Communication:**\n",
        "- **Basic Concept:** The lecturer likens the attention mechanism to a communication system. Just as in a communication network where nodes exchange information, in the attention mechanism, different parts (or nodes) of the data \"communicate\" or exchange information with one another.\n",
        "  \n",
        "### **2. Directed Graph Analogy:**\n",
        "- **Graph Structure:** The lecturer draws an analogy between attention and a directed graph. In this graph, nodes represent chunks of data, and the edges (or connections) between nodes represent the \"attention\" or focus one node gives to another.\n",
        "  \n",
        "- **Aggregating Information:** Each node in the graph possesses a vector of information. When it comes time to decide how much attention to pay to other nodes, it aggregates information from all nodes pointing to it. This aggregation isn't just a simple sum; it's a weighted sum, meaning some nodes influence it more heavily than others.\n",
        "\n",
        "### **3. Data-Dependent Nature:**\n",
        "- **Dynamic Adjustments:** The way nodes communicate or how much attention they give to one another isn't static. It's data-dependent, meaning it changes based on the information contained within each node. This dynamic adjustment ensures that the attention mechanism is flexible and adapts based on the data it's working with.\n",
        "\n",
        "### **4. Specific Structure (in the example):**\n",
        "- **Sequential Nodes:** The lecturer then describes a specific structure where there are eight nodes (due to a block size of eight tokens). This structure is sequential, much like a chain. The first node is self-referential (only points to itself), the second node is influenced by the first and itself, and so on, until the eighth node, which aggregates information from all previous nodes and itself.\n",
        "\n",
        "### **5. Applicability Beyond Sequences:**\n",
        "- **General Mechanism:** While the described structure is sequential and auto-regressive (fitting scenarios like language modeling), the lecturer emphasizes that attention isn't limited to this kind of structure. It can be applied to any arbitrary directed graph. This means attention is a versatile tool, not just limited to sequences but adaptable to various data structures and scenarios.\n",
        "\n",
        "### **In Summary:**\n",
        "The lecturer is teaching that attention is a dynamic communication mechanism, akin to nodes in a directed graph exchanging information. The amount of information exchanged is data-dependent and can adapt based on the data at hand. While attention is often associated with sequential tasks, its foundational principles are general and can be applied to a wide array of problems."
      ],
      "metadata": {
        "id": "7pp8dQN01okg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note 2: Attention has no notion of space`\n",
        "\n",
        "### **1. Attention's Lack of Spatial Awareness:**\n",
        "- **Basic Idea:** The lecturer starts by emphasizing that, fundamentally, attention mechanisms don't have an inherent notion of where things are located in relation to one another. Instead, they operate on sets of vectors, treating them as distinct entities without considering their relative positions.\n",
        "\n",
        "### **2. Sets vs. Sequences:**\n",
        "- **Understanding Sets:** In a set, the order of elements doesn't matter. If you have a set of numbers like {3, 1, 2}, it's the same as {1, 2, 3}. So, when attention operates on a set of vectors, it doesn't inherently consider one vector as \"coming before\" or \"after\" another. They're all just part of the set.\n",
        "  \n",
        "- **Contrast with Sequences:** In sequences, the order matters. For instance, a sequence [3, 1, 2] is different from [1, 2, 3]. Traditional sequence processing techniques, like RNNs, inherently process data in an order, considering the relative positioning of elements.\n",
        "\n",
        "### **3. Convolution's Spatial Nature:**\n",
        "- **Spatial Intuition:** The lecturer contrasts attention with convolution operations, a staple in image processing and CNNs. Convolution inherently operates in spatial dimensions, recognizing patterns based on their layout and position within an image. For example, a convolutional filter might detect an edge or texture at a specific location in an image.\n",
        "\n",
        "- **Ordered vs. Unordered:** Convolutional operations respect spatial order, while attention mechanisms treat input data as unordered sets, unless explicitly given positional information.\n",
        "\n",
        "### **4. Positional Encoding:**\n",
        "- **Why It's Needed:** Because attention lacks this sense of order or position, when we want it to consider the order or position of data (like in sequence processing), we need to provide it with some explicit cues or hints about the positional structure. This is where positional encoding comes in.\n",
        "\n",
        "- **How It Works:** Positional encodings are added to the vectors in attention mechanisms to give them a sense of where each piece of data sits in relation to others. It's like tagging each data point with its \"address\" or \"location\" so the attention mechanism can use this information when deciding how to weigh or prioritize different pieces of data.\n",
        "\n",
        "### **In Summary:**\n",
        "The lecturer is highlighting the fundamental difference between attention and more spatially-aware operations like convolution. While attention is powerful, its lack of inherent spatial awareness means that when we want it to consider positions or order, we need to provide that information explicitly. This insight underscores the importance of positional encoding when using attention for sequence data."
      ],
      "metadata": {
        "id": "xe8qqsaH3WfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note 3: The is no communications across batch dimensions`\n",
        "\n",
        "### **1. Independence Across the Batch Dimension:**\n",
        "- **Basic Idea:** The lecturer emphasizes that when processing data in batches, each individual data point (or sequence) in the batch is processed independently. They don't \"communicate\" or share information with each other.\n",
        "\n",
        "### **2. The Concept of \"Talking\" or \"Communication\":**\n",
        "- **In Context:** When the lecturer refers to data points \"talking\" to each other, they're referencing the attention mechanism, where different parts of a sequence (or nodes in their analogy) can \"attend to\" or take information from other parts. This is the essence of attention.\n",
        "\n",
        "- **Limitation:** However, this \"communication\" is restricted within individual examples in the batch, not between them.\n",
        "\n",
        "### **3. Batch Matrix Multiply:**\n",
        "- **Parallel Processing:** The operation is applied in parallel across the batch dimension. This means that while the same operation (like attention) is applied to every example in the batch, it's done so independently for each one.\n",
        "\n",
        "### **4. Visualizing the Directed Graph Analogy:**\n",
        "- **Single Graph:** If we visualize the processing as a directed graph (as the lecturer has done previously), then a single sequence would be a set of nodes (e.g., eight nodes for eight tokens in a sequence) with edges indicating the \"communication\" or attention between them.\n",
        "  \n",
        "- **Multiple Graphs in a Batch:** However, since the batch size is four, the lecturer suggests visualizing this as four separate directed graphs (or four separate pools of eight nodes). Each graph represents a sequence in the batch, and they're processed simultaneously but independently.\n",
        "\n",
        "### **In Summary:**\n",
        "The lecturer's main point is to clarify how batching works in the context of attention mechanisms. Even though we might be processing multiple sequences at once (a batch), each sequence is handled as its own independent unit. There's no cross-communication or sharing of information between different sequences in the batch, even though they're being processed in parallel. This insight is essential for understanding how the attention mechanism scales and operates on batches of data."
      ],
      "metadata": {
        "id": "P5bejloI37M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note 4: Encoder blocks vs. Decoder blocks`\n",
        "\n",
        "### **1. Context Matters: Language Modeling vs. Other Tasks**\n",
        "- **Language Modeling:** The lecturer starts by discussing the unique structure of language modeling tasks. In such tasks, the goal is to predict the next token in a sequence given the previous ones. Consequently, it's crucial that future tokens (ones we're trying to predict) don't provide information about past tokens, as this would \"give away the answer.\"\n",
        "  \n",
        "### **2. Different Communication Patterns: Encoder vs. Decoder Blocks**\n",
        "- **Encoder Blocks:**\n",
        "  - **Function:** Used when all the tokens in a sequence need to \"talk\" or attend to each other without restrictions. This is common in tasks like sentiment analysis where understanding the entire sentence in context is critical for determining its sentiment.\n",
        "  - **Key Feature:** No masking is applied. Every token can attend to every other token.\n",
        "  \n",
        "- **Decoder Blocks:**\n",
        "  - **Function:** Used in tasks like language modeling where we're generating sequences and don't want future tokens to influence past ones.\n",
        "  - **Key Feature:** A triangular structure (masking) is used to prevent future tokens from attending to past tokens.\n",
        "\n",
        "### **3. Attention's Flexibility:**\n",
        "- **Arbitrary Connectivity:** The attention mechanism itself doesn't impose constraints on which tokens can attend to which other tokens. It's very flexible and allows for any connectivity pattern. The choice of pattern (like the triangular masking in decoder blocks) is imposed based on the specific task's requirements.\n",
        "\n",
        "### **In Summary:**\n",
        "The lecturer emphasizes the distinction between encoder and decoder blocks in the context of attention mechanisms. While the attention mechanism is inherently flexible, the way it's used can differ based on the task. For tasks like language modeling, where predicting the next token is crucial, decoder blocks are used to ensure a one-directional flow of information. In contrast, for tasks like sentiment analysis where the entire context is essential, encoder blocks are used to allow full bi-directional communication among tokens."
      ],
      "metadata": {
        "id": "P5ThEk3d4_qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note 5: Self-Attention` and `Cross-Attention`\n",
        "\n",
        "### **1. The Basic Idea: Attention Mechanism**\n",
        "- **Primary Concept:** Attention mechanisms enable certain parts of the input data to be \"focused on\" or \"attended to\" more than others. This allows neural networks to weigh the importance of different parts of the input differently when producing an output.\n",
        "\n",
        "### **2. Self-Attention:**\n",
        "- **Definition:** In self-attention, the keys, queries, and values all originate from the same source or dataset. The term \"self\" signifies that the mechanism is looking at its own data to determine what parts to focus on.\n",
        "- **Example:** In the context of language processing, each word in a sentence can look at other words in the same sentence to determine its context.\n",
        "\n",
        "### **3. Cross-Attention:**\n",
        "- **Definition:** In cross-attention, the queries come from one source, while the keys and values come from another, separate source. This allows one dataset to \"attend to\" or gather information from a different dataset.\n",
        "- **Example:** In a machine translation task, imagine translating English to French using an encoder-decoder architecture. The encoder processes the English sentence and the decoder generates the French translation. In cross-attention, the decoder (while generating the French words) can \"look at\" or \"attend to\" the original English sentence (encoded values) to get context and improve translation accuracy.\n",
        "\n",
        "### **4. Flexibility of Attention:**\n",
        "- **Inherent Versatility:** The lecturer emphasizes that the attention mechanism itself is inherently versatile. It can be used in various configurations, whether that's focusing within its own data (self-attention) or drawing information from a separate dataset (cross-attention).\n",
        "\n",
        "### **In Summary:**\n",
        "The lecturer clarifies the distinction between \"self-attention\" and \"cross-attention\". While both mechanisms allow a model to focus on specific parts of data, the difference lies in the source of that data. Self-attention refers to focusing within its own dataset, while cross-attention involves drawing context from an external dataset. This versatility makes attention mechanisms a powerful tool in various neural network architectures and tasks."
      ],
      "metadata": {
        "id": "_D5asp3Z5_U8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Note 6: 'Scaled' Self-Attention`\n",
        "\n",
        "Alright, let's dissect the essential points from the note:\n",
        "\n",
        "### **1. The Concept of \"Scaled\" Self-Attention:**\n",
        "- **Primary Idea:** In the attention mechanism, after multiplying the queries and keys, we perform a normalization step by dividing by the square root of the head size (often represented as $ \\sqrt{\\text{DK}} $ in literature). This normalization step is what makes the attention \"scaled.\"\n",
        "\n",
        "### **2. The Problem of Unit Gaussian Inputs:**\n",
        "- **Scenario:** Imagine if our keys and queries are unit gaussians (zero mean and unit variance). When they are multiplied (as in the dot product), the resulting weights (or affinities) end up with a variance proportional to the head size.\n",
        "- **Implication:** Having a variance this large can make the weights extreme, especially when fed into a softmax operation.\n",
        "\n",
        "### **3. Softmax Behavior with Extreme Values:**\n",
        "- **Insight:** Softmax is sensitive to the scale of its input values. When given values that are close together, the softmax output is spread out or \"diffuse\". However, if the input values become larger and more distinct from each other, the softmax output becomes more \"peaky\" or concentrated towards the max value.\n",
        "- **Why is this problematic?** If the weights become too extreme (either very large or very small), the softmax will favor only a single node, leading to almost a one-hot vector. This is undesirable because the mechanism would be heavily focusing on just one part of the data, neglecting other potentially valuable information.\n",
        "\n",
        "### **4. The Importance of the Scaling Factor:**\n",
        "- **Function:** By dividing by $ \\sqrt{\\text{DK}} $, the variance of the weights is normalized to be closer to 1, preventing them from becoming too extreme.\n",
        "- **Purpose:** The scaling ensures that during initialization, the softmax operation produces a more balanced distribution, where the model can consider information from multiple nodes rather than focusing excessively on a single one.\n",
        "\n",
        "### **In Summary:**\n",
        "The \"scaled\" in \"scaled self-attention\" refers to the normalization step introduced to control the variance of the attention weights, especially during initialization. This normalization ensures that the softmax operation doesn't produce extremely \"peaky\" distributions, allowing the attention mechanism to aggregate information from multiple sources effectively."
      ],
      "metadata": {
        "id": "TvEZXY2T8VJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Softmax`\n",
        "\n",
        "### **1. What is Softmax?**\n",
        "Imagine you have a list of raw scores, or logits, and you want to convert these scores into probabilities. The softmax function is a tool to do just that. It takes each score, exponentiates it, and then normalizes the results so that they sum up to 1.\n",
        "\n",
        "### **2. What do we want from Softmax?**\n",
        "\n",
        "**a. Convert Scores to Probabilities:**  \n",
        "- The primary purpose of softmax is to transform the raw scores into a distribution of probabilities. After applying softmax, each score is squashed between 0 and 1, and the sum of all scores equals 1.\n",
        "\n",
        "**b. Highlight Differences:**  \n",
        "- Softmax accentuates the differences between scores. If one score is slightly larger than another, after softmax, the difference between their probabilities will be even more significant.\n",
        "\n",
        "**c. Compatibility with Multi-Class Classification:**  \n",
        "- In machine learning, especially in classification tasks, we often want to assign a data point to one of several classes. Softmax is perfect for this because it gives a probability distribution across multiple classes.\n",
        "\n",
        "### **3. What do we want to avoid from Softmax?**\n",
        "\n",
        "**a. Extreme Confidence:**  \n",
        "- One potential downside is that softmax can be very confident (i.e., a probability very close to 1 for one class and close to 0 for others) even if the model's prediction is wrong. This extreme confidence can be problematic, especially if the model is not very accurate.\n",
        "\n",
        "**b. Temperature Sensitivity:**  \n",
        "- The softmax function is sensitive to the scale or \"temperature\" of the logits. If the logits are multiplied by a high value, the softmax output becomes more \"peaky,\" concentrating on the maximum value. Conversely, if they are multiplied by a small value, the output becomes more uniform. We must be careful about the scale of the logits fed into softmax.\n",
        "\n",
        "**c. Not Suitable for Independent Classes:**  \n",
        "- Softmax assumes that classes are mutually exclusive. It's not suitable for multi-label classification where a data point can belong to multiple classes simultaneously.\n",
        "\n",
        "### **In Summary:**\n",
        "Softmax is like a talent show judge. If the performances are somewhat similar in quality, the judge might spread out the points. But if one performance stands out, even by a bit, the judge might give it a significantly higher score. However, like all judges, softmax can sometimes be too confident or swayed by certain factors, so it's essential to be aware of its characteristics when using it in models."
      ],
      "metadata": {
        "id": "R4YqbDZV96-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Dot Product of Unit Gaussians`\n",
        "\n",
        "### **1. What are Unit Gaussians?**\n",
        "Imagine you're tossing darts at a dartboard. If you're pretty good, most of your darts will land near the center. The pattern of your darts could be described by a bell-shaped curve, which is wider or narrower based on how consistent you are. This bell-shaped curve is the Gaussian or Normal distribution. When we say \"unit\" Gaussian, we're specifying a particular kind of Gaussian where the bell is centered at zero (zero mean) and has a specific width (unit variance).\n",
        "\n",
        "### **2. Dot Product of Unit Gaussians:**\n",
        "When we take the dot product of two vectors sampled from unit Gaussians, the result tends to spread out. This is analogous to two skilled dart players tossing darts. If they're both pretty consistent, but one player always aims slightly off to the right and the other slightly to the left, when they play on the same board, the darts will be more spread out than when either plays alone.\n",
        "\n",
        "In mathematical terms, if our vectors (keys and queries) are of length `d` (also referred to as head size), the variance of their dot product tends to grow with `d`.\n",
        "\n",
        "### **3. The Problem:**\n",
        "Now, why is this spreading out a problem? Well, when we feed these spread-out values into the softmax function (as in the attention mechanism), the output can become very \"peaky\". This means the softmax will heavily favor one particular value over others, even if the differences between the original scores are quite small.\n",
        "\n",
        "Going back to our dart analogy, it's as if our scoring system gives exponentially more points the closer you are to the center. If one player's darts are just a tiny bit closer on average, they'll end up with a massively higher score, even if the actual performance difference was minor.\n",
        "\n",
        "### **4. The Solution:**\n",
        "To counteract this problem, the dot products are often scaled down by dividing by the square root of the head size (`d`). This scaling keeps the results from becoming too extreme and helps the softmax produce a more balanced distribution of weights.\n",
        "\n",
        "### **In Summary:**\n",
        "When unit gaussians are involved in dot products, they're like our two skilled dart players with slightly different aiming points. Their combined game tends to spread out more. In the world of neural networks, this can lead to an overly confident softmax unless we adjust for it."
      ],
      "metadata": {
        "id": "fek48WgT-g3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-headed Attention\n",
        "\n",
        "### 1. **The Essence of Attention**:\n",
        "\n",
        "Imagine you're reading a book, and you come across a sentence where the protagonist refers to \"her\". To understand who \"her\" refers to, you may need to look back a few sentences or even a few pages. This act of referring back to get context is a form of \"attention\".\n",
        "\n",
        "### 2. **Single-Head Attention**:\n",
        "\n",
        "Now, imagine you're wearing a pair of glasses that only lets you focus on one specific type of detail at a time. For instance, when wearing one pair, you only notice emotions in the text. With another pair, you only catch details about the surroundings.\n",
        "\n",
        "In the context of our model, this is what a single attention \"head\" does. It focuses on specific relationships or patterns in the data.\n",
        "\n",
        "### 3. **Why Not Just One Pair of Glasses?**:\n",
        "\n",
        "But, if you're trying to understand a story deeply, one type of detail isn't enough. You want to catch emotions, surroundings, actions, and more. So, instead of reading the story multiple times with different glasses, wouldn't it be great if you could wear multiple pairs at once? This way, with a single look, you'd catch various details.\n",
        "\n",
        "### 4. **Multi-Head Attention**:\n",
        "\n",
        "This is the essence of multi-head attention. Instead of having just one \"head\" or \"pair of glasses\" focusing on one pattern, we have multiple heads working in parallel, each looking at different aspects of the input. When we combine the insights from all these heads, we get a richer, more comprehensive understanding of the data.\n",
        "\n",
        "### 5. **Combining the Outputs**:\n",
        "\n",
        "After each head has done its job, we concatenate their outputs and pass them through a linear layer to merge the information. This combination ensures that the model can use insights from multiple perspectives for subsequent processing.\n",
        "\n",
        "### 6. **Why It's Useful**:\n",
        "\n",
        "Just like in our reading example, where understanding both emotions and surroundings gives a fuller picture of the story, in tasks like language understanding, capturing various relationships (like syntactic, semantic, or positional) simultaneously can be crucial for deep comprehension.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Multi-head attention is like reading with multiple specialized pairs of glasses at once. Each pair focuses on a different detail, and when combined, they give a comprehensive understanding of the text."
      ],
      "metadata": {
        "id": "SoOJWnqB2vYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-headed Self-Attention\n",
        "\n",
        "### 1. **Overview**:\n",
        "- At its core, self-attention allows an input sequence to focus on different parts of itself when producing an output sequence. It's as if each word in a sentence can examine other words to determine its context and meaning.\n",
        "  \n",
        "### 2. **The Need for Multiple Heads**:\n",
        "- Think of self-attention as giving each word in a sentence a pair of \"glasses\" that lets it look at other words. But why have just one pair of glasses? What if we gave each word multiple pairs, each with a different lens or focus? That's multi-head attention – multiple sets of attentions (or \"views\") for each word.\n",
        "\n",
        "### 3. **Basic Mechanism**:\n",
        "- For each word/token in our sequence, we compute three things: a **query** (Q), a **key** (K), and a **value** (V). These are derived from the input by multiplying it with three weight matrices (which we learn during training).\n",
        "- The attention scores (how much focus a word should have on other words) are computed by taking the dot product of the query of one word with the key of every other word. This gives a measure of similarity or importance.\n",
        "- These scores are then scaled down (to handle large values which can be problematic in deep networks) and passed through a softmax function to turn them into probabilities.\n",
        "- Finally, these probabilities are used to create a weighted combination of the values, producing the output for that word.\n",
        "\n",
        "### 4. **Multi-head Twist**:\n",
        "- Instead of computing this attention once (single view or single pair of glasses), we do it multiple times in parallel with different weight matrices. Each parallel operation is called a \"head\".\n",
        "- By having multiple heads, our network can focus on different parts of the input or capture various aspects of the information. For instance, one head might capture syntactic information (sentence structure) while another might focus on semantic information (meaning).\n",
        "  \n",
        "### 5. **Aggregation**:\n",
        "- Once we have the outputs from each head, we concatenate them and pass them through a linear layer to produce the final output. This ensures that the multi-head mechanism is smoothly integrated into the rest of the model.\n",
        "\n",
        "### 6. **Why It Works**:\n",
        "- This mechanism allows the model to consider different interpretations of each word in the context of a sentence. It's like reading a sentence while focusing on different features each time – grammar, sentiment, subject matter, etc.\n",
        "  \n",
        "### 7. **Real-world Analogy**:\n",
        "- Imagine reading a scientific paper. The first time, you focus on understanding the main results. The second time, you might pay more attention to the methodology, and the third time, you might look at references and contextual information. Each reading (or \"head\") gives you a different perspective, and by combining them, you get a comprehensive understanding.\n",
        "\n",
        "The power of multi-headed self-attention lies in its ability to capture diverse and rich contextual information from sequences, making it a cornerstone of modern transformer architectures."
      ],
      "metadata": {
        "id": "uFwFYsomRjF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.ModuleList`\n",
        "\n",
        "### 1. **The Basics**:\n",
        "`torch.nn.ModuleList` is a container module in PyTorch that can be used to contain other modules (like layers of a neural network). It's essentially a list for PyTorch modules, but with some special properties that make it integrate nicely with the rest of the PyTorch ecosystem.\n",
        "\n",
        "### 2. **Why Not Just Use Python Lists?**:\n",
        "You might wonder, \"Why do I need a `ModuleList`? Can't I just use a Python list?\". The answer lies in how PyTorch tracks modules. If you use a regular Python list to store sub-modules, PyTorch won't be aware of these sub-modules, and methods like `.to(device)`, `.eval()`, or `.train()` won't work as expected on them.\n",
        "\n",
        "### 3. **Use Cases**:\n",
        "The primary use case for `ModuleList` is when you have a varying number of similar sub-modules and want to iterate over them. For instance:\n",
        "- **Dynamic Neural Networks**: When the number of layers or components is decided at runtime.\n",
        "- **Multiple Attention Heads**: In a transformer architecture, where you have multiple attention mechanisms working in parallel.\n",
        "- **Ensemble Models**: When you have multiple models or sub-models and want to iterate over them for training or prediction.\n",
        "\n",
        "### 4. **Example**:\n",
        "\n",
        "Let's say you want to create a feed-forward neural network, but you want the flexibility to specify the number of layers at runtime. Here's how you could use `ModuleList`:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class DynamicFeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(DynamicFeedForwardNN, self).__init__()\n",
        "        \n",
        "        # Initialize layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        # Input layer\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        \n",
        "        # Hidden layers\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "        \n",
        "        # Output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.layers[-1](x)  # No activation for the final layer in this example\n",
        "        return x\n",
        "\n",
        "# Create a model with 3 hidden layers of sizes 128, 64, and 32 respectively\n",
        "model = DynamicFeedForwardNN(input_size=784, hidden_sizes=[128, 64, 32], output_size=10)\n",
        "```\n",
        "\n",
        "### 5. **Summary**:\n",
        "`torch.nn.ModuleList` offers a way to maintain a list of sub-modules that can be iterated over, and it ensures that PyTorch is aware of each sub-module so that operations applied to the parent module propagate to the sub-modules correctly."
      ],
      "metadata": {
        "id": "WyBs4cza6lrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.LayerNorm`\n",
        "\n",
        "### **1. The Need for Normalization:**\n",
        "Neural networks, especially deep ones, can be sensitive to the scale and distribution of their input features. If one feature has a range of [0, 1] while another has a range of [0, 1000], it can create problems in learning. Normalizing these features to a common scale can help alleviate these issues.\n",
        "\n",
        "### **2. Batch Normalization:**\n",
        "Before diving into Layer Normalization, it's worth mentioning Batch Normalization, which is a widely used normalization technique in deep learning. It normalizes each feature across a batch of data. However, it depends on the batch size and may behave differently during training and inference.\n",
        "\n",
        "### **3. Introduction to Layer Normalization:**\n",
        "Layer Normalization is an alternative to Batch Normalization. Instead of normalizing over the batch dimension, Layer Normalization normalizes over the feature dimension. This makes it independent of batch sizes, and hence it behaves consistently during both training and inference.\n",
        "\n",
        "### **4. Mathematical Representation:**\n",
        "Given an input tensor \\( x \\) of shape \\([B, F]\\) where \\( B \\) is the batch size and \\( F \\) is the number of features:\n",
        "\n",
        "- Compute the mean \\( \\mu \\) and variance \\( \\sigma^2 \\) across the feature dimension.\n",
        "- Normalize the features as:\n",
        "\\[ y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\]\n",
        "where \\( \\epsilon \\) is a small number to ensure numerical stability.\n",
        "- Scale and shift the normalized output:\n",
        "\\[ y = \\gamma \\times y + \\beta \\]\n",
        "where \\( \\gamma \\) and \\( \\beta \\) are learnable parameters of the same shape as \\( x \\).\n",
        "\n",
        "### **5. Benefits of Layer Normalization:**\n",
        "- **Batch Size Independence:** Since Layer Normalization doesn't normalize across the batch dimension, it's independent of the batch size. This is particularly useful for tasks where batch size might be variable or for models like transformers where batch normalization might not be ideal.\n",
        "  \n",
        "- **Consistent Behavior:** Layer Normalization behaves the same during training and inference since it doesn't rely on batch statistics.\n",
        "\n",
        "### **6. Implementation in PyTorch:**\n",
        "In PyTorch, Layer Normalization can be easily implemented using the `torch.nn.LayerNorm` module. It takes in the normalized shape as an argument, and optionally, epsilon for numerical stability.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "import torch.nn as nn\n",
        "layer_norm = nn.LayerNorm(normalized_shape=512, eps=1e-5)\n",
        "```\n",
        "\n",
        "### **7. Conclusion:**\n",
        "Layer Normalization is a versatile normalization technique that addresses some of the challenges posed by Batch Normalization. By normalizing across features instead of batches, it offers consistent behavior across different phases of the model lifecycle and is particularly useful for models and tasks where batch size can vary."
      ],
      "metadata": {
        "id": "96ZLgk69lk8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Connections\n",
        "\n",
        "Residual connections, also known as shortcut connections or skip connections, are a powerful architectural feature that help combat the vanishing gradient problem in deep networks. Let's break this concept down step-by-step:\n",
        "\n",
        "### 1. **Problem with Deep Networks**:\n",
        "- Deep networks can be hard to train. As networks get deeper, gradients during backpropagation can become extremely small, a problem known as the vanishing gradient. This means that weights don't get updated effectively, and training can stall.\n",
        "- Additionally, deeper layers might end up learning identity functions, where the output is the same as the input, which doesn't contribute any additional information.\n",
        "\n",
        "### 2. **What are Residual Connections?**:\n",
        "- A residual connection is a shortcut around one or more layers. Instead of being sent through successive layers, the data can bypass them.\n",
        "- Specifically, if you have an input \\(X\\), and after one or more layers it's transformed into \\(F(X)\\), a residual connection would compute the output as \\(X + F(X)\\). Here, \\(F(X)\\) represents the transformation learned by the layers.\n",
        "\n",
        "### 3. **How Do They Help?**:\n",
        "- By adding the original input \\(X\\) back to the output of the network, the model is encouraged to learn just the residual (difference) between the input and output, which can be easier.\n",
        "- If any layers become uninformative and start approximating an identity function, the network can still ensure proper training using the direct shortcut.\n",
        "- Residual connections also provide an alternate path for gradients during backpropagation, making it easier for them to flow through the network. This mitigates the vanishing gradient problem.\n",
        "\n",
        "### 4. **Implementation**:\n",
        "- In practice, the input \\(X\\) is added to the output \\(F(X)\\) of the block of layers. This sum then goes through the next layer.\n",
        "- If the dimensions of \\(X\\) and \\(F(X)\\) don't match, a linear transformation is applied to \\(X\\) to bring it to the required dimension.\n",
        "\n",
        "### 5. **Results**:\n",
        "- Networks with residual connections, such as ResNets, can be trained to be much deeper than those without, leading to better performance without overfitting.\n",
        "- ResNets, which prominently use residual connections, have set performance benchmarks in various deep learning tasks, especially image classification.\n",
        "\n",
        "### Step-by-Step Summary:\n",
        "\n",
        "1. Deep networks often face the vanishing gradient problem which hampers training.\n",
        "2. Residual connections provide shortcuts around layers in the network.\n",
        "3. These connections allow the network to learn the residual (difference) between layers' input and output.\n",
        "4. They facilitate the flow of gradients during backpropagation, making deep networks easier to train.\n",
        "5. ResNets, which use these connections, have achieved state-of-the-art results in many tasks.\n",
        "\n",
        "In essence, residual connections provide a sort of \"safety net\" for the training process, ensuring that even if some layers in the network aren't helpful, the network as a whole can still train effectively."
      ],
      "metadata": {
        "id": "WrFkce_Ejc9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a feed-forward neural network (FFN)?\n",
        "\n",
        "### 1. **Basic Definition**:\n",
        "A feed-forward neural network is a type of artificial neural network where the connections between nodes (often called \"neurons\" or \"units\") do not form any cycles. This means the data flows in one direction: from the input layer, through one or more hidden layers, to the output layer. There are no feedback connections.\n",
        "\n",
        "### 2. **Components of a Feed-forward Neural Network**:\n",
        "\n",
        "- **Input Layer**: This is where the network receives its input. The number of neurons in this layer is determined by the dimensionality of the input data.\n",
        "  \n",
        "- **Hidden Layers**: These are the layers between the input and output layers. A feed-forward network can have zero (making it a \"single-layer perceptron\"), one, or many hidden layers. The neurons in these layers apply transformations to the data as it flows through the network.\n",
        "  \n",
        "- **Output Layer**: This layer produces the final predictions or classifications. The number of neurons in the output layer is determined by the type of problem (e.g., binary classification, multi-class classification, regression, etc.).\n",
        "  \n",
        "- **Weights and Biases**: These are the parameters of the network that are learned during training. Each connection between neurons has an associated weight, and each neuron has an associated bias.\n",
        "\n",
        "- **Activation Function**: This is a function applied to the output of each neuron, introducing non-linearity into the model. Common activation functions include the sigmoid, tanh, and ReLU.\n",
        "\n",
        "### 3. **How It Works**:\n",
        "\n",
        "1. **Initialization**: The weights and biases of the network are usually initialized with small random numbers.\n",
        "   \n",
        "2. **Data Flow**: For a given input, data flows through the network. The input is passed through the hidden layers, transformed by weights, biases, and activation functions, until an output is produced.\n",
        "   \n",
        "3. **Training**: Using a dataset, the network's predictions are compared to the true values. A loss function measures the difference between the predictions and the true values. The goal during training is to adjust the weights and biases to minimize this loss.\n",
        "   \n",
        "4. **Backpropagation**: This is the algorithm used to adjust the weights and biases based on the computed loss. It calculates the gradient of the loss with respect to each parameter and then updates the parameters in the direction that reduces the loss.\n",
        "   \n",
        "5. **Iteration**: Steps 2-4 are repeated for many iterations (often called \"epochs\") until the loss converges to a minimum value, or until some other stopping criterion is met.\n",
        "\n",
        "### 4. **Why \"Feed-forward\"?**:\n",
        "The term \"feed-forward\" emphasizes the fact that data flows forward through the network. There are no backward or recurrent connections as there are in other types of networks like Recurrent Neural Networks (RNNs).\n",
        "\n",
        "### 5. **Advantages**:\n",
        "- Simplicity: Feed-forward networks are straightforward to understand and implement.\n",
        "- Universality: A feed-forward network with just one hidden layer containing a finite number of neurons can approximate any continuous function to any desired accuracy, given a suitable activation function.\n",
        "\n",
        "### 6. **Limitations**:\n",
        "- No Memory: Since there's no feedback or recurrent connections, FFNs don't have any memory of previous inputs. This makes them less suitable for tasks like time series prediction or natural language processing where context or sequence information is important.\n",
        "\n",
        "### 7. **Summary**:\n",
        "A feed-forward neural network is a basic type of neural network where data flows in one direction, from input to output. It's a foundational architecture in deep learning and serves as a building block for many other types of neural networks."
      ],
      "metadata": {
        "id": "PSCNGUiy7FTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15 different types of Neural Networks\n",
        "\n",
        "Certainly! Neural networks come in various architectures, each designed to tackle specific types of tasks or data. Here's a step-by-step breakdown of some of the most prominent types:\n",
        "\n",
        "### 1. **Feed-forward Neural Networks (FFN) / Multi-layer Perceptrons (MLP)**:\n",
        "- **Description**: The simplest type of artificial neural network architecture. As described previously, it's characterized by having inputs that travel only in one direction, from the input layer to the output layer, with one or more hidden layers in between.\n",
        "- **Use Cases**: Basic classification and regression tasks.\n",
        "\n",
        "### 2. **Convolutional Neural Networks (CNN or ConvNet)**:\n",
        "- **Description**: Designed to process grid-structured data like images. They use convolutional layers that apply convolutional filters to local regions of the input, making them translation invariant.\n",
        "- **Use Cases**: Image and video recognition, image classification, medical image analysis.\n",
        "\n",
        "### 3. **Recurrent Neural Networks (RNN)**:\n",
        "- **Description**: Designed to recognize patterns in sequences of data, such as time series or natural language. They have connections that loop back on themselves, allowing information to persist.\n",
        "- **Use Cases**: Natural language processing, speech recognition, time series prediction.\n",
        "\n",
        "### 4. **Long Short-Term Memory (LSTM) Networks**:\n",
        "- **Description**: A special kind of RNN that can learn long-term dependencies. They are explicitly designed to avoid long-term dependency issues, making them more effective for sequences.\n",
        "- **Use Cases**: Machine translation, speech synthesis.\n",
        "\n",
        "### 5. **Gated Recurrent Units (GRU)**:\n",
        "- **Description**: A simplified version of LSTM with fewer gates, but often offers comparable performance.\n",
        "- **Use Cases**: Similar to LSTMs - sequence prediction, machine translation, etc.\n",
        "\n",
        "### 6. **Radial Basis Function Neural Networks (RBFNN)**:\n",
        "- **Description**: Uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters.\n",
        "- **Use Cases**: Function approximation, time series prediction.\n",
        "\n",
        "### 7. **Modular Neural Networks**:\n",
        "- **Description**: Comprises multiple individual networks that are trained separately and whose outputs are then combined.\n",
        "- **Use Cases**: Large and complex problems which can be divided into smaller, more manageable sub-tasks.\n",
        "\n",
        "### 8. **Sequence-to-Sequence Models**:\n",
        "- **Description**: Consists of two main components, an encoder and a decoder, often implemented with LSTMs or GRUs. The encoder processes an input sequence and compresses its information into a context vector which the decoder then uses to produce an output sequence.\n",
        "- **Use Cases**: Machine translation, text summarization.\n",
        "\n",
        "### 9. **Transformers**:\n",
        "- **Description**: Introduced in the \"Attention Is All You Need\" paper. They rely heavily on self-attention mechanisms and have shown state-of-the-art performance on various NLP tasks.\n",
        "- **Use Cases**: Almost all advanced NLP tasks now, including machine translation, text generation, and more.\n",
        "\n",
        "### 10. **Generative Adversarial Networks (GAN)**:\n",
        "- **Description**: Comprises two networks: a generator and a discriminator. The generator tries to produce data, while the discriminator tries to distinguish between real and generated data. They are trained together in a cat-and-mouse game.\n",
        "- **Use Cases**: Generating realistic images, art creation, image-to-image translation.\n",
        "\n",
        "### 11. **Neural Architecture Search (NAS)**:\n",
        "- **Description**: An automated approach for neural network model design. Algorithms search the best neural network architecture for a particular dataset and task.\n",
        "- **Use Cases**: Automated machine learning, finding optimal network designs.\n",
        "\n",
        "### 12. **Self-Organizing Maps (SOM)**:\n",
        "- **Description**: A type of unsupervised learning method that reduces dimensions and visualizes similarities.\n",
        "- **Use Cases**: Data visualization, clustering.\n",
        "\n",
        "### 13. **Echo State Networks (ESN)**:\n",
        "- **Description**: A type of recurrent neural network where only the output weights are trained.\n",
        "- **Use Cases**: Time series forecasting, dynamic system modeling.\n",
        "\n",
        "### 14. **Hopfield Networks**:\n",
        "- **Description**: A type of recurrent neural network that can serve as content-addressable memory systems.\n",
        "- **Use Cases**: Associative memory, pattern recognition.\n",
        "\n",
        "### 15. **Boltzmann Machines**:\n",
        "- **Description**: Stochastic recurrent neural networks that can learn internal representations using a set of visible and hidden units.\n",
        "- **Use Cases**: Feature learning, optimization problems.\n",
        "\n",
        "This is a broad overview, and there are many subtypes and variations of these neural networks, as well as emerging architectures developed for specific applications. The choice of architecture often depends on the nature of the problem and the type of data available."
      ],
      "metadata": {
        "id": "3IW81VccOEMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `The most crucial matrix operations in ML`\n",
        "\n",
        "Matrix operations are fundamental to the inner workings of many machine learning algorithms. Let's step through some of the most crucial matrix operations in ML:\n",
        "\n",
        "### 1. **Matrix Multiplication:**\n",
        "- **Why it's important:** At its core, a neural network layer is often just a matrix multiplication followed by an activation function. This operation allows transformation of the input data.\n",
        "- **Intuition:** Think of it as a way to combine and recombine features from the input data, letting the model decide which features are most important.\n",
        "\n",
        "### 2. **Element-wise Operations:**\n",
        "- **Why it's important:** Activation functions in neural networks, like the sigmoid or ReLU, are applied element-wise. This means they process each matrix (or tensor) element independently.\n",
        "- **Intuition:** Imagine a filter that highlights or dims each pixel in an image based on its brightness. That's an element-wise operation.\n",
        "\n",
        "### 3. **Matrix Transposition:**\n",
        "- **Why it's important:** Transposition is often used in preparing matrices for multiplication, especially in operations like calculating the gradient in backpropagation.\n",
        "- **Intuition:** Picture rotating a table of numbers (matrix) so that rows become columns and vice versa.\n",
        "\n",
        "### 4. **Matrix Inversion:**\n",
        "- **Why it's important:** Used in algorithms like linear regression to solve for parameters. It helps in finding a solution that minimizes the error.\n",
        "- **Intuition:** If matrix multiplication is like moving forward in a maze, inversion is akin to finding your way back to the start.\n",
        "\n",
        "### 5. **Determinant Calculation:**\n",
        "- **Why it's important:** While not as common in deep learning, the determinant can be critical in traditional ML methods to check if a matrix can be inverted.\n",
        "- **Intuition:** Think of the determinant as a value that gives a sense of the \"volume\" or \"scaling factor\" a matrix represents.\n",
        "\n",
        "### 6. **Eigenvalues and Eigenvectors:**\n",
        "- **Why it's important:** Central to Principal Component Analysis (PCA) and many other dimensionality reduction methods. They help identify directions in data with the most variance.\n",
        "- **Intuition:** In a group photo, people's heights vary. If you had to line them up to capture the most height variation in a single picture, you'd line them from shortest to tallest. Eigenvectors help find similar \"lines\" in data.\n",
        "\n",
        "### 7. **Dot Product:**\n",
        "- **Why it's important:** The dot product measures the similarity between vectors, which is critical in operations like calculating cosine similarity or in the attention mechanism in modern neural networks.\n",
        "- **Intuition:** Imagine two people pushing a box. If they push in the same direction, the box moves faster (high dot product). If they push in opposite directions, the box might not move (low or negative dot product).\n",
        "\n",
        "### 8. **Outer Product:**\n",
        "- **Why it's important:** Useful in certain algorithms to compute rank-1 updates to matrices.\n",
        "- **Intuition:** It's like mapping the influence of one vector onto another, creating a matrix that captures all possible interactions between their components.\n",
        "\n",
        "### In Summary:\n",
        "Matrix operations are the building blocks of machine learning algorithms. Understanding them provides clarity on how algorithms transform, dissect, and learn from data."
      ],
      "metadata": {
        "id": "0jEc5kYwA7rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Matrix operations in PyTorch`\n",
        "\n",
        "PyTorch is a popular deep learning framework that provides a rich set of matrix operations (often called tensor operations, since PyTorch operates on multi-dimensional matrices, or tensors). Let's walk through an overview of the most commonly used matrix operations in PyTorch:\n",
        "\n",
        "### 1. **Tensor Creation:**\n",
        "- **torch.tensor()**: Creates a tensor from data.\n",
        "- **torch.zeros()**: Creates a tensor filled with zeros.\n",
        "- **torch.ones()**: Creates a tensor filled with ones.\n",
        "- **torch.rand()**: Creates a tensor with random values between 0 and 1.\n",
        "- **Intuition:** These functions are like setting up your workspace, getting sheets of paper ready for calculations.\n",
        "\n",
        "### 2. **Basic Operations:**\n",
        "- **torch.add()**: Adds two tensors.\n",
        "- **torch.sub()**: Subtracts one tensor from another.\n",
        "- **torch.mul()**: Multiplies two tensors element-wise.\n",
        "- **torch.div()**: Divides one tensor by another element-wise.\n",
        "- **Intuition:** These are your basic arithmetic tools, like adding or subtracting numbers.\n",
        "\n",
        "### 3. **Matrix Multiplication:**\n",
        "- **torch.mm()**: Performs matrix multiplication.\n",
        "- **torch.matmul()** or **@**: Performs matrix multiplication which can handle batches of matrices.\n",
        "- **Intuition:** It's like combining features or information from two sets of data.\n",
        "\n",
        "### 4. **Element-wise Operations:**\n",
        "- **torch.exp()**: Computes the exponential of each element.\n",
        "- **torch.sqrt()**: Computes the square root of each element.\n",
        "- **Intuition:** Apply a function to each number on your sheet of paper independently.\n",
        "\n",
        "### 5. **Reshaping:**\n",
        "- **torch.reshape()** or **tensor.view()**: Reshapes a tensor to a different size.\n",
        "- **torch.squeeze()**: Removes dimensions of size 1.\n",
        "- **torch.unsqueeze()**: Adds a dimension of size 1.\n",
        "- **Intuition:** Adjusting the layout of your data, like rearranging rows and columns of numbers.\n",
        "\n",
        "### 6. **Reductions:**\n",
        "- **torch.sum()**: Sums elements of a tensor.\n",
        "- **torch.mean()**: Computes the mean of a tensor.\n",
        "- **torch.max()**: Finds the maximum value in a tensor.\n",
        "- **torch.min()**: Finds the minimum value in a tensor.\n",
        "- **Intuition:** Summarizing or getting a bird's eye view of your data.\n",
        "\n",
        "### 7. **Linear Algebra:**\n",
        "- **torch.inverse()**: Computes the inverse of a matrix.\n",
        "- **torch.eig()**: Computes eigenvalues and eigenvectors of a matrix.\n",
        "- **torch.dot()**: Computes the dot product of two vectors.\n",
        "- **Intuition:** Advanced tools that help in understanding relationships and patterns in data.\n",
        "\n",
        "### 8. **Broadcasting:**\n",
        "- PyTorch automatically broadcasts tensors during arithmetic operations when the shapes don't match exactly.\n",
        "- **Intuition:** Imagine adjusting smaller matrices to perform operations with larger ones without manually resizing them.\n",
        "\n",
        "### 9. **Device Transfers:**\n",
        "- **tensor.to('cuda')**: Moves a tensor to the GPU.\n",
        "- **tensor.to('cpu')**: Moves a tensor back to the CPU.\n",
        "- **Intuition:** Decide where you want to perform your calculations, on your desk (CPU) or using a calculator (GPU).\n",
        "\n",
        "### 10. **Gradients:**\n",
        "- PyTorch tensors have a property called **requires_grad**. If set to True, PyTorch will track operations on the tensor, allowing for automatic differentiation.\n",
        "- **tensor.backward()**: Computes gradients.\n",
        "- **Intuition:** Automatically find out how changing one number affects a final result.\n",
        "\n",
        "### In Summary:\n",
        "PyTorch offers a vast range of tensor operations, making it a powerful tool for machine learning and numerical computations. Understanding these operations can greatly aid in building and debugging deep learning models efficiently."
      ],
      "metadata": {
        "id": "8C4Kt9mCBDKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AVBQ1fYO80J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Simple addition\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "result = torch.add(a, b)\n",
        "print(result)  # tensor([5, 7, 9])\n",
        "\n",
        "c = a + 1\n",
        "print(c)  # tensor([5, 7, 9])\n",
        "\n",
        "# Using the alpha parameter\n",
        "result_with_alpha = torch.add(a, b, alpha=2)\n",
        "print(result_with_alpha)  # tensor([ 9, 12, 15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qegzHnl7y5e",
        "outputId": "b8bba281-3132-4049-9e64-3a8093488764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 7, 9])\n",
            "tensor([2, 3, 4])\n",
            "tensor([ 9, 12, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NumPy's `ndarray` and PyTorch's `tensor`\n",
        "\n",
        "Understanding the distinction between NumPy's ndarray and PyTorch's tensor is crucial for deep learning practitioners. Let's delve into their key differences and the innovations brought about by PyTorch's tensor.\n",
        "\n",
        "### 1. Nature and Primary Use:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- It's a multi-dimensional array object in the NumPy library, designed for numerical operations in Python.\n",
        "- ndarray stands for \"n-dimensional array,\" and it's primarily used for numerical computing tasks like linear algebra, statistical operations, and other mathematical functions.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- A tensor in PyTorch is very similar to NumPy's ndarray, but with some additional capabilities tailored for deep learning.\n",
        "- PyTorch tensors are primarily designed for use in neural networks and deep learning models.\n",
        "\n",
        "### 2. GPU Acceleration:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- Operates only on the CPU.\n",
        "- Does not natively support GPU acceleration.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- Can operate both on CPU and GPU. This is one of the significant innovations PyTorch brought.\n",
        "- With a simple command (like `.cuda()`), a tensor can be moved from CPU to GPU, allowing for faster numerical computations that are essential for training large deep learning models.\n",
        "\n",
        "### 3. Automatic Differentiation:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- Does not support automatic differentiation.\n",
        "- If you're building a neural network from scratch using NumPy, you'd have to manually compute the gradients during backpropagation.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- Supports automatic differentiation using its `autograd` mechanism. This is another major innovation.\n",
        "- This allows developers to automatically compute gradients or derivatives, which is a cornerstone for training neural networks using gradient descent.\n",
        "\n",
        "### 4. Deep Learning Framework Integration:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- While NumPy is not specifically a deep learning framework, many frameworks (including early versions of TensorFlow and others) used NumPy arrays as a base structure or for data manipulation.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- PyTorch's tensors are integrated into the PyTorch deep learning framework. This means you can define neural network layers, loss functions, and optimizers, and then directly use tensors within this ecosystem.\n",
        "\n",
        "### 5. Dynamic vs. Static Computation Graph:\n",
        "\n",
        "While this is more about PyTorch vs. other deep learning frameworks, it's worth mentioning:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- Does not have a notion of computation graphs as it's not a deep learning tool by itself.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- PyTorch uses a dynamic computation graph (or define-by-run graph). This means the graph is built on-the-fly as operations are created. This provides more flexibility, especially for models with dynamic control flow (like RNNs).\n",
        "- This contrasts with other frameworks that use a static computation graph (or define-and-run), where the graph is defined before any computations take place.\n",
        "\n",
        "### 6. Interoperability:\n",
        "\n",
        "**NumPy's ndarray:**\n",
        "- Serves as a foundational library, so many other libraries support converting to/from NumPy arrays.\n",
        "\n",
        "**PyTorch's Tensor:**\n",
        "- Provides easy conversion to and from NumPy arrays using methods like `.numpy()` and `torch.from_numpy()`. This ensures smooth interoperability between general numerical computing tasks and deep learning tasks.\n",
        "\n",
        "### Summary:\n",
        "While both NumPy's ndarray and PyTorch's tensor serve as multi-dimensional arrays suitable for numerical operations, PyTorch's tensor is designed with deep learning in mind. The major innovations brought by PyTorch's tensor include GPU acceleration, automatic differentiation, deep integration with a deep learning framework, and flexibility via dynamic computation graphs. These innovations have made it easier and more efficient to design, train, and deploy deep learning models."
      ],
      "metadata": {
        "id": "9OyFLAsmMy4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batched matrix multiplication\n",
        "\n",
        "Batched matrix multiplication is a powerful feature in PyTorch that allows you to perform matrix multiplication over batches of matrices, rather than just two individual matrices. This is particularly useful in deep learning, where we often work with batches of data.\n",
        "\n",
        "### 1. Introduction to Matrix Multiplication:\n",
        "\n",
        "Let's first recap the simple case of matrix multiplication.\n",
        "\n",
        "\n",
        "Given two matrices \\( A \\) of shape \\($ (m, n) $\\) and \\( B \\) of shape \\( $(n, p)$ \\), their product will be a matrix \\( C \\) of shape \\( (m, p) \\).\n",
        "\n",
        "\n",
        "### 2. Batched Matrix Multiplication:\n",
        "\n",
        "Now, imagine you don't have just one pair of matrices, but a whole batch of them.\n",
        "\n",
        "For instance:\n",
        "- You have a tensor \\( A \\) of shape \\( $(b, m, n)$ \\) and another tensor \\( B \\) of shape \\( $(b, n, p)$ \\).\n",
        "- Here, \\( b \\) represents the batch size.\n",
        "\n",
        "The batched matrix multiplication of \\( A \\) and \\( B \\) will result in a tensor \\( C \\) of shape \\($ (b, m, p) $\\), where each of the \\( b \\) matrices in \\( C \\) is the product of the corresponding matrices in \\( A \\) and \\( B \\).\n",
        "\n",
        "### 3. Using PyTorch:\n",
        "\n",
        "In PyTorch, batched matrix multiplication can be performed using the `torch.bmm()` function.\n",
        "\n",
        "### 4. Example:\n",
        "\n",
        "Let's consider a simple example:\n",
        "\n",
        "``` python\n",
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Define two batched tensors\n",
        "A = torch.tensor([\n",
        "    [[1, 2], [3, 4]],\n",
        "    [[5, 6], [7, 8]]\n",
        "])\n",
        "\n",
        "B = torch.tensor([\n",
        "    [[1, 0], [0, 1]],\n",
        "    [[2, 0], [0, 2]]\n",
        "])\n",
        "\n",
        "# Perform batched matrix multiplication\n",
        "result = torch.bmm(A, B)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n",
        "# Expected output:\n",
        "# tensor([[[ 1,  2],\n",
        "#          [ 3,  4]],\n",
        "#         \n",
        "#         [[10,  0],\n",
        "#          [14,  0]]])\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- \\( A \\) and \\( B \\) each contain 2 matrices.\n",
        "- The first matrix in \\( A \\) is multiplied by the first matrix in \\( B \\) to produce the first matrix in the result.\n",
        "- Similarly, the second matrix in \\( A \\) is multiplied by the second matrix in \\( B \\) to produce the second matrix in the result.\n",
        "\n",
        "### 5. Use Cases:\n",
        "\n",
        "Batched matrix multiplication is widely used in deep learning, especially in operations like:\n",
        "- Transformations in neural network layers where each item in a batch must be transformed using the same weight matrix.\n",
        "- Attention mechanisms, especially in models like Transformers, where batched matrix multiplication speeds up the computation.\n",
        "\n",
        "### 6. Note:\n",
        "\n",
        "It's crucial to ensure that the inner dimensions match for matrix multiplication. In the context of batched matrix multiplication, the matrices within each batch must have compatible shapes for multiplication.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "Batched matrix multiplication in PyTorch allows for efficient matrix multiplication operations over batches of matrices. This is achieved using the `torch.bmm()` function, which takes in two tensors of shape \\( (b, m, n) \\) and \\( (b, n, p) \\) and returns a tensor of shape \\( (b, m, p) \\). This operation is fundamental in deep learning models, where batch processing is commonplace."
      ],
      "metadata": {
        "id": "vBRzgGezNeDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "vkuJelWA-Fy2"
      }
    }
  ]
}